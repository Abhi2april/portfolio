{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer-Wise Learning Rate in PyTorch\n",
    "> Implementing discriminative learning rate across model layers\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Nikita Kozodoi\n",
    "- categories: [python, deep learning, pytorch, tutorial]\n",
    "- image: images/posts/lr.png\n",
    "- cover: images/covers/lr.png\n",
    "- permalink: /blog/:year:month:day/discriminative-lr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Last update: 29.03.2022. All opinions are my own.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017101,
     "end_time": "2020-09-28T10:41:26.670058",
     "exception": false,
     "start_time": "2020-09-28T10:41:26.652957",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Overview\n",
    "\n",
    "In deep learning tasks, we often use transfer learning to take advantage of the available pre-trained models. Fine-tuning such models is a careful process. On the one hand, we want to adjust the model to the new data set. On the other hand, we also want to retain and leverage as much knowledge learned during pre-training as possible. \n",
    "\n",
    "Discriminative learning rate is one of the tricks that can help us guide fine-tuning. By using lower learning rates on deeper layers of the network, we make sure we are not tempering too much with the model blocks that have already learned general patterns and concentrate fine-tuning on further layers.\n",
    "\n",
    "This blog post provides a tutorial on implementing discriminative layer-wise learning rates in `PyTorch`. We will see how to specify individual learning rates for each of the model parameter blocks and set up the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016886,
     "end_time": "2020-09-28T10:41:26.750200",
     "exception": false,
     "start_time": "2020-09-28T10:41:26.733314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Implementation \n",
    "\n",
    "The implementation of layer-wise learning rates is rather straightforward. It consists of three simple steps:\n",
    "1. Identifying a list of trainable layers in the neural net.\n",
    "2. Setting up a list of model parameter blocks together with the corresponding learning rates.\n",
    "3. Supplying the list with this information to the model optimizer.\n",
    "\n",
    "Let's go through each of these steps one by one and see how it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Identifying network layers\n",
    "\n",
    "The first step in our journey is to instantiate a model and retrieve the list of its layers. This step is essential to figure out how exactly to adjust the learning rate as we go through different parts of the network.\n",
    "\n",
    "As an example, we will load one of the CNNs from the `timm` library and print out its parameter groups by iterating through `model.named_parameters()` and saving their names in a list called `layer_names`. Note that the framework discussed in this post is model-agnostic. It will work with any architecture, including CNNs, RNNs and transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: conv1.weight\n",
      "1: bn1.weight\n",
      "2: bn1.bias\n",
      "3: layer1.0.conv1.weight\n",
      "4: layer1.0.bn1.weight\n",
      "5: layer1.0.bn1.bias\n",
      "...\n",
      "58: layer4.1.bn2.weight\n",
      "59: layer4.1.bn2.bias\n",
      "60: fc.weight\n",
      "61: fc.bias\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "import timm\n",
    "model = timm.create_model('resnet18', num_classes = 2)\n",
    "\n",
    "# save layer names\n",
    "layer_names = []\n",
    "for idx, (name, param) in enumerate(model.named_parameters()):\n",
    "    layer_names.append(name)\n",
    "    print(f'{idx}: {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the output suggests, our model has 62 parameter groups. When doing a forward pass, an image is fed to the first convolutional layer named `conv1`, whose parameters are stored as `conv1.weight`. Next, the output travels through the batch normalization layer `bn1`, which has weights and biases stored as `bn1.weight` and `bn1.bias`. From that point, the output goes through the network blocks grouped into four big chunks labeled as `layer1`, ..., `layer4`. Finally, extracted features are fed into the fully connected part of the network denoted as `fc`.\n",
    "\n",
    "In the cell below, we reverse the list of parameter group names to have the deepest layer in the end of the list. This will be useful on the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fc.bias',\n",
       " 'fc.weight',\n",
       " 'layer4.1.bn2.bias',\n",
       " 'layer4.1.bn2.weight',\n",
       " 'layer4.1.conv2.weight']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reverse layers\n",
    "layer_names.reverse()\n",
    "layer_names[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Specifying learning rates\n",
    "\n",
    "Knowing the architecture of our network, we can reason about the appropriate learning rates. \n",
    "\n",
    "There is some flexibility in how to approach this step. \n",
    "The key idea is to gradually reduce the learning rate when going deeper into the network. \n",
    "The first layers should already have a pretty good understanding of general domain-agnostic patterns after pre-training. \n",
    "In a computer vision setting, the first layers may have learned to distinguish simple shapes and edges; in natural language processing, the first layers may be responsible for general word relationships. \n",
    "We don't want to update parameters on the first layers too much, so it makes sense to reduce the corresponding learning rates. \n",
    "In contrast, we would like to set a higher learning rate for the final layers, especially for the fully-connected classifier part of the network. \n",
    "Those layers usually focus on domain-specific information and need to be trained on new data.\n",
    "\n",
    "The easiest approach to incorporate this logic is to incrementally reduce the learning rate when going deeper into the network. \n",
    "Let's simply multiply it by a certain coefficient between 0 and 1 after each parameter group.\n",
    "In our example, this would gives us 62 gradually diminishing learning rate values for 62 model blocks. \n",
    "\n",
    "Let's implement it in code! Below, we set up a list of dictionaries called `parameters` that stores model parameters and learning rates. \n",
    "We will simply go through all parameter blocks and iteratively reduce and assign the appropriate learning rate.\n",
    "In our example, we start with `lr = 0.01` and multiply it by `0.9` at each step.\n",
    "Each item in `parameters` becomes a dictionary with two elements:\n",
    "- `params`: tensor with the model parameters\n",
    "- `lr`: corresponding learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: lr = 0.010000, fc.bias\n",
      "1: lr = 0.009000, fc.weight\n",
      "2: lr = 0.008100, layer4.1.bn2.bias\n",
      "3: lr = 0.007290, layer4.1.bn2.weight\n",
      "4: lr = 0.006561, layer4.1.conv2.weight\n",
      "5: lr = 0.005905, layer4.1.bn1.bias\n",
      "...\n",
      "58: lr = 0.000022, layer1.0.conv1.weight\n",
      "59: lr = 0.000020, bn1.bias\n",
      "60: lr = 0.000018, bn1.weight\n",
      "61: lr = 0.000016, conv1.weight\n"
     ]
    }
   ],
   "source": [
    "# learning rate\n",
    "lr      = 1e-2\n",
    "lr_mult = 0.9\n",
    "\n",
    "# placeholder\n",
    "parameters = []\n",
    "\n",
    "# store params & learning rates\n",
    "for idx, name in enumerate(layer_names):\n",
    "    \n",
    "    # display info\n",
    "    print(f'{idx}: lr = {lr:.6f}, {name}')\n",
    "    \n",
    "    # append layer parameters\n",
    "    parameters += [{'params': [p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                    'lr':     lr}]\n",
    "    \n",
    "    # update learning rate\n",
    "    lr *= lr_mult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we gradually reduce our learning rate from `0.01` for the bias on the classification layer to `0.00001` on the first convolutional layer. Looks good, right?!\n",
    "\n",
    "Well, if you look closely, you will notice that we are setting different learning rates for parameter groups from the same layer. For example, having different learning rates for `fc.bias` and `fc.weight` does not really make that much sense. To address that, we can increment the learning rate only when going from one group of layers to another. The cell below provides an improved implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: lr = 0.010000, fc.bias\n",
      "1: lr = 0.010000, fc.weight\n",
      "2: lr = 0.009000, layer4.1.bn2.bias\n",
      "3: lr = 0.009000, layer4.1.bn2.weight\n",
      "4: lr = 0.009000, layer4.1.conv2.weight\n",
      "5: lr = 0.009000, layer4.1.bn1.bias\n",
      "...\n",
      "58: lr = 0.006561, layer1.0.conv1.weight\n",
      "59: lr = 0.005905, bn1.bias\n",
      "60: lr = 0.005905, bn1.weight\n",
      "61: lr = 0.005314, conv1.weight\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "# learning rate\n",
    "lr      = 1e-2\n",
    "lr_mult = 0.9\n",
    "\n",
    "# placeholder\n",
    "parameters      = []\n",
    "prev_group_name = layer_names[0].split('.')[0]\n",
    "\n",
    "# store params & learning rates\n",
    "for idx, name in enumerate(layer_names):\n",
    "    \n",
    "    # parameter group name\n",
    "    cur_group_name = name.split('.')[0]\n",
    "    \n",
    "    # update learning rate\n",
    "    if cur_group_name != prev_group_name:\n",
    "        lr *= lr_mult\n",
    "    prev_group_name = cur_group_name\n",
    "    \n",
    "    # display info\n",
    "    print(f'{idx}: lr = {lr:.6f}, {name}')\n",
    "    \n",
    "    # append layer parameters\n",
    "    parameters += [{'params': [p for n, p in model.named_parameters() if n == name and p.requires_grad],\n",
    "                    'lr':     lr}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks more interesting!\n",
    "\n",
    "Note that we can become very creative in customizing the learning rates and the decay speed. There is no fixed rule that always works well. In my experience, simple linear decay with a multiplier between 0.9 and 1 is a good starting point. Still, the framework provides a lot of space for experimentation, so feel free to test out your ideas and see what works best on your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Setting up the optimizer\n",
    "\n",
    "We are almost done. The last and the easiest step is to supply our list of model parameters together with the selected learning rates to the optimizer. In the cell below, we provide `parameters` to the Adam optimizer, which is one of the most frequently used ones in the field. \n",
    "\n",
    "Note that we don't need to supply the learning rate to `Adam()` as we have already done it in our `parameters` object. As long as individual learning rates are available, `optimizer` will prioritize them over the single learning rate supplied to the `Adam()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up optimizer\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it! Now we can proceed to training our model as usual. When calling `optimizer.step()` inside the training loop, the optimizer will update model parameters by  subtracting the gradient multiplied by the corresponding group-wise learning rates. This implies that there is no need to adjust the training loop, which usually looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "# loop through batches\n",
    "for (inputs, labels) in data_loader:\n",
    "\n",
    "    # extract inputs and labels\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # passes and weights update\n",
    "    with torch.set_grad_enabled(True):\n",
    "        \n",
    "        # forward pass \n",
    "        preds = model(inputs)\n",
    "        loss  = criterion(preds, labels)\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward() \n",
    "\n",
    "        # weights update\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.08316,
     "end_time": "2020-09-28T13:15:57.370796",
     "exception": false,
     "start_time": "2020-09-28T13:15:57.287636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Closing words\n",
    "\n",
    "In this post, we went through the steps of implementing a layer-wise discriminative learning rate in `PyTorch`. I hope this brief tutorial will help you set up your transfer learning pipeline and squeeze out the maximum of your pre-trained model. If you are interested, check out my other blog posts on tips on deep learning and `PyTorch`. Happy learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "duration": 9276.743122,
   "end_time": "2020-09-28T13:15:58.077845",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-28T10:41:21.334723",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "13c1b0b60dc243baa0859d1c6bd17129": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6ec176035b11447582886c6d337eeff6",
       "max": 87306240,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b89c0127f3a94527b58b7d710c43e951",
       "value": 87306240
      }
     },
     "149e60260f0b49268d9e6ce0c8461dce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "19703cdff07249fab2d78d23d35192f3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_149e60260f0b49268d9e6ce0c8461dce",
       "placeholder": "​",
       "style": "IPY_MODEL_68071ef4d8ad4ce782ddc9b045860848",
       "value": " 83.3M/83.3M [00:00&lt;00:00, 116MB/s]"
      }
     },
     "44310773133f44999edfef5f5ff34fbb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_13c1b0b60dc243baa0859d1c6bd17129",
        "IPY_MODEL_19703cdff07249fab2d78d23d35192f3"
       ],
       "layout": "IPY_MODEL_ef696dddbf0e4bdf851a3c08acd0a3c9"
      }
     },
     "68071ef4d8ad4ce782ddc9b045860848": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6ec176035b11447582886c6d337eeff6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b89c0127f3a94527b58b7d710c43e951": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "ef696dddbf0e4bdf851a3c08acd0a3c9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
