{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Extracting Intermediate Layer Outputs in PyTorch\n","> Simple way to extract activations from deep networks with hooks\n","\n","- toc: true \n","- badges: true\n","- comments: true\n","- author: Nikita Kozodoi\n","- categories: [python, deep learning, pytorch, tutorial]\n","- image: images/posts/extraction.png\n","- cover: images/covers/extraction.png\n","- permalink: /blog/:year:month:day/extracting-features\n","- sticky_rank: 3"]},{"attachments":{},"cell_type":"markdown","metadata":{"papermill":{"duration":0.009939,"end_time":"2020-08-22T17:15:25.872644","exception":false,"start_time":"2020-08-22T17:15:25.862705","status":"completed"},"tags":[]},"source":["*Last update: 23.10.2021. All opinions are my own.*\n","\n","# 1. Overview\n","\n","In deep learning tasks, we usually work with predictions outputted by the final layer of a neural network. In some cases, we might also be interested in the outputs of intermediate layers. Whether we want to extract data embeddings or inspect what is learned by earlier layers, it may not be straightforward how to extract the intermediate features from the network.\n"," \n","This blog post provides a quick tutorial on the extraction of intermediate activations from any layer of a deep learning model in `PyTorch` using the forward hook functionality. The important advantage of this method is its simplicity and ability to extract features without having to run the inference twice, only requiring a single forward pass through the model to save multiple outputs."]},{"cell_type":"markdown","metadata":{},"source":["# 2. Why do we need intermediate features?\n","\n","Extracting intermediate activations (also called features) can be useful in many applications. In computer vision problems, outputs of intermediate CNN layers are frequently used to visualize the learning process and illustrate visual features distinguished by the model on different layers. Another popular use case is extracting intermediate outputs to create image or text embeddings, which can be used to detect duplicate items, included as input features in a classical ML model, visualize data clusters and much more. When working with Encoder-Decoder architectures, outputs of intermediate layers can also be used to compress the data into a smaller-sized vector containing the data represenatation. There are many further use cases in which intermediate activations can be useful. So, let's discuss how to get them!\n"]},{"cell_type":"markdown","metadata":{},"source":["# 3. How to extract activations?\n","\n","To extract activations from intermediate layers, we will need to register a so-called forward hook for the layers of interest in our neural network and perform inference to store the relevant outputs. \n","\n","For the purpose of this tutorial, I will use image data from a [Cassava Leaf Disease Classification](https://www.kaggle.com/c/cassava-leaf-disease-classification) Kaggle competition. In the next few cells, we will import relevant libraries and set up a Dataloader object. Feel free to skip them if you are familiar with standard `PyTorch` data loading practices and go directly to the feature extraction part."]},{"cell_type":"markdown","metadata":{},"source":["## Preparations"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#collapse-hide\n","\n","##### PACKAGES\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","!pip install timm\n","import timm\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","import cv2\n","import os\n","\n","device = torch.device('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#collapse-hide\n","\n","##### DATASET\n","\n","class ImageData(Dataset):\n","    \n","    # init\n","    def __init__(self, \n","                 data, \n","                 directory, \n","                 transform):\n","        self.data      = data\n","        self.directory = directory\n","        self.transform = transform\n","        \n","    # length\n","    def __len__(self):\n","        return len(self.data)\n","    \n","    # get item  \n","    def __getitem__(self, idx):\n","        \n","        # import\n","        image = cv2.imread(os.path.join(self.directory, self.data.iloc[idx]['image_id']))\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","            \n","        # augmentations\n","        image = self.transform(image = image)['image']\n","        \n","        return image"]},{"cell_type":"markdown","metadata":{},"source":["We will use a standrd `PyTorch` dataloader to load the data in batches of 32 images. "]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T14:59:53.270704Z","iopub.status.busy":"2021-05-20T14:59:53.270463Z","iopub.status.idle":"2021-05-20T14:59:53.300423Z","shell.execute_reply":"2021-05-20T14:59:53.299474Z","shell.execute_reply.started":"2021-05-20T14:59:53.270680Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>image_id</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1000015157.jpg</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1000201771.jpg</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>100042118.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1000723321.jpg</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1000812911.jpg</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         image_id  label\n","0  1000015157.jpg      0\n","1  1000201771.jpg      3\n","2   100042118.jpg      1\n","3  1000723321.jpg      1\n","4  1000812911.jpg      3"]},"metadata":{},"output_type":"display_data"}],"source":["#collapse-show\n","\n","##### DATA LOADER\n","\n","# import data\n","df = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\n","display(df.head())\n","\n","# augmentations\n","transforms = A.Compose([A.Resize(height = 128, width = 128),\n","                        A.Normalize(),\n","                        ToTensorV2()])\n","\n","# dataset\n","data_set = ImageData(data      = df, \n","                     directory = '../input/cassava-leaf-disease-classification/train_images/',\n","                     transform = transforms)\n","\n","# dataloader\n","data_loader = DataLoader(data_set, \n","                         batch_size  = 32, \n","                         shuffle     = False, \n","                         num_workers = 2)"]},{"cell_type":"markdown","metadata":{},"source":["## Model\n","\n","To extract anything from a neural net, we first need to set up this net, right? In the cell below, we define a simple `resnet18` model with a two-node output layer. We use `timm` library to instantiate the model, but feature extraction will also work with any neural network written in `PyTorch`. \n","\n","We also print out the architecture of our network. As you can see, there are many intermediate layers through which our image travels during a forward pass before turning into a two-number output. We should note the names of the layers because we will need to provide them to a feature extraction function. "]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T14:59:53.302349Z","iopub.status.busy":"2021-05-20T14:59:53.301999Z","iopub.status.idle":"2021-05-20T14:59:53.633042Z","shell.execute_reply":"2021-05-20T14:59:53.632050Z","shell.execute_reply.started":"2021-05-20T14:59:53.302300Z"},"trusted":true},"outputs":[{"data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (act1): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (act1): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (act2): ReLU(inplace=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (act1): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (act2): ReLU(inplace=True)\n","    )\n","  )\n"," \n"," ... \n"," \n"," (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (act1): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (act2): ReLU(inplace=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (act1): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (act2): ReLU(inplace=True)\n","    )\n","  )\n","  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)\n","  (fc): Linear(in_features=512, out_features=2, bias=True)\n",")"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["##### DEFINE MODEL\n","\n","model    = timm.create_model(model_name = 'resnet18', pretrained = True)\n","model.fc = nn.Linear(512, 2)\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Feature extraction\n","\n","The implementation of feature extraction requires two simple steps:\n","\n","1. Registering a forward hook on a certain layer of the network.\n","2. Performing standard inference to extract features of that layer.\n","\n","First, we need to define a helper function that will introduce a so-called hook. A hook is simply a command that is executed when a forward or backward call to a certain layer is performed. If you want to know more about hooks, you can check out [this link](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/). \n","\n","In out setup, we are interested in a forward hook that simply copies the layer outputs, sends them to CPU and saves them to a dictionary object we call `features`. \n","\n","The hook is defined in a cell below. The `name` argument in `get_features()` specifies the dictionary key under which we will store our intermediate activations."]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T15:00:26.823962Z","iopub.status.busy":"2021-05-20T15:00:26.823637Z","iopub.status.idle":"2021-05-20T15:00:26.828524Z","shell.execute_reply":"2021-05-20T15:00:26.827434Z","shell.execute_reply.started":"2021-05-20T15:00:26.823932Z"},"trusted":true},"outputs":[],"source":["##### HELPER FUNCTION FOR FEATURE EXTRACTION\n","\n","def get_features(name):\n","    def hook(model, input, output):\n","        features[name] = output.detach()\n","    return hook"]},{"cell_type":"markdown","metadata":{},"source":["After the helper function is defined, we can register a hook using `.register_forward_hook()` method. The hook can be applied to any layer of the neural network. \n","\n","Since we work with a CNN, extracting features from the last convolutional layer might be useful to get image embeddings. Therefore, we are registering a hook for the outputs of the `(global_pool)`. To extract features from an earlier layer, we could also access them with, e.g., `model.layer1[1].act2` and save it under a different name in the `features` dictionary. With this method, we can actually register multiple hooks (one for every layer of interest), but we will only keep one for the purpose of this example."]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T15:00:27.758265Z","iopub.status.busy":"2021-05-20T15:00:27.757910Z","iopub.status.idle":"2021-05-20T15:00:27.766799Z","shell.execute_reply":"2021-05-20T15:00:27.765834Z","shell.execute_reply.started":"2021-05-20T15:00:27.758233Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<torch.utils.hooks.RemovableHandle at 0x7f2540254290>"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["##### REGISTER HOOK\n","\n","model.global_pool.register_forward_hook(get_features('feats'))"]},{"cell_type":"markdown","metadata":{},"source":["Now we are ready to extract features! The nice thing about hooks is that we can now perform inference as we usually would and get multiple outputs at the same time:\n","- outputs of the final layer\n","- outputs of every layer with a registered hook\n","\n","The feature extraction happens automatically during the forward pass whenever we run `model(inputs)`. To store intermediate features and concatenate them over batches, we just need to include the following in our inference loop:\n","\n","1. Create placeholder list `FEATS = []`. This list will store intermediate outputs from all batches.\n","2. Create placeholder dict `features = {}`. We will use this dictionary for storing intermediate outputs from each batch.\n","3. Iteratively extract batch features to `features`, send them to CPU and append to the list `FEATS`. "]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T15:00:40.541012Z","iopub.status.busy":"2021-05-20T15:00:40.540598Z","iopub.status.idle":"2021-05-20T15:00:44.748668Z","shell.execute_reply":"2021-05-20T15:00:44.747581Z","shell.execute_reply.started":"2021-05-20T15:00:40.540975Z"},"trusted":true},"outputs":[],"source":["##### FEATURE EXTRACTION LOOP\n","\n","# placeholders\n","PREDS = []\n","FEATS = []\n","\n","# placeholder for batch features\n","features = {}\n","\n","# loop through batches\n","for idx, inputs in enumerate(data_loader):\n","\n","    # move to device\n","    inputs = inputs.to(device)\n","       \n","    # forward pass [with feature extraction]\n","    preds = model(inputs)\n","    \n","    # add feats and preds to lists\n","    PREDS.append(preds.detach().cpu().numpy())\n","    FEATS.append(features['feats'].cpu().numpy())\n","\n","    # early stop\n","    if idx == 9:\n","        break"]},{"cell_type":"markdown","metadata":{},"source":["This is it! Looking at the shapes of resulting arrays, you can see that the code worked well: we extracted both final layer outputs as `PREDS` and intermediate activations as `FEATS`. We can now save these features and work with them further.\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2021-05-20T15:00:44.750808Z","iopub.status.busy":"2021-05-20T15:00:44.750462Z","iopub.status.idle":"2021-05-20T15:00:44.759555Z","shell.execute_reply":"2021-05-20T15:00:44.758505Z","shell.execute_reply.started":"2021-05-20T15:00:44.750757Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["- preds shape: (320, 2)\n","- feats shape: (320, 512)\n"]}],"source":["##### INSPECT FEATURES\n","\n","PREDS = np.concatenate(PREDS)\n","FEATS = np.concatenate(FEATS)\n","\n","print('- preds shape:', PREDS.shape)\n","print('- feats shape:', FEATS.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# 4. Closing words\n","\n","The purpose of this tutorial was to learn you how to extract intermediate outputs from the most interesting layers of your neural networks. With hooks, you can do all feature extraction in a single inference run and avoid complex modifications of your model. I hope you found this post helpful.\n","\n","If you are interested, check out my other blog posts to see more tips on deep learning and `PyTorch`. Happy learning!"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.8 64-bit ('py3': conda)","name":"python388jvsc74a57bd0310cd5f5609f69ea751e1868ad0136080dc62c502a2bbda6a0995c7a80ed57e1"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"metadata":{"interpreter":{"hash":"310cd5f5609f69ea751e1868ad0136080dc62c502a2bbda6a0995c7a80ed57e1"}}},"nbformat":4,"nbformat_minor":4}
