{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Demand Forecasting: Data Mining Cup 2020 Solution\"\n",
    "> Profit-driven demand forecasting with gradient boosted trees\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Nikita Kozodoi\n",
    "- categories: [python, time series, demand forecasting]\n",
    "- image: images/posts/demand.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Data Mining Cup](https://www.data-mining-cup.com) is an annual international machine learning competition. The 2020 edition was devoted to demand forecasting. Together with [Elizaveta Zinovyeva](https://www.linkedin.com/in/elizaveta-zinovyeva-4155a184/), we represented the Humboldt University of Berlin in the competition and finished in the top-15 of the leaderboard.\n",
    "\n",
    "Demand forecasting is an important managerial task that helps to optimize inventory planning. The optimized stocks can reduce retailer's costs and increase customer satisfaction due to faster delivery time. The competition task is to use historical purchase data to predict future demand for different products.\n",
    "\n",
    "This blogpost provides a detailed walkthrough covering the crucial steps of our solution:\n",
    "- data preparation and feature engineering\n",
    "- aggregation of transactional data into the daily format\n",
    "- implementation of custom profit-driven loss functions\n",
    "- two-stage demand forecasting with LightGBM models\n",
    "- performing hyper-parameter tuning using Bayesian search\n",
    "\n",
    "Feel free to jump directly to the parts that are interesting to you!\n",
    "\n",
    "The complete notebooks reproducing our solution are [available on Github](https://github.com/kozodoi/DMC_2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The competition data consists of three data sets: \n",
    "- `infos.csv`: provides information on prices and promotion days in the unlabeled test set\n",
    "- `items.csv`: provides item-specific characteristics such as brand, customer rating, etc\n",
    "- `orders.csv`: contains all customer transactions over the 6-month period\n",
    "\n",
    "Let's import relevant packages and have a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10463, 3)\n",
      "(10463, 8)\n",
      "(2181955, 5)\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "########## PACKAGES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "!pip install --upgrade dptools\n",
    "from dptools import *\n",
    "\n",
    "\n",
    "########## DATA IMPORT\n",
    "\n",
    "infos  = pd.read_csv('../data/raw/infos.csv',  sep = '|')\n",
    "items  = pd.read_csv('../data/raw/items.csv',  sep = '|')\n",
    "orders = pd.read_csv('../data/raw/orders.csv', sep = '|')\n",
    "\n",
    "print(infos.shape)\n",
    "print(items.shape)\n",
    "print(orders.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemID</th>\n",
       "      <th>simulationPrice</th>\n",
       "      <th>promotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.43</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9.15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>14.04</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>14.10</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7.48</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   itemID  simulationPrice promotion\n",
       "0       1             3.43       NaN\n",
       "1       2             9.15       NaN\n",
       "2       3            14.04       NaN\n",
       "3       4            14.10       NaN\n",
       "4       5             7.48       NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "infos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemID</th>\n",
       "      <th>brand</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>customerRating</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>category3</th>\n",
       "      <th>recommendedRetailPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.44</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>40.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   itemID  brand  manufacturer  customerRating  category1  category2  \\\n",
       "0       1      0             1            4.38          1          1   \n",
       "1       2      0             2            3.00          1          2   \n",
       "2       3      0             3            5.00          1          3   \n",
       "3       4      0             2            4.44          1          2   \n",
       "4       5      0             2            2.33          1          1   \n",
       "\n",
       "   category3  recommendedRetailPrice  \n",
       "0          1                    8.84  \n",
       "1          1                   16.92  \n",
       "2          1                   15.89  \n",
       "3          1                   40.17  \n",
       "4          1                   17.04  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>transactID</th>\n",
       "      <th>itemID</th>\n",
       "      <th>order</th>\n",
       "      <th>salesPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-01 00:01:56</td>\n",
       "      <td>2278968</td>\n",
       "      <td>450</td>\n",
       "      <td>1</td>\n",
       "      <td>17.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2018-01-01 00:01:56</td>\n",
       "      <td>2278968</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>5.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2018-01-01 00:07:11</td>\n",
       "      <td>2255797</td>\n",
       "      <td>7851</td>\n",
       "      <td>2</td>\n",
       "      <td>20.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2018-01-01 00:09:24</td>\n",
       "      <td>2278968</td>\n",
       "      <td>450</td>\n",
       "      <td>1</td>\n",
       "      <td>17.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2018-01-01 00:09:24</td>\n",
       "      <td>2278968</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>5.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time  transactID  itemID  order  salesPrice\n",
       "0  2018-01-01 00:01:56     2278968     450      1       17.42\n",
       "1  2018-01-01 00:01:56     2278968      83      1        5.19\n",
       "2  2018-01-01 00:07:11     2255797    7851      2       20.47\n",
       "3  2018-01-01 00:09:24     2278968     450      1       17.42\n",
       "4  2018-01-01 00:09:24     2278968      83      1        5.19"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction task is to forecast the future demand: for each of the 10,463 items, we need to predict the total number of orders in the 14-day period following the last day in the `orders` datadrame.\n",
    "\n",
    "Let's do some data preprocessing. First, we merge `items` and `infos` since they both contain information on the item level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10463, 3)\n",
      "(10463, 8)\n",
      "(10463, 10)\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "print(infos.shape)\n",
    "print(items.shape)\n",
    "items = pd.merge(infos, items, on = 'itemID', how = 'left')\n",
    "print(items.shape)\n",
    "del infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we check and convert feature types to the appropriate format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "itemID                      int64\n",
      "simulationPrice           float64\n",
      "promotion                  object\n",
      "brand                       int64\n",
      "manufacturer                int64\n",
      "customerRating            float64\n",
      "category1                   int64\n",
      "category2                   int64\n",
      "category3                   int64\n",
      "recommendedRetailPrice    float64\n",
      "dtype: object\n",
      "--------------------------------------------------\n",
      "time           object\n",
      "transactID      int64\n",
      "itemID          int64\n",
      "order           int64\n",
      "salesPrice    float64\n",
      "dtype: object\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "print('-' * 50)\n",
    "print(items.dtypes)\n",
    "print('-' * 50)\n",
    "print(orders.dtypes)\n",
    "print('-' * 50)\n",
    "\n",
    "# items\n",
    "for var in ['itemID', 'brand', 'manufacturer', 'category1', 'category2', 'category3']:\n",
    "    items[var] = items[var].astype('str').astype('object') \n",
    "    \n",
    "# orders\n",
    "for var in ['transactID', 'itemID']:\n",
    "    orders[var] = orders[var].astype('str').astype('object') \n",
    "    \n",
    "# dates\n",
    "orders['time'] = pd.to_datetime(orders['time'].astype('str'), infer_datetime_format = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to unfold the `promotion` feature that contains a list of ptomotion dates separated by a coma. We use `split_nested_features()` from `dptools` that separates a string column into separate features.\n",
    "\n",
    "`dptools` is a package developed by me to simplify some of the common data preprocessing and feature engineering tasks. Below, you will see more examples on using `dptools` for other applications. You can read more about the package [here](https://github.com/kozodoi/dptools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 split-based features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemID</th>\n",
       "      <th>simulationPrice</th>\n",
       "      <th>brand</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>customerRating</th>\n",
       "      <th>category1</th>\n",
       "      <th>category2</th>\n",
       "      <th>category3</th>\n",
       "      <th>recommendedRetailPrice</th>\n",
       "      <th>promotion_0</th>\n",
       "      <th>promotion_1</th>\n",
       "      <th>promotion_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.84</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>14.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>5.00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15.89</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>14.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>4.44</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>40.17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  itemID  simulationPrice brand manufacturer  customerRating category1  \\\n",
       "0      1             3.43   NaN            1            4.38         1   \n",
       "1      2             9.15   NaN            2            3.00         1   \n",
       "2      3            14.04   NaN            3            5.00         1   \n",
       "3      4            14.10   NaN            2            4.44         1   \n",
       "4      5             7.48   NaN            2            2.33         1   \n",
       "\n",
       "  category2 category3  recommendedRetailPrice promotion_0 promotion_1  \\\n",
       "0         1         1                    8.84         NaN         NaN   \n",
       "1         2         1                   16.92         NaN         NaN   \n",
       "2         3         1                   15.89         NaN         NaN   \n",
       "3         2         1                   40.17         NaN         NaN   \n",
       "4         1         1                   17.04         NaN         NaN   \n",
       "\n",
       "  promotion_2  \n",
       "0         NaN  \n",
       "1         NaN  \n",
       "2         NaN  \n",
       "3         NaN  \n",
       "4         NaN  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "\n",
    "# split promotion feature\n",
    "items = split_nested_features(items, split_vars = 'promotion', sep = ',')\n",
    "print(items.head())\n",
    "\n",
    "# convert dates\n",
    "promotion_vars = items.filter(like = 'promotion_').columns\n",
    "for var in promotion_vars:\n",
    "    items[var] = pd.to_datetime(items[var], infer_datetime_format = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now export the two prepared datafranmes as csv files. I am using the `save_csv_version()` function that automatically adds a version number to the file name to prevent overwriting the exported data after making changes in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as ../data/prepared/orders_v2.csv\n",
      "Saved as ../data/prepared/items_v2.csv\n",
      "(2181955, 5)\n",
      "(10463, 12)\n"
     ]
    }
   ],
   "source": [
    "#collapse-show\n",
    "\n",
    "save_csv_version('../data/prepared/orders.csv', orders, index = False, compression = 'gzip')\n",
    "save_csv_version('../data/prepared/items.csv',  items,  index = False, compression = 'gzip')\n",
    "print(orders.shape)\n",
    "print(items.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Aggregation and feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with the `orders` dataframe. The data in `orders` is formatted as a list of transactions with the corresponding timestaps. We need to aggregate this data in order to use it for the modeling.\n",
    "\n",
    "Since the task is a 14-day demand forecasting, a simple way forward would be to aggregate transactions on a two-week basis. However, this could lead to losing some more granular information. Instead, we aggregate transactions by day and construct a target variable based on orders within a two-week period following that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemID</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   itemID  day_of_year  order\n",
       "0       1           23      1\n",
       "1       1           25      1\n",
       "2       1           29    307\n",
       "3       1           30      3\n",
       "4       1           31      1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-show\n",
    "\n",
    "orders['day_of_year'] = orders['time'].dt.dayofyear\n",
    "orders_price = orders.groupby(['itemID', 'day_of_year'])['salesPrice'].agg('mean').reset_index()\n",
    "orders = orders.groupby(['itemID', 'day_of_year'])['order'].agg('sum').reset_index()\n",
    "orders.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding missing item-day combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregated dataframe only contains entries for day-item pairs when there is at least one transaction. This results in missing information:\n",
    "- Most items are only sold on a few days within the considered timeframe. The information on days with no orders is valuable but is not observed in the aggregated data.\n",
    "- There are a few items that are never sold and therefore do not appear in `orders` at all.\n",
    "\n",
    "To account for the missing zeroes, we append the items that were never sold and add entries with `order = 0` for day-item comibnations that are missing. This increases the number of observations from 100,771 to 1,883,340."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100771, 3)\n",
      "(1883340, 3)\n"
     ]
    }
   ],
   "source": [
    "#collapse-show\n",
    "\n",
    "# add items that were never sold before\n",
    "missing_itemIDs = set(items['itemID'].unique()) - set(orders['itemID'].unique())\n",
    "missing_rows = pd.DataFrame({'itemID':     list(missing_itemIDs), \n",
    "                            'day_of_year': np.ones(len(missing_itemIDs)).astype('int'), \n",
    "                            'order':       np.zeros(len(missing_itemIDs)).astype('int')})\n",
    "orders = pd.concat([orders, missing_rows], axis = 0)\n",
    "print(orders.shape)\n",
    "\n",
    "# add zeros for days with no transactions\n",
    "agg_orders = orders.groupby(['itemID', 'day_of_year']).order.unique().unstack('day_of_year').stack('day_of_year', dropna = False)\n",
    "agg_orders = agg_orders.reset_index()\n",
    "agg_orders.columns = ['itemID', 'day_of_year', 'order']\n",
    "agg_orders['order'].fillna(0, inplace = True)\n",
    "agg_orders['order'] = agg_orders['order'].astype(int)\n",
    "print(agg_orders.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also append mean daily prices for all item-day combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100771, 3)\n",
      "(1883340, 3)\n",
      "(1883340, 4)\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "# add items that were never sold before\n",
    "missing_rows = pd.DataFrame({'itemID':     list(missing_itemIDs), \n",
    "                            'day_of_year': np.ones(len(missing_itemIDs)).astype('int'), \n",
    "                            'salesPrice':  np.zeros(len(missing_itemIDs)).astype('int')})\n",
    "orders_price = pd.concat([orders_price, missing_rows], axis = 0)\n",
    "print(orders_price.shape)\n",
    "\n",
    "# add zeros for days with no transactions\n",
    "agg_orders_price = orders_price.groupby(['itemID', 'day_of_year']).salesPrice.unique().unstack('day_of_year').stack('day_of_year', dropna = False)\n",
    "agg_orders_price = agg_orders_price.reset_index()\n",
    "agg_orders_price.columns = ['itemID', 'day_of_year', 'salesPrice']\n",
    "agg_orders_price['salesPrice'].fillna(0, inplace = True)\n",
    "agg_orders_price['salesPrice'] = agg_orders_price['salesPrice'].astype(int)\n",
    "agg_orders_price['salesPrice'][agg_orders_price['salesPrice'] == 0] = np.nan\n",
    "print(agg_orders_price.shape)\n",
    "\n",
    "# fill missing prices for dates with no orders\n",
    "agg_orders_price['salesPrice'] = agg_orders_price.groupby(['itemID']).salesPrice.fillna(method = 'ffill')\n",
    "agg_orders_price['salesPrice'] = agg_orders_price.groupby(['itemID']).salesPrice.fillna(method = 'bfill')\n",
    "agg_orders_price = agg_orders_price.merge(items[['itemID', 'simulationPrice']], how = 'left', on = 'itemID')\n",
    "agg_orders_price['salesPrice'][agg_orders_price['salesPrice'].isnull()] = agg_orders_price['simulationPrice'][agg_orders_price['salesPrice'].isnull()]\n",
    "del agg_orders_price['simulationPrice']\n",
    "\n",
    "# merge prices to orders\n",
    "agg_orders = agg_orders.merge(agg_orders_price, how = 'left', on = ['itemID', 'day_of_year'])\n",
    "print(agg_orders.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling promotions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competition documentation indicates that the training period contains promotion days that are not explicilty marked. \n",
    "\n",
    "We need to manually mark promotion days. Without it, making predictions is difficult because orders in some days explode without an apparent reason. In such cases, the underlying reason is likely to be a promotion carried out by the retailer.\n",
    "\n",
    "We need to be very careful and conservative with a threshold used to mark promotions. Labeling too may promotions based on the number of orders risks introducing data leakage since the number of orders is unknown at the prediction time. Below, I am using `find_peaks()` function to isolate peaks in the `order` time series and encode such outliers as promotion days:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily p(promotion) per item in train: 0.0079\n",
      "Daily p(promotion) per item in test:  0.0141\n"
     ]
    }
   ],
   "source": [
    "#collapse-show\n",
    "\n",
    "# computations\n",
    "agg_orders['promotion'] = 0\n",
    "for itemID in tqdm(agg_orders['itemID'].unique()):\n",
    "    promo    = np.zeros(len(agg_orders[agg_orders['itemID'] == itemID]))\n",
    "    avg      = agg_orders[(agg_orders['itemID'] == itemID)]['order'].median()\n",
    "    std      = agg_orders[(agg_orders['itemID'] == itemID)]['order'].std()\n",
    "    peaks, _ = find_peaks(np.append(agg_orders[agg_orders['itemID'] == itemID]['order'].values, avg), # append avg to enable marking last point as promo\n",
    "                          prominence = max(5, std),  # peak difference with neighbor points; max(5,std) to exclude cases when std is too small\n",
    "                          height     = avg + 2*std)  # minimal height of a peak\n",
    "    promo[peaks] = 1\n",
    "    agg_orders.loc[agg_orders['itemID'] == itemID, 'promotion'] = promo\n",
    "\n",
    "# compare promotion number\n",
    "promo_in_train = (agg_orders['promotion'].sum() / agg_orders['day_of_year'].max()) / len(items)\n",
    "promo_in_test  = (3*len(items) - items.promotion_0.isnull().sum() - items.promotion_2.isnull().sum() - items.promotion_1.isnull().sum()) / 14 / len(items)\n",
    "print('Daily p(promotion) per item in train: {}'.format(np.round(promo_in_train, 4)))\n",
    "print('Daily p(promotion) per item in test:  {}'.format(np.round(promo_in_test , 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our method identifies 14,911 promotions. Compared to the unlabeled test set where promotion days are explicitly reported, this amounts to about twice as few promotions per item and day. \n",
    "\n",
    "Let's look at the actual order data for a set of items to check which observations are marked as promotions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "# compute promo count\n",
    "promo_count = agg_orders.groupby('itemID')['promotion'].agg('sum').reset_index()\n",
    "promo_count = promo_count.sort_values('promotion').reset_index(drop = True)\n",
    "\n",
    "# plot some items\n",
    "item_plots = [0, 2000, 4000, 6000, 8000, 9000, 10000, 10100, 10200, 10300, 10400, 10462]\n",
    "fig = plt.figure(figsize = (16, 12))\n",
    "for i in range(len(item_plots)):\n",
    "    plt.subplot(3, 4, i + 1)\n",
    "    df = agg_orders[agg_orders.itemID == promo_count['itemID'][item_plots[i]]]\n",
    "    plt.scatter(df['day_of_year'], df['order'], c = df['promotion'])\n",
    "    plt.ylabel('Total Orders')\n",
    "    plt.xlabel('Day')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/fig_promotions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yellow marker indicates days with promotions. Results suggest that our method identifies some outliers as promotions but misses a few points that are less prominent. At the same time, we can not be sure that these cases are necessarily promotions: the large number of orders on these days could be observed due to other reasons. We will stick to this promotion identification method for now but note that this aspect might require further improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is aggregated, we can construct transaction-based features as well as the target variable. For each day, we compute target as the total number of orders in the following 14 days. The days preceeding the considered day are used to extract lag-based features. This is done in the computation loop. For each day, we extract slices of the past [1, 7, ..., 35] days and compute features based on data from that slice.\n",
    "\n",
    "For each item, we construct the following features:\n",
    "- the total count of orders and the total number of ordered items\n",
    "- the total count of promotions\n",
    "- mean item price\n",
    "- recency of the last order\n",
    "\n",
    "The number of orders and promotions is also aggregated on a manufacturer and category level.\n",
    "\n",
    "In addition to the described features, we use `tsfresh` package to automatically extract features based on the `order` time series from the last 35 days. `tsfresh` uses built-in functions to compute hundreds of features describing the time series. After extraction, we only keep features with no missing values for all day-item combinations.\n",
    "\n",
    "Finally, we compute features based on the two-week for which we predict demand: the number of promotions and mean prices per item, manufacturer and category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1391579, 458)\n",
      "(1391579, 470)\n"
     ]
    }
   ],
   "source": [
    "#collapse-show\n",
    "\n",
    "# libraries\n",
    "from tsfresh import extract_featuresfrom tsfresh import extract_features\n",
    "\n",
    "# parameters\n",
    "days_input  = [1, 7, 14, 21, 28, 35]\n",
    "days_target = 14\n",
    "\n",
    "# preparations\n",
    "day_first = np.max(days_input)\n",
    "day_last  = agg_orders['day_of_year'].max() - days_target + 1\n",
    "orders    = None\n",
    "\n",
    "# merge manufacturer and category\n",
    "agg_orders = agg_orders.merge(items[['itemID', 'manufacturer']], how = 'left')\n",
    "agg_orders = agg_orders.merge(items[['itemID', 'category']],     how = 'left')\n",
    "\n",
    "\n",
    "# computations\n",
    "for day_of_year in tqdm(list(range(149, day_last)) + [agg_orders['day_of_year'].max()]):\n",
    "                \n",
    "\n",
    "    ###### VALIDAION: TARGET, PROMOTIONS, PRICES\n",
    "        \n",
    "    # day intervals\n",
    "    target_day_min = day_of_year + 1\n",
    "    target_day_max = day_of_year + days_target\n",
    "    \n",
    "    # compute target and promo: labeled data\n",
    "    if day_of_year < agg_orders['day_of_year'].max():\n",
    "        \n",
    "        # target and future promo\n",
    "        tmp_df = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
    "                            (agg_orders['day_of_year'] <= target_day_max)\n",
    "                           ].groupby('itemID')['order', 'promotion'].agg('sum').reset_index()\n",
    "        tmp_df.columns = ['itemID', 'target', 'promo_in_test']\n",
    "        \n",
    "        # future price\n",
    "        tmp_df['mean_price_test'] = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
    "                                               (agg_orders['day_of_year'] <= target_day_max)\n",
    "                                              ].groupby('itemID')['salesPrice'].agg('mean').reset_index()['salesPrice']\n",
    "        \n",
    "        # merge manufacturer and category\n",
    "        tmp_df = tmp_df.merge(items[['itemID', 'manufacturer', 'category']], how = 'left', on = 'itemID')\n",
    "        \n",
    "        # future price per manufacturer\n",
    "        tmp_df_manufacturer = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
    "                                         (agg_orders['day_of_year'] <= target_day_max)\n",
    "                                         ].groupby('manufacturer')['salesPrice'].agg('mean').reset_index()\n",
    "        tmp_df_manufacturer.columns = ['manufacturer', 'mean_price_test_manufacturer']\n",
    "        tmp_df = tmp_df.merge(tmp_df_manufacturer, how = 'left', on = 'manufacturer')\n",
    "        \n",
    "        # future price per category\n",
    "        tmp_df_category = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
    "                                     (agg_orders['day_of_year'] <= target_day_max)\n",
    "                                     ].groupby('category')['salesPrice'].agg('mean').reset_index()\n",
    "        tmp_df_category.columns = ['category', 'mean_price_test_category']\n",
    "        tmp_df = tmp_df.merge(tmp_df_category, how = 'left', on = 'category')\n",
    "        \n",
    "        # future promo per manufacturer\n",
    "        tmp_df_manufacturer = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
    "                                         (agg_orders['day_of_year'] <= target_day_max)\n",
    "                                         ].groupby('manufacturer')['promotion'].agg('sum').reset_index()\n",
    "        tmp_df_manufacturer.columns = ['manufacturer', 'promo_in_test_manufacturer']\n",
    "        tmp_df = tmp_df.merge(tmp_df_manufacturer, how = 'left', on = 'manufacturer')\n",
    "\n",
    "        # future promo per category\n",
    "        tmp_df_category = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
    "                                     (agg_orders['day_of_year'] <= target_day_max)\n",
    "                                     ].groupby('category')['promotion'].agg('sum').reset_index()\n",
    "        tmp_df_category.columns = ['category', 'promo_in_test_category']\n",
    "        tmp_df = tmp_df.merge(tmp_df_category, how = 'left', on = 'category')\n",
    "                       \n",
    "        \n",
    "    # compute target and promo: unlabeled data\n",
    "    else:\n",
    "        \n",
    "        # placeholders\n",
    "        tmp_df = pd.DataFrame({'itemID':                     items.itemID,\n",
    "                               'target':                     np.nan,\n",
    "                               'promo_in_test':              np.nan,\n",
    "                               'mean_price_test':            items.simulationPrice,\n",
    "                               'manufacturer':               items.manufacturer,\n",
    "                               'category':                   items.category,\n",
    "                               'promo_in_test_manufacturer': np.nan,\n",
    "                               'promo_in_test_category':     np.nan})\n",
    "\n",
    "        \n",
    "    ###### TRAINING: LAG-BASED FEATURES\n",
    "            \n",
    "    # compute features\n",
    "    for day_input in days_input:\n",
    "        \n",
    "        # day intervals\n",
    "        input_day_min  = day_of_year - day_input + 1\n",
    "        input_day_max  = day_of_year\n",
    "    \n",
    "        # frequency, promo and price\n",
    "        tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
    "                                  (agg_orders['day_of_year'] <= input_day_max)\n",
    "                                 ].groupby('itemID')\n",
    "        tmp_df['order_sum_last_'   + str(day_input)] = tmp_df_input['order'].agg('sum').reset_index()['order']\n",
    "        tmp_df['order_count_last_' + str(day_input)] = tmp_df_input['order'].agg(lambda x: len(x[x > 0])).reset_index()['order']\n",
    "        tmp_df['promo_count_last_' + str(day_input)] = tmp_df_input['promotion'].agg('sum').reset_index()['promotion']\n",
    "        tmp_df['mean_price_last_'  + str(day_input)] = tmp_df_input['salesPrice'].agg('mean').reset_index()['salesPrice']\n",
    "\n",
    "        # frequency, promo per manufacturer\n",
    "        tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
    "                                  (agg_orders['day_of_year'] <= input_day_max)\n",
    "                                 ].groupby('manufacturer')\n",
    "        tmp_df_manufacturer = tmp_df_input['order'].agg('sum').reset_index()\n",
    "        tmp_df_manufacturer.columns = ['manufacturer', 'order_manufacturer_sum_last_' + str(day_input)]\n",
    "        tmp_df_manufacturer['order_manufacturer_count_last_' + str(day_input)] = tmp_df_input['order'].agg(lambda x: len(x[x > 0])).reset_index()['order']\n",
    "        tmp_df_manufacturer['promo_manufacturer_count_last_' + str(day_input)] = tmp_df_input['promotion'].agg('sum').reset_index()['promotion']\n",
    "        tmp_df = tmp_df.merge(tmp_df_manufacturer, how = 'left', on = 'manufacturer')\n",
    "    \n",
    "        # frequency, promo per category\n",
    "        tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
    "                                  (agg_orders['day_of_year'] <= input_day_max)\n",
    "                                 ].groupby('category')\n",
    "        tmp_df_category = tmp_df_input['order'].agg('sum').reset_index()\n",
    "        tmp_df_category.columns = ['category', 'order_category_sum_last_' + str(day_input)]       \n",
    "        tmp_df_category['order_category_count_last_' + str(day_input)] = tmp_df_input['order'].agg(lambda x: len(x[x > 0])).reset_index()['order']\n",
    "        tmp_df_category['promo_category_count_last_' + str(day_input)] = tmp_df_input['promotion'].agg('sum').reset_index()['promotion']\n",
    "        tmp_df = tmp_df.merge(tmp_df_category, how = 'left', on = 'category')\n",
    "\n",
    "        # frequency, promo per all items\n",
    "        tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
    "                                  (agg_orders['day_of_year'] <= input_day_max)]\n",
    "        tmp_df['order_all_sum_last_'   + str(day_input)] = tmp_df_input['order'].agg('sum')\n",
    "        tmp_df['order_all_count_last_' + str(day_input)] = tmp_df_input['order'].agg(lambda x: len(x[x > 0]))\n",
    "        tmp_df['promo_all_count_last_' + str(day_input)] = tmp_df_input['promotion'].agg('sum')\n",
    "        \n",
    "        # recency\n",
    "        if day_input == max(days_input):\n",
    "            tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
    "                                      (agg_orders['day_of_year'] <= input_day_max) &\n",
    "                                      (agg_orders['order'] > 0)\n",
    "                                     ].groupby('itemID')\n",
    "            tmp_df['days_since_last_order'] = (day_of_year - tmp_df_input['day_of_year'].agg('max')).reindex(tmp_df.itemID).reset_index()['day_of_year']\n",
    "            tmp_df['days_since_last_order'].fillna(day_input, inplace = True)\n",
    "            \n",
    "            \n",
    "        # tsfresh features\n",
    "        if day_input == max(days_input):\n",
    "            tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
    "                                      (agg_orders['day_of_year'] <= input_day_max)]\n",
    "            tmp_df_input = tmp_df_input[['day_of_year', 'itemID', 'order']]\n",
    "            extracted_features = extract_features(tmp_df_input, column_id = 'itemID', column_sort = 'day_of_year')\n",
    "            extracted_features['itemID'] = extracted_features.index\n",
    "            tmp_df = tmp_df.merge(extracted_features, how = 'left', on = 'itemID')\n",
    "            \n",
    "            \n",
    "    ###### FINAL PREPARATIONS\n",
    "            \n",
    "    # add day of year\n",
    "    tmp_df.insert(1, column = 'day_of_year', value = day_of_year)\n",
    "        \n",
    "    # merge data\n",
    "    orders = pd.concat([orders, tmp_df], axis = 0)\n",
    "    \n",
    "    # drop manufacturer and category\n",
    "    del orders['manufacturer']\n",
    "    del orders['category']#collapse-show\n",
    "\n",
    "# libraries\n",
    "from tsfresh import extract_featuresfrom tsfresh import extract_features\n",
    "\n",
    "# parameters\n",
    "days_input  = [1, 7, 14, 21, 28, 35]\n",
    "days_target = 14\n",
    "\n",
    "# preparations\n",
    "day_first = np.max(days_input)\n",
    "day_last  = agg_orders['day_of_year'].max() - days_target + 1\n",
    "orders    = None\n",
    "\n",
    "# merge manufacturer and category\n",
    "agg_orders = agg_orders.merge(items[['itemID', 'manufacturer']], how = 'left')\n",
    "agg_orders = agg_orders.merge(items[['itemID', 'category']],     how = 'left')\n",
    "\n",
    "\n",
    "# computations\n",
    "for day_of_year in tqdm(list(range(149, day_last)) + [agg_orders['day_of_year'].max()]):\n",
    "                \n",
    "\n",
    "    ###### VALIDAION: TARGET, PROMOTIONS, PRICES\n",
    "        \n",
    "    # day intervals\n",
    "    target_day_min = day_of_year + 1\n",
    "    target_day_max = day_of_year + days_target\n",
    "    \n",
    "    # compute target and promo: labeled data\n",
    "    if day_of_year < agg_orders['day_of_year'].max():\n",
    "        \n",
    "        # target and future promo\n",
    "        tmp_df = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
    "                            (agg_orders['day_of_year'] <= target_day_max)\n",
    "                           ].groupby('itemID')['order', 'promotion'].agg('sum').reset_index()\n",
    "        tmp_df.columns = ['itemID', 'target', 'promo_in_test']\n",
    "        \n",
    "        # future price\n",
    "        tmp_df['mean_price_test'] = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
    "                                               (agg_orders['day_of_year'] <= target_day_max)\n",
    "                                              ].groupby('itemID')['salesPrice'].agg('mean').reset_index()['salesPrice']\n",
    "        \n",
    "        # merge manufacturer and category\n",
    "        tmp_df = tmp_df.merge(items[['itemID', 'manufacturer', 'category']], how = 'left', on = 'itemID')\n",
    "        \n",
    "        # future price per manufacturer\n",
    "        tmp_df_manufacturer = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
    "                                         (agg_orders['day_of_year'] <= target_day_max)\n",
    "                                         ].groupby('manufacturer')['salesPrice'].agg('mean').reset_index()\n",
    "        tmp_df_manufacturer.columns = ['manufacturer', 'mean_price_test_manufacturer']\n",
    "        tmp_df = tmp_df.merge(tmp_df_manufacturer, how = 'left', on = 'manufacturer')\n",
    "        \n",
    "        # future price per category\n",
    "        tmp_df_category = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
    "                                     (agg_orders['day_of_year'] <= target_day_max)\n",
    "                                     ].groupby('category')['salesPrice'].agg('mean').reset_index()\n",
    "        tmp_df_category.columns = ['category', 'mean_price_test_category']\n",
    "        tmp_df = tmp_df.merge(tmp_df_category, how = 'left', on = 'category')\n",
    "        \n",
    "        # future promo per manufacturer\n",
    "        tmp_df_manufacturer = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
    "                                         (agg_orders['day_of_year'] <= target_day_max)\n",
    "                                         ].groupby('manufacturer')['promotion'].agg('sum').reset_index()\n",
    "        tmp_df_manufacturer.columns = ['manufacturer', 'promo_in_test_manufacturer']\n",
    "        tmp_df = tmp_df.merge(tmp_df_manufacturer, how = 'left', on = 'manufacturer')\n",
    "\n",
    "        # future promo per category\n",
    "        tmp_df_category = agg_orders[(agg_orders['day_of_year'] >= target_day_min) &\n",
    "                                     (agg_orders['day_of_year'] <= target_day_max)\n",
    "                                     ].groupby('category')['promotion'].agg('sum').reset_index()\n",
    "        tmp_df_category.columns = ['category', 'promo_in_test_category']\n",
    "        tmp_df = tmp_df.merge(tmp_df_category, how = 'left', on = 'category')\n",
    "                       \n",
    "        \n",
    "    # compute target and promo: unlabeled data\n",
    "    else:\n",
    "        \n",
    "        # placeholders\n",
    "        tmp_df = pd.DataFrame({'itemID':                     items.itemID,\n",
    "                               'target':                     np.nan,\n",
    "                               'promo_in_test':              np.nan,\n",
    "                               'mean_price_test':            items.simulationPrice,\n",
    "                               'manufacturer':               items.manufacturer,\n",
    "                               'category':                   items.category,\n",
    "                               'promo_in_test_manufacturer': np.nan,\n",
    "                               'promo_in_test_category':     np.nan})\n",
    "\n",
    "        \n",
    "    ###### TRAINING: LAG-BASED FEATURES\n",
    "            \n",
    "    # compute features\n",
    "    for day_input in days_input:\n",
    "        \n",
    "        # day intervals\n",
    "        input_day_min  = day_of_year - day_input + 1\n",
    "        input_day_max  = day_of_year\n",
    "    \n",
    "        # frequency, promo and price\n",
    "        tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
    "                                  (agg_orders['day_of_year'] <= input_day_max)\n",
    "                                 ].groupby('itemID')\n",
    "        tmp_df['order_sum_last_'   + str(day_input)] = tmp_df_input['order'].agg('sum').reset_index()['order']\n",
    "        tmp_df['order_count_last_' + str(day_input)] = tmp_df_input['order'].agg(lambda x: len(x[x > 0])).reset_index()['order']\n",
    "        tmp_df['promo_count_last_' + str(day_input)] = tmp_df_input['promotion'].agg('sum').reset_index()['promotion']\n",
    "        tmp_df['mean_price_last_'  + str(day_input)] = tmp_df_input['salesPrice'].agg('mean').reset_index()['salesPrice']\n",
    "\n",
    "        # frequency, promo per manufacturer\n",
    "        tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
    "                                  (agg_orders['day_of_year'] <= input_day_max)\n",
    "                                 ].groupby('manufacturer')\n",
    "        tmp_df_manufacturer = tmp_df_input['order'].agg('sum').reset_index()\n",
    "        tmp_df_manufacturer.columns = ['manufacturer', 'order_manufacturer_sum_last_' + str(day_input)]\n",
    "        tmp_df_manufacturer['order_manufacturer_count_last_' + str(day_input)] = tmp_df_input['order'].agg(lambda x: len(x[x > 0])).reset_index()['order']\n",
    "        tmp_df_manufacturer['promo_manufacturer_count_last_' + str(day_input)] = tmp_df_input['promotion'].agg('sum').reset_index()['promotion']\n",
    "        tmp_df = tmp_df.merge(tmp_df_manufacturer, how = 'left', on = 'manufacturer')\n",
    "    \n",
    "        # frequency, promo per category\n",
    "        tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
    "                                  (agg_orders['day_of_year'] <= input_day_max)\n",
    "                                 ].groupby('category')\n",
    "        tmp_df_category = tmp_df_input['order'].agg('sum').reset_index()\n",
    "        tmp_df_category.columns = ['category', 'order_category_sum_last_' + str(day_input)]       \n",
    "        tmp_df_category['order_category_count_last_' + str(day_input)] = tmp_df_input['order'].agg(lambda x: len(x[x > 0])).reset_index()['order']\n",
    "        tmp_df_category['promo_category_count_last_' + str(day_input)] = tmp_df_input['promotion'].agg('sum').reset_index()['promotion']\n",
    "        tmp_df = tmp_df.merge(tmp_df_category, how = 'left', on = 'category')\n",
    "\n",
    "        # frequency, promo per all items\n",
    "        tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
    "                                  (agg_orders['day_of_year'] <= input_day_max)]\n",
    "        tmp_df['order_all_sum_last_'   + str(day_input)] = tmp_df_input['order'].agg('sum')\n",
    "        tmp_df['order_all_count_last_' + str(day_input)] = tmp_df_input['order'].agg(lambda x: len(x[x > 0]))\n",
    "        tmp_df['promo_all_count_last_' + str(day_input)] = tmp_df_input['promotion'].agg('sum')\n",
    "        \n",
    "        # recency\n",
    "        if day_input == max(days_input):\n",
    "            tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
    "                                      (agg_orders['day_of_year'] <= input_day_max) &\n",
    "                                      (agg_orders['order'] > 0)\n",
    "                                     ].groupby('itemID')\n",
    "            tmp_df['days_since_last_order'] = (day_of_year - tmp_df_input['day_of_year'].agg('max')).reindex(tmp_df.itemID).reset_index()['day_of_year']\n",
    "            tmp_df['days_since_last_order'].fillna(day_input, inplace = True)\n",
    "            \n",
    "            \n",
    "        # tsfresh features\n",
    "        if day_input == max(days_input):\n",
    "            tmp_df_input = agg_orders[(agg_orders['day_of_year'] >= input_day_min) &\n",
    "                                      (agg_orders['day_of_year'] <= input_day_max)]\n",
    "            tmp_df_input = tmp_df_input[['day_of_year', 'itemID', 'order']]\n",
    "            extracted_features = extract_features(tmp_df_input, column_id = 'itemID', column_sort = 'day_of_year')\n",
    "            extracted_features['itemID'] = extracted_features.index\n",
    "            tmp_df = tmp_df.merge(extracted_features, how = 'left', on = 'itemID')\n",
    "            \n",
    "            \n",
    "    ###### FINAL PREPARATIONS\n",
    "            \n",
    "    # add day of year\n",
    "    tmp_df.insert(1, column = 'day_of_year', value = day_of_year)\n",
    "        \n",
    "    # merge data\n",
    "    orders = pd.concat([orders, tmp_df], axis = 0)\n",
    "    \n",
    "    # drop manufacturer and category\n",
    "    del orders['manufacturer']\n",
    "    del orders['category']\n",
    "\n",
    "\n",
    "##### REMOVE MISSINGS\n",
    "\n",
    "good_nas = ['target', \n",
    "            'mean_price_test_category', 'mean_price_test_manufacturer',\n",
    "            'promo_in_test', 'promo_in_test_category', 'promo_in_test_manufacturer']\n",
    "nonas = list(orders.columns[orders.isnull().sum() == 0]) + good_nas\n",
    "orders = orders[nonas]\n",
    "print(orders.shape)\n",
    "\n",
    "\n",
    "##### COMPUTE MEAN PRICE RATIOS\n",
    "\n",
    "print(orders.shape)\n",
    "price_vars = ['mean_price_last_1', 'mean_price_last_7', 'mean_price_last_14', \n",
    "              'mean_price_last_21', 'mean_price_last_28', 'mean_price_last_35']\n",
    "for var in price_vars:\n",
    "    orders['ratio_'              + str(var)] = orders['mean_price_test']              / orders[var]\n",
    "    orders['ratio_manufacturer_' + str(var)] = orders['mean_price_test_manufacturer'] / orders[var]\n",
    "    orders['ratio_category_'     + str(var)] = orders['mean_price_test_category']     / orders[var]\n",
    "print(orders.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature extraction takes about ten hours and outputs a data set with 470 features. Great job!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create some new features in the `items` dataframe, which provides data on item level. We add the following features:\n",
    "- ratio of the actual and recommended price\n",
    "- item category index constructed of three subcategories\n",
    "- customer rating realtive to the average rating of the items of the same manufacturer\n",
    "- customer rating realtive to the average rating of the items of the same category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "\n",
    "# price ratio\n",
    "items['recommended_simulation_price_ratio'] = items['simulationPrice'] / items['recommendedRetailPrice']\n",
    "\n",
    "# detailed item category\n",
    "items['category'] = items['category1'].astype(str) + items['category2'].astype(str) + items['category3'].astype(str)\n",
    "items['category'] = items['category'].astype(int)\n",
    "\n",
    "# customer rating ratio per manufacturer\n",
    "rating_manufacturer = items.groupby('manufacturer')['customerRating'].agg('mean').reset_index()\n",
    "rating_manufacturer.columns = ['manufacturer', 'mean_customerRating_manufacturer']\n",
    "items = items.merge(rating_manufacturer, how = 'left', on = 'manufacturer')\n",
    "items['customerRating_manufacturer_ratio'] = items['customerRating'] / items['mean_customerRating_manufacturer']\n",
    "del items['mean_customerRating_manufacturer']\n",
    "\n",
    "# customer rating ratio per category\n",
    "rating_category = items.groupby('category')['customerRating'].agg('mean').reset_index()\n",
    "rating_category.columns = ['category', 'mean_customerRating_category']\n",
    "items = items.merge(rating_category, how = 'left', on = 'category')\n",
    "items['customerRating_category_ratio'] = items['customerRating'] / items['mean_customerRating_category']\n",
    "del items['mean_customerRating_category']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now merge `orders` and `items`. We also partition the obtained data into the labeled training set and the unlabeled test set, compute some missing features for the test set and export the data as csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved as ../data/prepared/df_v14.csv\n",
      "Saved as ../data/prepared/df_test_v14.csv\n",
      "(1381116, 476)\n",
      "(10463, 476)\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "########## DATA PARTITIONING\n",
    "\n",
    "# merge data\n",
    "df = pd.merge(orders, items, on = 'itemID', how = 'left')\n",
    "\n",
    "# partition intro train and test\n",
    "df_train = df[df['day_of_year'] <  df['day_of_year'].max()]\n",
    "df_test  = df[df['day_of_year'] == df['day_of_year'].max()]\n",
    "\n",
    "\n",
    "########## COMPUTE FEATURES FOR TEST DATA\n",
    "\n",
    "# add promotion info to test\n",
    "promo_vars = df_test.filter(like = 'promotion_').columns\n",
    "df_test['promo_in_test'] = 3 - df_test[promo_vars].isnull().sum(axis = 1)\n",
    "df_test['promo_in_test'].describe()\n",
    "\n",
    "del df_test['promo_in_test_manufacturer'], df_test['promo_in_test_category']\n",
    "\n",
    "# future promo per manufacturer\n",
    "tmp_df_manufacturer = df_test.groupby('manufacturer')['promo_in_test'].agg('sum').reset_index()\n",
    "tmp_df_manufacturer.columns = ['manufacturer', 'promo_in_test_manufacturer']\n",
    "df_test = df_test.merge(tmp_df_manufacturer, how = 'left', on = 'manufacturer')\n",
    "\n",
    "# future promo per category\n",
    "tmp_df_category = df_test.groupby('category')['promo_in_test'].agg('sum').reset_index()\n",
    "tmp_df_category.columns = ['category', 'promo_in_test_category']\n",
    "df_test = df_test.merge(tmp_df_category, how = 'left', on = 'category')\n",
    "\n",
    "del df_test['mean_price_test_manufacturer'], df_test['mean_price_test_category']\n",
    "\n",
    "# future price per manufacturer\n",
    "tmp_df_manufacturer = df_test.groupby('manufacturer')['mean_price_test'].agg('mean').reset_index()\n",
    "tmp_df_manufacturer.columns = ['manufacturer', 'mean_price_test_manufacturer']\n",
    "df_test = df_test.merge(tmp_df_manufacturer, how = 'left', on = 'manufacturer')\n",
    "\n",
    "# future price per category\n",
    "tmp_df_category = df_test.groupby('category')['mean_price_test'].agg('mean').reset_index()\n",
    "tmp_df_category.columns = ['category', 'mean_price_test_category']\n",
    "df_test = df_test.merge(tmp_df_category, how = 'left', on = 'category')\n",
    "\n",
    "# mean price ratios\n",
    "for var in price_vars:\n",
    "    df_test['ratio_'              + str(var)] = df_test['mean_price_test']              / df_test[var]\n",
    "    df_test['ratio_manufacturer_' + str(var)] = df_test['mean_price_test_manufacturer'] / df_test[var]\n",
    "    df_test['ratio_category_'     + str(var)] = df_test['mean_price_test_category']     / df_test[var]\n",
    "\n",
    "\n",
    "########## DROP FEATURES\n",
    "\n",
    "# drop promotion dates\n",
    "df_test.drop(promo_vars,  axis = 1, inplace = True)\n",
    "df_train.drop(promo_vars, axis = 1, inplace = True)\n",
    "\n",
    "# drop mean prices\n",
    "price_vars = price_vars + ['mean_price_test_manufacturer', 'mean_price_test_category']\n",
    "df_test.drop(price_vars,  axis = 1, inplace = True)\n",
    "df_train.drop(price_vars, axis = 1, inplace = True)\n",
    "\n",
    "# export data\n",
    "save_csv_version('../data/prepared/df.csv',      df_train, index = False, compression = 'gzip')\n",
    "save_csv_version('../data/prepared/df_test.csv', df_test,  index = False, compression = 'gzip', min_version = 3)\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mahcine learning encompasses a wide range of statically-inspired performance metrics such as MSE, MAE and others. In practice, machine learning models are used by a company that has specific goals. Usually, these goals can not be expressed in terms of such simple metrics. Therefore, it is important to come up with an evaluation metric that is consistent with the company's objectives to ensure that we judge the model's performance on the criterion that actually matters.\n",
    "\n",
    "In the DMC 2020 task, we are given a profit function of a retailer who is doing demand forecasting. The function accounts for asymmetric error costs: underpredicting demand results in lost revenue because the retailer can not sell a product that is not ready to ship, whereas overpredicting demand incurs a fee for storing the excessive amount of product. Below, we derive the profit function according to the task description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/fig_profit_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the profit function in Python so we can use it to estimate the quality of our solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "\n",
    "def profit(y_true, y_pred, price):\n",
    "    '''\n",
    "    Computes profit according to DMC 2020 task.\n",
    "    \n",
    "    Arguments:\n",
    "    - y_true (numpy array or list): ground truth (correct) target values.\n",
    "    - y_pred (numpy array or list): estimated target values.\n",
    "    - price (numpy array or list): item prices.\n",
    "\n",
    "    Returns:\n",
    "    - profit value\n",
    "    '''\n",
    "\n",
    "    # remove negative and round\n",
    "    y_pred = np.where(y_pred > 0, y_pred, 0)\n",
    "    y_pred = np.round(y_pred).astype('int')\n",
    "\n",
    "    # sold units\n",
    "    units_sold = np.minimum(y_true, y_pred)\n",
    "\n",
    "    # overstocked units\n",
    "    units_overstock = y_pred - y_true\n",
    "    units_overstock[units_overstock < 0] = 0\n",
    "\n",
    "    # profit\n",
    "    revenue = units_sold * price\n",
    "    fee     = units_overstock * price * 0.6\n",
    "    profit  = revenue - fee\n",
    "    profit  = profit.sum()\n",
    "    \n",
    "    # return values\n",
    "    return profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above is great for evaluation. Can we go further and directly optimize it during modeling? \n",
    "\n",
    "We will use LightGBM which supports custom loss functions on training and validation stages. In order to use a custom loss on the the training stage, one needs to define a function with its first and second-order derivative matrices (Gradient and Hessian). Unfortunately, the profit function above is not differentiable. This means that we can not compute derivateves necessary to plug this loss into the model. At the same time, we could come up with a different function that approximates the profit and satisfies the loss conditions. \n",
    "\n",
    "The profit function has several properties:\n",
    "- items with higher prices make larger contributions to the profit\n",
    "- error costs are asymmetric\n",
    "- costs are linear with respect to the prediction error\n",
    "\n",
    "Let's address these points one at a time. First, the account for asymmetric error costs is easy to implement: we can do it on top of the standard MSE loss used in regression. To introduce an asymmetric MSE, we need to compute its derivative. This is simple since we only modify MSE by multipling the error by the overstock fee in case of overpredicting.\n",
    "\n",
    "The snippet below provides two custom asymmetric MSE functions: the first one is used for training the model and the second one is used on validation stage to perform early stopping. The LightGBM requires these two functions to come in a slightly different format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collpase-show\n",
    "\n",
    "##### TRAINING LOSS\n",
    "def asymmetric_mse(y_true, y_pred):\n",
    "    '''\n",
    "    Asymmetric MSE objective for training Lightgbm regressor.\n",
    "    \n",
    "    The asymmetric MSE can be used as a training loss to approximate profit:\n",
    "     - overpredicting demand by one unit decreases profit by 0.6p\n",
    "     - underpredicting demand by one unit decreases profit by p\n",
    "     - hence, overpredicting is 0.6 times less costly\n",
    "     \n",
    "    Arguments:\n",
    "    - y_true (numpy array or list): ground truth (correct) target values.\n",
    "    - y_pred (numpy array or list): estimated target values.\n",
    "    \n",
    "    Returns:\n",
    "    - gradient matrix\n",
    "    - hessian matrix\n",
    "    '''\n",
    "    \n",
    "    # asymmetry parameter\n",
    "    fee_mult = 0.6\n",
    "    \n",
    "    # computations\n",
    "    residual = (y_true - y_pred).astype('float')    \n",
    "    grad = np.where(residual > 0, -2*residual*fee_mult, -2*residual)\n",
    "    hess = np.where(residual > 0,  2*fee_mult, 2.0)\n",
    "    \n",
    "    # return values\n",
    "    return grad, hess\n",
    "\n",
    "\n",
    "##### VALIDATION LOSS\n",
    "def asymmetric_mse_eval(y_true, y_pred):\n",
    "    \n",
    "    '''\n",
    "    Asymmetric MSE evaluation metric for Lightgbm regressor.\n",
    "     \n",
    "    Arguments:\n",
    "    - y_true (numpy array or list): ground truth (correct) target values.\n",
    "    - y_pred (numpy array or list): estimated target values.\n",
    "    \n",
    "    Returns:\n",
    "    - name of the metric\n",
    "    - value od the metric\n",
    "    - whether the metric is maximized\n",
    "    '''\n",
    "    \n",
    "    # asymmetry parameter\n",
    "    fee_mult = 0.6\n",
    "    \n",
    "    # computations\n",
    "    residual = (y_true - y_pred).astype('float')      \n",
    "    loss = np.where(residual > 0, (residual**2)*fee_mult, residual**2) \n",
    "    return 'asymmetric_mse_eval', np.mean(loss), False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we need to account for item prices. It turns out that LightGBM supports weighting of observations on both training and validation stages! This is done using the arguments `sample_weight` and `eval_sample_weight`. By suppling the price vector for training data as `sample_weight` and the price vector for validation data as `eval_sample_weight`, we can incorporate the price information into the performance evaluation. You will see how we specify this in the modeling code in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only missing piece is the relationship between the penalty size and the prediction error. The standard MSE loss takes a square root of the difference between the real and predicted values, which results in penalizing larger errors more than the smaller ones. However, our profit function changes linearly with the error size. \n",
    "\n",
    "A simple way to address this would be to switch to MAE, which stands for the mean absolute error. However, MAE suffers from the same problem as the profit function: it is not differentiable and can only be used for evaluation. \n",
    "\n",
    "What we can do instead is the following:\n",
    "- transform target variable using a non-linear transofmration (e.g. square root or logarithm)\n",
    "- train a model that optimzes the MSE loss on the transformed target variable \n",
    "- apply the inverse transofmration to the model predictions\n",
    "\n",
    "The target transformation smooths out the square effect in the MSE loss. We still penalize large errors more, but the large errors on a transformed scale are also smaller compared to the original scale. This helps to balance out the two effects and approximate a scenario where we want a linear relationship between the error size and the loss penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start building models! First, we extract the target variable as a vector `y` and flag ID features that are not used for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1381116, 475) (1381116,)\n",
      "(10463, 475)\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "\n",
    "# extract target\n",
    "y = df_train['target']\n",
    "X = df_train.drop('target', axis = 1)\n",
    "del df_train\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# format test data\n",
    "X_test = df_test.drop('target', axis = 1)\n",
    "del df_test\n",
    "print(X_test.shape)\n",
    "\n",
    "# relevant features\n",
    "drop_feats = ['itemID', 'day_of_year']\n",
    "features = [var for var in X.columns if var not in drop_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modeling pipeline uses multiple tricks discovered during the model refinement process. We toogle these tricks using logical variables that define the following training options:\n",
    "- `target_transform = True`: transforms a target variabe using a square root transformation to reduce penalty for large errors. This helps to slightly imporve the performance. A detailed motivation for target transofmration is provided in the previous section.\n",
    "- `train_on_positive = False`: trains a model only on cases with positive sales (i.e., at least one of the order lags is greater than 0) and predicts 0 demand for items with no sales. This substantially reduces the training time but also leads to a drop in the performance.\n",
    "- `two_stage = True`: trains a two-stage model: first, we fit a binary classifier predicting whether or not the future sales will be zero. Second, we train a regression model predicting the colume of sales. The final prediction is computed as a multiple of the predictions produced by the two models. In other words, predictions of the regression model are only stored for the cases where the classifier predicts positive sales.\n",
    "- `tuned_params = True`: imports LightGBM hyper-parameter values from the sotred file. The next section provides more detail about the tuning procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the partitioning parameters. We use a sliding window approach with 7 folds, where each subsequent fold is shifted by one day into the past. \n",
    "\n",
    "Let's explain the partitioning using one fold as an example. Given a labeled data set with 180 days, we construct the first fold using days 1 - 173. Within the fold, the first 35 days are cut off: they are only used to compute lag-based features for the days starting from 36. Days 36 - 145 are used for training. For each of these days, we have features based on the previous 35 days and targets based on the next 14 days. On validation stage, we use days 159 - 173. Days 146 - 158 between training and validation periods are skipped to avoid data leakage since the target for these dates would use infomration from the validation period.\n",
    "\n",
    "The partitioning is illustrated below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/fig_partitioning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "\n",
    "##### DATA PARTITIONING\n",
    "\n",
    "# paritioning\n",
    "num_folds = 7   # no. CV folds\n",
    "test_days = 14  # no. days in the test set\n",
    "\n",
    "# settings\n",
    "seed  = 23\n",
    "\n",
    "\n",
    "##### TRAINING OPTIONS\n",
    "\n",
    "# target transformation\n",
    "target_transform = True\n",
    "\n",
    "# train on positive sales only\n",
    "train_on_positive = False\n",
    "\n",
    "# two-stage model\n",
    "two_stage = True\n",
    "\n",
    "# use tuned meta-params\n",
    "tuned_params = True\n",
    "\n",
    "\n",
    "##### CLASSIFIER PARAMETERS\n",
    "\n",
    "# rounds and options\n",
    "cores       = 4\n",
    "stop_rounds = 100\n",
    "verbose     = 100\n",
    "\n",
    "# LGB parameters\n",
    "lgb_params = {\n",
    "    'boosting_type':    'goss',\n",
    "    'objective':        asymmetric_mse,\n",
    "    'metrics':          asymmetric_mse_eval,\n",
    "    'n_estimators':     1000,\n",
    "    'learning_rate':    0.1,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'feature_fraction': 0.8,\n",
    "    'lambda_l1':        0.1,\n",
    "    'lambda_l2':        0.1,\n",
    "    'silent':           True,\n",
    "    'verbosity':        -1,\n",
    "    'nthread' :         cores,\n",
    "    'random_state':     seed,\n",
    "}\n",
    "\n",
    "# load optimal parameters\n",
    "if tuned_params:\n",
    "    par_file   = open('../lgb_meta_params_100.pkl', 'rb')\n",
    "    lgb_params = pickle.load(par_file)\n",
    "    lgb_params['nthread']      = cores\n",
    "    lgb_params['random_state'] = seed\n",
    "\n",
    "# second-stage LGB\n",
    "if two_stage:\n",
    "    lgb_classifier_params              = lgb_params.copy()\n",
    "    lgb_classifier_params['objective'] = 'binary'\n",
    "    lgb_classifier_params['metrics']   = 'logloss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "### HELPER FUNCTION TO POSTPROCESS PREDICTIONS\n",
    "\n",
    "def postprocess_preds(y_pred):\n",
    "    '''\n",
    "    Processess demand predictions outputted by a model.\n",
    "    \n",
    "    Arguments:\n",
    "    - y_pred (numpy array or list): estimated target values.\n",
    "\n",
    "    Returns:\n",
    "    - corrected y_pred\n",
    "    '''\n",
    "\n",
    "    # demand can not be negative\n",
    "    y_pred = np.where(y_pred > 0, y_pred, 0)\n",
    "    \n",
    "    # demand has to be integer\n",
    "    y_pred = np.round(y_pred).astype('int')\n",
    "\n",
    "    # return values\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement the modeling pipeline! We write one master modeling loop. Within the loop, we do the following steps for each of the folds:\n",
    "- extract data from the fold and partition it into training and validation sets\n",
    "- train LightGBM on the training set and perform early stopping on the validation set\n",
    "- save predictions for validation set (denoted as OOF predictions) and predictions for the test set\n",
    "- save variable importance and performance on the validation fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "- train period days: 41 -- 151 (n = 1161393)\n",
      "- valid period days: 166 -- 166 (n = 10463)\n",
      "-----------------------------------------------------------------\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.29915\tvalid_1's binary_logloss: 0.384927\n",
      "[200]\ttraining's binary_logloss: 0.264402\tvalid_1's binary_logloss: 0.356034\n",
      "[300]\ttraining's binary_logloss: 0.252268\tvalid_1's binary_logloss: 0.349405\n",
      "[400]\ttraining's binary_logloss: 0.245571\tvalid_1's binary_logloss: 0.347868\n",
      "[500]\ttraining's binary_logloss: 0.24001\tvalid_1's binary_logloss: 0.347445\n",
      "[600]\ttraining's binary_logloss: 0.235189\tvalid_1's binary_logloss: 0.347438\n",
      "Early stopping, best iteration is:\n",
      "[532]\ttraining's binary_logloss: 0.238417\tvalid_1's binary_logloss: 0.347182\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.98306\ttraining's asymmetric_mse_eval: 2.75817\tvalid_1's rmse: 1.10051\tvalid_1's asymmetric_mse_eval: 3.95162\n",
      "[200]\ttraining's rmse: 0.788328\ttraining's asymmetric_mse_eval: 1.98496\tvalid_1's rmse: 1.00484\tvalid_1's asymmetric_mse_eval: 3.40489\n",
      "[300]\ttraining's rmse: 0.725131\ttraining's asymmetric_mse_eval: 1.78036\tvalid_1's rmse: 0.990361\tvalid_1's asymmetric_mse_eval: 3.32818\n",
      "[400]\ttraining's rmse: 0.687858\ttraining's asymmetric_mse_eval: 1.65306\tvalid_1's rmse: 0.98448\tvalid_1's asymmetric_mse_eval: 3.2921\n",
      "[500]\ttraining's rmse: 0.660717\ttraining's asymmetric_mse_eval: 1.55681\tvalid_1's rmse: 0.983343\tvalid_1's asymmetric_mse_eval: 3.26755\n",
      "[600]\ttraining's rmse: 0.639825\ttraining's asymmetric_mse_eval: 1.48866\tvalid_1's rmse: 0.982914\tvalid_1's asymmetric_mse_eval: 3.26933\n",
      "[700]\ttraining's rmse: 0.623963\ttraining's asymmetric_mse_eval: 1.43483\tvalid_1's rmse: 0.981493\tvalid_1's asymmetric_mse_eval: 3.26372\n",
      "[800]\ttraining's rmse: 0.610448\ttraining's asymmetric_mse_eval: 1.39184\tvalid_1's rmse: 0.982008\tvalid_1's asymmetric_mse_eval: 3.25859\n",
      "Early stopping, best iteration is:\n",
      "[788]\ttraining's rmse: 0.611924\ttraining's asymmetric_mse_eval: 1.39737\tvalid_1's rmse: 0.98004\tvalid_1's asymmetric_mse_eval: 3.25976\n",
      "-----------------------------------------------------------------\n",
      "FOLD 1/7: RMSE = 74.46, PROFIT = 4146664\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "- train period days: 40 -- 150 (n = 1161393)\n",
      "- valid period days: 165 -- 165 (n = 10463)\n",
      "-----------------------------------------------------------------\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.297685\tvalid_1's binary_logloss: 0.385773\n",
      "[200]\ttraining's binary_logloss: 0.263138\tvalid_1's binary_logloss: 0.35656\n",
      "[300]\ttraining's binary_logloss: 0.251309\tvalid_1's binary_logloss: 0.349471\n",
      "[400]\ttraining's binary_logloss: 0.244528\tvalid_1's binary_logloss: 0.347454\n",
      "[500]\ttraining's binary_logloss: 0.239086\tvalid_1's binary_logloss: 0.346844\n",
      "[600]\ttraining's binary_logloss: 0.234168\tvalid_1's binary_logloss: 0.346417\n",
      "[700]\ttraining's binary_logloss: 0.229735\tvalid_1's binary_logloss: 0.346198\n",
      "[800]\ttraining's binary_logloss: 0.225367\tvalid_1's binary_logloss: 0.345921\n",
      "[900]\ttraining's binary_logloss: 0.221212\tvalid_1's binary_logloss: 0.345823\n",
      "[1000]\ttraining's binary_logloss: 0.217269\tvalid_1's binary_logloss: 0.345654\n",
      "[1100]\ttraining's binary_logloss: 0.213563\tvalid_1's binary_logloss: 0.345742\n",
      "Early stopping, best iteration is:\n",
      "[1022]\ttraining's binary_logloss: 0.216471\tvalid_1's binary_logloss: 0.345619\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.98092\ttraining's asymmetric_mse_eval: 2.74525\tvalid_1's rmse: 1.07503\tvalid_1's asymmetric_mse_eval: 3.63975\n",
      "[200]\ttraining's rmse: 0.786559\ttraining's asymmetric_mse_eval: 1.9713\tvalid_1's rmse: 0.978522\tvalid_1's asymmetric_mse_eval: 3.18935\n",
      "[300]\ttraining's rmse: 0.723538\ttraining's asymmetric_mse_eval: 1.76786\tvalid_1's rmse: 0.961131\tvalid_1's asymmetric_mse_eval: 3.12737\n",
      "[400]\ttraining's rmse: 0.687315\ttraining's asymmetric_mse_eval: 1.64682\tvalid_1's rmse: 0.954431\tvalid_1's asymmetric_mse_eval: 3.09221\n",
      "[500]\ttraining's rmse: 0.660826\ttraining's asymmetric_mse_eval: 1.55847\tvalid_1's rmse: 0.953356\tvalid_1's asymmetric_mse_eval: 3.0763\n",
      "[600]\ttraining's rmse: 0.640512\ttraining's asymmetric_mse_eval: 1.49468\tvalid_1's rmse: 0.951799\tvalid_1's asymmetric_mse_eval: 3.07024\n",
      "[700]\ttraining's rmse: 0.625387\ttraining's asymmetric_mse_eval: 1.44799\tvalid_1's rmse: 0.950137\tvalid_1's asymmetric_mse_eval: 3.06467\n",
      "[800]\ttraining's rmse: 0.611541\ttraining's asymmetric_mse_eval: 1.39919\tvalid_1's rmse: 0.94917\tvalid_1's asymmetric_mse_eval: 3.05808\n",
      "[900]\ttraining's rmse: 0.599788\ttraining's asymmetric_mse_eval: 1.36215\tvalid_1's rmse: 0.948618\tvalid_1's asymmetric_mse_eval: 3.05218\n",
      "Early stopping, best iteration is:\n",
      "[872]\ttraining's rmse: 0.603032\ttraining's asymmetric_mse_eval: 1.37071\tvalid_1's rmse: 0.94897\tvalid_1's asymmetric_mse_eval: 3.05009\n",
      "-----------------------------------------------------------------\n",
      "FOLD 2/7: RMSE = 71.46, PROFIT = 4085365\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "- train period days: 39 -- 149 (n = 1161393)\n",
      "- valid period days: 164 -- 164 (n = 10463)\n",
      "-----------------------------------------------------------------\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.296244\tvalid_1's binary_logloss: 0.387062\n",
      "[200]\ttraining's binary_logloss: 0.261733\tvalid_1's binary_logloss: 0.357384\n",
      "[300]\ttraining's binary_logloss: 0.24967\tvalid_1's binary_logloss: 0.350467\n",
      "[400]\ttraining's binary_logloss: 0.242967\tvalid_1's binary_logloss: 0.34917\n",
      "[500]\ttraining's binary_logloss: 0.237436\tvalid_1's binary_logloss: 0.348454\n",
      "[600]\ttraining's binary_logloss: 0.232391\tvalid_1's binary_logloss: 0.348113\n",
      "[700]\ttraining's binary_logloss: 0.227854\tvalid_1's binary_logloss: 0.34821\n",
      "Early stopping, best iteration is:\n",
      "[643]\ttraining's binary_logloss: 0.230494\tvalid_1's binary_logloss: 0.347911\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.978946\ttraining's asymmetric_mse_eval: 2.73178\tvalid_1's rmse: 1.07754\tvalid_1's asymmetric_mse_eval: 3.67104\n",
      "[200]\ttraining's rmse: 0.784647\ttraining's asymmetric_mse_eval: 1.96469\tvalid_1's rmse: 0.977894\tvalid_1's asymmetric_mse_eval: 3.21402\n",
      "[300]\ttraining's rmse: 0.72147\ttraining's asymmetric_mse_eval: 1.76168\tvalid_1's rmse: 0.962791\tvalid_1's asymmetric_mse_eval: 3.15714\n",
      "[400]\ttraining's rmse: 0.684988\ttraining's asymmetric_mse_eval: 1.63817\tvalid_1's rmse: 0.956868\tvalid_1's asymmetric_mse_eval: 3.12548\n",
      "[500]\ttraining's rmse: 0.65739\ttraining's asymmetric_mse_eval: 1.54764\tvalid_1's rmse: 0.956678\tvalid_1's asymmetric_mse_eval: 3.12145\n",
      "[600]\ttraining's rmse: 0.636332\ttraining's asymmetric_mse_eval: 1.47765\tvalid_1's rmse: 0.955408\tvalid_1's asymmetric_mse_eval: 3.10978\n",
      "[700]\ttraining's rmse: 0.619941\ttraining's asymmetric_mse_eval: 1.41646\tvalid_1's rmse: 0.954089\tvalid_1's asymmetric_mse_eval: 3.09487\n",
      "[800]\ttraining's rmse: 0.60524\ttraining's asymmetric_mse_eval: 1.36688\tvalid_1's rmse: 0.954892\tvalid_1's asymmetric_mse_eval: 3.09542\n",
      "Early stopping, best iteration is:\n",
      "[714]\ttraining's rmse: 0.617809\ttraining's asymmetric_mse_eval: 1.40912\tvalid_1's rmse: 0.953991\tvalid_1's asymmetric_mse_eval: 3.09394\n",
      "-----------------------------------------------------------------\n",
      "FOLD 3/7: RMSE = 69.27, PROFIT = 4098194\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "- train period days: 38 -- 148 (n = 1161393)\n",
      "- valid period days: 163 -- 163 (n = 10463)\n",
      "-----------------------------------------------------------------\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.294744\tvalid_1's binary_logloss: 0.389313\n",
      "[200]\ttraining's binary_logloss: 0.260466\tvalid_1's binary_logloss: 0.360484\n",
      "[300]\ttraining's binary_logloss: 0.248455\tvalid_1's binary_logloss: 0.354484\n",
      "[400]\ttraining's binary_logloss: 0.241585\tvalid_1's binary_logloss: 0.352589\n",
      "[500]\ttraining's binary_logloss: 0.235874\tvalid_1's binary_logloss: 0.351815\n",
      "[600]\ttraining's binary_logloss: 0.230981\tvalid_1's binary_logloss: 0.351529\n",
      "[700]\ttraining's binary_logloss: 0.226505\tvalid_1's binary_logloss: 0.351476\n",
      "Early stopping, best iteration is:\n",
      "[676]\ttraining's binary_logloss: 0.227568\tvalid_1's binary_logloss: 0.351372\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.976305\ttraining's asymmetric_mse_eval: 2.72244\tvalid_1's rmse: 1.08443\tvalid_1's asymmetric_mse_eval: 3.6575\n",
      "[200]\ttraining's rmse: 0.782722\ttraining's asymmetric_mse_eval: 1.95227\tvalid_1's rmse: 0.983707\tvalid_1's asymmetric_mse_eval: 3.22189\n",
      "[300]\ttraining's rmse: 0.71861\ttraining's asymmetric_mse_eval: 1.74661\tvalid_1's rmse: 0.968763\tvalid_1's asymmetric_mse_eval: 3.16466\n",
      "[400]\ttraining's rmse: 0.683076\ttraining's asymmetric_mse_eval: 1.6298\tvalid_1's rmse: 0.962707\tvalid_1's asymmetric_mse_eval: 3.13565\n",
      "[500]\ttraining's rmse: 0.655386\ttraining's asymmetric_mse_eval: 1.54154\tvalid_1's rmse: 0.959279\tvalid_1's asymmetric_mse_eval: 3.11773\n",
      "[600]\ttraining's rmse: 0.635611\ttraining's asymmetric_mse_eval: 1.47304\tvalid_1's rmse: 0.959294\tvalid_1's asymmetric_mse_eval: 3.12124\n",
      "Early stopping, best iteration is:\n",
      "[522]\ttraining's rmse: 0.650259\ttraining's asymmetric_mse_eval: 1.52634\tvalid_1's rmse: 0.959465\tvalid_1's asymmetric_mse_eval: 3.11444\n",
      "-----------------------------------------------------------------\n",
      "FOLD 4/7: RMSE = 68.60, PROFIT = 4005400\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "- train period days: 37 -- 147 (n = 1161393)\n",
      "- valid period days: 162 -- 162 (n = 10463)\n",
      "-----------------------------------------------------------------\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.293185\tvalid_1's binary_logloss: 0.393204\n",
      "[200]\ttraining's binary_logloss: 0.25896\tvalid_1's binary_logloss: 0.364377\n",
      "[300]\ttraining's binary_logloss: 0.246946\tvalid_1's binary_logloss: 0.357616\n",
      "[400]\ttraining's binary_logloss: 0.240047\tvalid_1's binary_logloss: 0.356039\n",
      "[500]\ttraining's binary_logloss: 0.234547\tvalid_1's binary_logloss: 0.354523\n",
      "[600]\ttraining's binary_logloss: 0.229447\tvalid_1's binary_logloss: 0.354377\n",
      "Early stopping, best iteration is:\n",
      "[552]\ttraining's binary_logloss: 0.231805\tvalid_1's binary_logloss: 0.354317\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.974592\ttraining's asymmetric_mse_eval: 2.70055\tvalid_1's rmse: 1.06538\tvalid_1's asymmetric_mse_eval: 3.3982\n",
      "[200]\ttraining's rmse: 0.780523\ttraining's asymmetric_mse_eval: 1.93526\tvalid_1's rmse: 0.968067\tvalid_1's asymmetric_mse_eval: 3.01454\n",
      "[300]\ttraining's rmse: 0.716398\ttraining's asymmetric_mse_eval: 1.73404\tvalid_1's rmse: 0.954169\tvalid_1's asymmetric_mse_eval: 2.96665\n",
      "[400]\ttraining's rmse: 0.680153\ttraining's asymmetric_mse_eval: 1.60911\tvalid_1's rmse: 0.949196\tvalid_1's asymmetric_mse_eval: 2.95346\n",
      "[500]\ttraining's rmse: 0.653027\ttraining's asymmetric_mse_eval: 1.52393\tvalid_1's rmse: 0.945873\tvalid_1's asymmetric_mse_eval: 2.94252\n",
      "[600]\ttraining's rmse: 0.632113\ttraining's asymmetric_mse_eval: 1.45636\tvalid_1's rmse: 0.945226\tvalid_1's asymmetric_mse_eval: 2.93626\n",
      "[700]\ttraining's rmse: 0.615705\ttraining's asymmetric_mse_eval: 1.4009\tvalid_1's rmse: 0.944486\tvalid_1's asymmetric_mse_eval: 2.92587\n",
      "[800]\ttraining's rmse: 0.601913\ttraining's asymmetric_mse_eval: 1.35617\tvalid_1's rmse: 0.943612\tvalid_1's asymmetric_mse_eval: 2.91838\n",
      "[900]\ttraining's rmse: 0.589792\ttraining's asymmetric_mse_eval: 1.31152\tvalid_1's rmse: 0.943332\tvalid_1's asymmetric_mse_eval: 2.91615\n",
      "Early stopping, best iteration is:\n",
      "[855]\ttraining's rmse: 0.594887\ttraining's asymmetric_mse_eval: 1.33142\tvalid_1's rmse: 0.943402\tvalid_1's asymmetric_mse_eval: 2.91211\n",
      "-----------------------------------------------------------------\n",
      "FOLD 5/7: RMSE = 62.13, PROFIT = 3947894\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "- train period days: 36 -- 146 (n = 1161393)\n",
      "- valid period days: 161 -- 161 (n = 10463)\n",
      "-----------------------------------------------------------------\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.291614\tvalid_1's binary_logloss: 0.387615\n",
      "[200]\ttraining's binary_logloss: 0.257478\tvalid_1's binary_logloss: 0.359308\n",
      "[300]\ttraining's binary_logloss: 0.245529\tvalid_1's binary_logloss: 0.352512\n",
      "[400]\ttraining's binary_logloss: 0.238613\tvalid_1's binary_logloss: 0.350745\n",
      "[500]\ttraining's binary_logloss: 0.233298\tvalid_1's binary_logloss: 0.350015\n",
      "[600]\ttraining's binary_logloss: 0.228256\tvalid_1's binary_logloss: 0.349602\n",
      "[700]\ttraining's binary_logloss: 0.223616\tvalid_1's binary_logloss: 0.349133\n",
      "[800]\ttraining's binary_logloss: 0.219297\tvalid_1's binary_logloss: 0.349483\n",
      "Early stopping, best iteration is:\n",
      "[711]\ttraining's binary_logloss: 0.223124\tvalid_1's binary_logloss: 0.349099\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.970837\ttraining's asymmetric_mse_eval: 2.68399\tvalid_1's rmse: 1.06976\tvalid_1's asymmetric_mse_eval: 3.56476\n",
      "[200]\ttraining's rmse: 0.777536\ttraining's asymmetric_mse_eval: 1.92887\tvalid_1's rmse: 0.975885\tvalid_1's asymmetric_mse_eval: 3.07571\n",
      "[300]\ttraining's rmse: 0.714503\ttraining's asymmetric_mse_eval: 1.72385\tvalid_1's rmse: 0.964195\tvalid_1's asymmetric_mse_eval: 3.01319\n",
      "[400]\ttraining's rmse: 0.677795\ttraining's asymmetric_mse_eval: 1.60302\tvalid_1's rmse: 0.960126\tvalid_1's asymmetric_mse_eval: 2.98284\n",
      "[500]\ttraining's rmse: 0.650518\ttraining's asymmetric_mse_eval: 1.51493\tvalid_1's rmse: 0.958814\tvalid_1's asymmetric_mse_eval: 2.96858\n",
      "[600]\ttraining's rmse: 0.629813\ttraining's asymmetric_mse_eval: 1.44562\tvalid_1's rmse: 0.957799\tvalid_1's asymmetric_mse_eval: 2.96073\n",
      "Early stopping, best iteration is:\n",
      "[593]\ttraining's rmse: 0.631258\ttraining's asymmetric_mse_eval: 1.4508\tvalid_1's rmse: 0.957476\tvalid_1's asymmetric_mse_eval: 2.95785\n",
      "-----------------------------------------------------------------\n",
      "FOLD 6/7: RMSE = 67.31, PROFIT = 3735558\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "- train period days: 35 -- 145 (n = 1161393)\n",
      "- valid period days: 160 -- 160 (n = 10463)\n",
      "-----------------------------------------------------------------\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's binary_logloss: 0.290004\tvalid_1's binary_logloss: 0.380416\n",
      "[200]\ttraining's binary_logloss: 0.255972\tvalid_1's binary_logloss: 0.349857\n",
      "[300]\ttraining's binary_logloss: 0.244096\tvalid_1's binary_logloss: 0.34236\n",
      "[400]\ttraining's binary_logloss: 0.237444\tvalid_1's binary_logloss: 0.340587\n",
      "[500]\ttraining's binary_logloss: 0.231799\tvalid_1's binary_logloss: 0.339699\n",
      "[600]\ttraining's binary_logloss: 0.226952\tvalid_1's binary_logloss: 0.339069\n",
      "[700]\ttraining's binary_logloss: 0.222406\tvalid_1's binary_logloss: 0.338695\n",
      "[800]\ttraining's binary_logloss: 0.218139\tvalid_1's binary_logloss: 0.338911\n",
      "Early stopping, best iteration is:\n",
      "[711]\ttraining's binary_logloss: 0.22193\tvalid_1's binary_logloss: 0.338637\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 0.968589\ttraining's asymmetric_mse_eval: 2.67085\tvalid_1's rmse: 1.1157\tvalid_1's asymmetric_mse_eval: 3.72032\n",
      "[200]\ttraining's rmse: 0.77563\ttraining's asymmetric_mse_eval: 1.91911\tvalid_1's rmse: 1.0123\tvalid_1's asymmetric_mse_eval: 3.18949\n",
      "[300]\ttraining's rmse: 0.712255\ttraining's asymmetric_mse_eval: 1.71481\tvalid_1's rmse: 0.994281\tvalid_1's asymmetric_mse_eval: 3.09891\n",
      "[400]\ttraining's rmse: 0.675656\ttraining's asymmetric_mse_eval: 1.59486\tvalid_1's rmse: 0.989734\tvalid_1's asymmetric_mse_eval: 3.06705\n",
      "[500]\ttraining's rmse: 0.649052\ttraining's asymmetric_mse_eval: 1.50774\tvalid_1's rmse: 0.988755\tvalid_1's asymmetric_mse_eval: 3.05022\n",
      "[600]\ttraining's rmse: 0.627704\ttraining's asymmetric_mse_eval: 1.4327\tvalid_1's rmse: 0.987653\tvalid_1's asymmetric_mse_eval: 3.0367\n",
      "[700]\ttraining's rmse: 0.611841\ttraining's asymmetric_mse_eval: 1.37598\tvalid_1's rmse: 0.987165\tvalid_1's asymmetric_mse_eval: 3.03409\n",
      "Early stopping, best iteration is:\n",
      "[645]\ttraining's rmse: 0.61979\ttraining's asymmetric_mse_eval: 1.40252\tvalid_1's rmse: 0.987628\tvalid_1's asymmetric_mse_eval: 3.03272\n",
      "-----------------------------------------------------------------\n",
      "FOLD 7/7: RMSE = 74.69, PROFIT = 3951805\n",
      "-----------------------------------------------------------------\n",
      "\n",
      "\n",
      "-----------------------------------------------------------------\n",
      "- AVERAGE RMSE:   69.70\n",
      "- AVERAGE PROFIT: 3995840 (54.16%)\n",
      "- RUNNING TIME:   114.40 minutes\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# collapse-show\n",
    "\n",
    "# placeholders\n",
    "importances   = pd.DataFrame()\n",
    "preds_oof     = np.zeros((num_folds, items.shape[0]))\n",
    "reals_oof     = np.zeros((num_folds, items.shape[0]))\n",
    "prices_oof    = np.zeros((num_folds, items.shape[0]))\n",
    "preds_test    = np.zeros(items.shape[0])\n",
    "oof_rmse      = []\n",
    "oof_profit    = []\n",
    "oracle_profit = []\n",
    "clfs          = []\n",
    "train_idx     = []\n",
    "valid_idx     = []\n",
    "\n",
    "# objects\n",
    "train_days = X['day_of_year'].max() - test_days + 1 - num_folds - X['day_of_year'].min() # no. days in the train set\n",
    "time_start = time.time()\n",
    "\n",
    "# modeling loop\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    ##### PARTITIONING\n",
    "    \n",
    "    # validation dates\n",
    "    if fold == 0:\n",
    "        v_end = X['day_of_year'].max()\n",
    "    else:\n",
    "        v_end = v_end - 1\n",
    "    v_start = v_end\n",
    "    \n",
    "    # training dates\n",
    "    t_end   = v_start - (test_days + 1)\n",
    "    t_start = t_end   - (train_days - 1)\n",
    "    \n",
    "    # extract index\n",
    "    train_idx.append(list(X[(X.day_of_year >= t_start) & (X.day_of_year <= t_end)].index))\n",
    "    valid_idx.append(list(X[(X.day_of_year >= v_start) & (X.day_of_year <= v_end)].index))   \n",
    "    \n",
    "    # extract samples\n",
    "    X_train, y_train = X.iloc[train_idx[fold]][features], y.iloc[train_idx[fold]]\n",
    "    X_valid, y_valid = X.iloc[valid_idx[fold]][features], y.iloc[valid_idx[fold]]\n",
    "    X_test = X_test[features]\n",
    "    \n",
    "    # keep positive cases\n",
    "    if train_on_positive:\n",
    "        y_train = y_train.loc[(X_train['order_sum_last_28'] > 0) | (X_train['promo_in_test'] > 0)]\n",
    "        X_train = X_train.loc[(X_train['order_sum_last_28'] > 0) | (X_train['promo_in_test'] > 0)]\n",
    "\n",
    "    # information\n",
    "    print('-' * 65)\n",
    "    print('- train period days: {} -- {} (n = {})'.format(t_start, t_end, len(train_idx[fold])))\n",
    "    print('- valid period days: {} -- {} (n = {})'.format(v_start, v_end, len(valid_idx[fold])))\n",
    "    print('-' * 65)\n",
    "\n",
    "    \n",
    "    ##### MODELING\n",
    "    \n",
    "    # target transformation\n",
    "    if target_transform:\n",
    "        y_train = np.sqrt(y_train)\n",
    "        y_valid = np.sqrt(y_valid)\n",
    "        \n",
    "    # first stage model\n",
    "    if two_stage:\n",
    "        y_train_binary, y_valid_binary = y_train.copy(), y_valid.copy()\n",
    "        y_train_binary[y_train_binary > 0] = 1\n",
    "        y_valid_binary[y_valid_binary > 0] = 1\n",
    "        clf_classifier = lgb.LGBMClassifier(**lgb_classifier_params) \n",
    "        clf_classifier = clf_classifier.fit(X_train, y_train_binary, \n",
    "                                            eval_set              = [(X_train, y_train_binary), (X_valid, y_valid_binary)],\n",
    "                                            eval_metric           = 'logloss',\n",
    "                                            early_stopping_rounds = stop_rounds,\n",
    "                                            verbose               = verbose)\n",
    "        preds_oof_fold_binary  = clf_classifier.predict(X_valid)\n",
    "        preds_test_fold_binary = clf_classifier.predict(X_test)\n",
    "\n",
    "    # training\n",
    "    clf = lgb.LGBMRegressor(**lgb_params) \n",
    "    clf = clf.fit(X_train, y_train, \n",
    "                  eval_set              = [(X_train, y_train), (X_valid, y_valid)], \n",
    "                  eval_metric           = asymmetric_mse_eval,\n",
    "                  sample_weight         = X_train['simulationPrice'].values,\n",
    "                  eval_sample_weight    = [X_train['simulationPrice'].values, X_valid['simulationPrice'].values],\n",
    "                  early_stopping_rounds = stop_rounds,\n",
    "                  verbose               = verbose)\n",
    "    clfs.append(clf)\n",
    "    \n",
    "    # inference\n",
    "    if target_transform:      \n",
    "        preds_oof_fold  = postprocess_preds(clf.predict(X_valid)**2)\n",
    "        reals_oof_fold  = y_valid**2\n",
    "        preds_test_fold = postprocess_preds(clf.predict(X_test)**2) / num_folds\n",
    "    else:\n",
    "        preds_oof_fold  = postprocess_preds(clf.predict(X_valid))\n",
    "        reals_oof_fold  = y_valid\n",
    "        preds_test_fold = postprocess_preds(clf.predict(X_test)) / num_folds\n",
    "        \n",
    "    # impute zeros\n",
    "    if train_on_positive:\n",
    "        preds_oof_fold[(X_valid['order_sum_last_28'] == 0) & (X_valid['promo_in_test'] == 0)] = 0\n",
    "        preds_test_fold[(X_test['order_sum_last_28'] == 0) & (X_test['promo_in_test']  == 0)] = 0\n",
    "        \n",
    "    # multiply with first stage predictions\n",
    "    if two_stage:\n",
    "        preds_oof_fold  = preds_oof_fold  * np.round(preds_oof_fold_binary)\n",
    "        preds_test_fold = preds_test_fold * np.round(preds_test_fold_binary)\n",
    "\n",
    "    # write predictions\n",
    "    preds_oof[fold, :] = preds_oof_fold\n",
    "    reals_oof[fold, :] = reals_oof_fold\n",
    "    preds_test        += preds_test_fold\n",
    "    \n",
    "    # save prices\n",
    "    prices_oof[fold, :] = X.iloc[valid_idx[fold]]['simulationPrice'].values\n",
    "        \n",
    "        \n",
    "    ##### EVALUATION\n",
    "\n",
    "    # evaluation\n",
    "    oof_rmse.append(np.sqrt(mean_squared_error(reals_oof[fold, :], \n",
    "                                               preds_oof[fold, :])))\n",
    "    oof_profit.append(profit(reals_oof[fold, :], \n",
    "                             preds_oof[fold, :], \n",
    "                             price = X.iloc[valid_idx[fold]]['simulationPrice'].values))\n",
    "    oracle_profit.append(profit(reals_oof[fold, :], \n",
    "                                reals_oof[fold, :], \n",
    "                                price = X.iloc[valid_idx[fold]]['simulationPrice'].values))\n",
    "    \n",
    "    # feature importance\n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df['Feature'] = features\n",
    "    fold_importance_df['Importance'] = clf.feature_importances_\n",
    "    fold_importance_df['Fold'] = fold + 1\n",
    "    importances = pd.concat([importances, fold_importance_df], axis = 0)\n",
    "    \n",
    "    # information\n",
    "    print('-' * 65)\n",
    "    print('FOLD {:d}/{:d}: RMSE = {:.2f}, PROFIT = {:.0f}'.format(fold + 1, \n",
    "                                                                  num_folds, \n",
    "                                                                  oof_rmse[fold], \n",
    "                                                                  oof_profit[fold]))\n",
    "    print('-' * 65)\n",
    "    print('')\n",
    "    \n",
    "\n",
    "# print performance\n",
    "print('')\n",
    "print('-' * 65)\n",
    "print('- AVERAGE RMSE:   {:.2f}'.format(np.mean(oof_rmse)))\n",
    "print('- AVERAGE PROFIT: {:.0f} ({:.2f}%)'.format(np.mean(oof_profit), 100 * np.mean(oof_profit) / np.mean(oracle_profit)))\n",
    "print('- RUNNING TIME:   {:.2f} minutes'.format((time.time() - time_start) / 60))\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good! The modeling loop took us two hours to run. \n",
    "\n",
    "Forecasting demand with our models results in 3,995,840 profit per day, which is about 54% of the maximum possible profit. Let's now visualize the performance of our models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "\n",
    "fig = plt.figure(figsize = (15, 8))\n",
    "\n",
    "# residual plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(reals_oof.reshape(-1), preds_oof.reshape(-1))\n",
    "axis_lim = np.max([reals_oof.max(), preds_oof.max()])\n",
    "plt.ylim(top   = 1.02*axis_lim)\n",
    "plt.xlim(right = 1.02*axis_lim)\n",
    "plt.plot((0, axis_lim), (0, axis_lim), 'r--')\n",
    "plt.title('Residual Plot')\n",
    "plt.ylabel('Predicted demand')\n",
    "plt.xlabel('Actual demand')\n",
    "\n",
    "# feature importance\n",
    "plt.subplot(1, 2, 2)\n",
    "top_feats = 50\n",
    "cols = importances[['Feature', 'Importance']].groupby('Feature').mean().sort_values(by = 'Importance', ascending = False)[0:top_feats].index\n",
    "importance = importances.loc[importances.Feature.isin(cols)]\n",
    "sns.barplot(x = 'Importance', y = 'Feature', data = importance.sort_values(by = 'Importance', ascending = False), ci = 0)\n",
    "plt.title('Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/fig_lgb_perf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatterplot shows that there is a space for further improvement as there are many predictions far from the 45-degree line where predictions and real orders are equal. As expected, the model overpredicts demand more frequently than underpredicting due to the structure of the loss. The important features mostly contain price information followed by features that count the number of previous orders.\n",
    "\n",
    "We can now use predictions stored as `preds_test` to predict future demand. Mission acomplished!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to improve our solution is to optimize the hyper-parameters of the LightGBM model used to make predictions.\n",
    "\n",
    "This section describes how to tune hyper-parameters using the `hyperopt` package, which performs optimization using Tree of Parzen Estimators (TPE) as a search algorithm. You don't really need to know how TPE works. As a user, you are only required to supply the hyper-parameter grid indicating the range of possible values. Compared to standard tuning methods like grid search or random search, TPE explores the search space more efficiently, allowing you to find a suitable solution faster. If you want to read more, please see the package documentation [here](http://hyperopt.github.io/hyperopt/). \n",
    "\n",
    "So, let's specify hyper-parameter ranges! Below, we create a dictionary of LightGBM hyper-parameters using three options:\n",
    "- `hp.choice('name', list_of_values)`: sets hyper-parameter to one of the list values. This is suitable for hyper-parameters that can have multiple distinct values like `boosting_type`.\n",
    "- `hp.uniform('name', min, max)`: sets hyper-paramater to a float value between `min` and `max`. This works well with hyper-parameters such as `learning_rate`.\n",
    "- `hp.quniform('name', min, max, step)`: sets hyper-parameter to a value between `min` and `max` with a step size of `step`. This is useful for integer parameters like `max_depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "\n",
    "# boosting types\n",
    "boost_types = ['gbdt', 'goss']\n",
    "\n",
    "# training params\n",
    "lgb_reg_params = {    \n",
    "    'boosting_type':    hp.choice('boosting_type', boost_types),    \n",
    "    'objective':        'rmse',\n",
    "    'metrics':          'rmse',\n",
    "    'n_estimators':     10000,\n",
    "    'learning_rate':    hp.uniform('learning_rate',  0.0001, 0.3),\n",
    "    'max_depth':        hp.quniform('max_depth',          1,  16, 1),\n",
    "    'num_leaves':       hp.quniform('num_leaves',        10,  64, 1),\n",
    "    'bagging_fraction': hp.uniform('bagging_fraction',  0.3,   1),\n",
    "    'feature_fraction': hp.uniform('feature_fraction',  0.3,   1),\n",
    "    'lambda_l1':        hp.uniform('lambda_l1',           0,   1),\n",
    "    'lambda_l2':        hp.uniform('lambda_l2',           0,   1),\n",
    "    'silent':           True,\n",
    "    'verbosity':        -1,\n",
    "    'nthread' :         4,\n",
    "    'random_state':     77,\n",
    "}\n",
    "\n",
    "# evaluation params\n",
    "lgb_fit_params = {\n",
    "    'eval_metric':           'rmse',\n",
    "    'early_stopping_rounds': 100,\n",
    "    'verbose':               False,\n",
    "}\n",
    "\n",
    "# combine params\n",
    "lgb_space = dict()\n",
    "lgb_space['reg_params'] = lgb_reg_params\n",
    "lgb_space['fit_params'] = lgb_fit_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create an `HPOpt` object that performs tuning given the data. We define three functions: \n",
    "- `process`: runs the optimization process. By default, `HPO` uses `fmin()` to minimize the specified objective.\n",
    "- `lgb_reg`: initializes LighGBM with the specified hyper-parameters.\n",
    "- `train_reg`: trains LightGBM and computes the loss. Since we aim to maximize profit, we simply define loss as negative profit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "\n",
    "class HPOpt(object):\n",
    "\n",
    "    # INIT\n",
    "    def __init__(self, x_train, x_test, y_train, y_test):\n",
    "        self.x_train = x_train\n",
    "        self.x_test  = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test  = y_test\n",
    "\n",
    "    # optimization process\n",
    "    def process(self, fn_name, space, trials, algo, max_evals):\n",
    "        fn = getattr(self, fn_name)\n",
    "        try:\n",
    "            result = fmin(fn        = fn, \n",
    "                          space     = space, \n",
    "                          algo      = algo, \n",
    "                          max_evals = max_evals, \n",
    "                          trials    = trials)\n",
    "        except Exception as e:\n",
    "            return {'status': STATUS_FAIL, 'exception': str(e)}\n",
    "        return result, trials\n",
    "    \n",
    "    \n",
    "    # LGBM INITIALIZATION\n",
    "    def lgb_reg(self, para):\n",
    "        para['reg_params']['max_depth']  = int(para['reg_params']['max_depth'])\n",
    "        para['reg_params']['num_leaves'] = int(para['reg_params']['num_leaves'])\n",
    "        reg = lgb.LGBMRegressor(**para['reg_params'])\n",
    "        return self.train_reg(reg, para)\n",
    "\n",
    "    \n",
    "    # TRAINING AND INFERENCE\n",
    "    def train_reg(self, reg, para):\n",
    "        \n",
    "        # fit LGB\n",
    "        reg.fit(self.x_train, self.y_train,\n",
    "                eval_set              = [(self.x_train, self.y_train), (self.x_test, self.y_test)], \n",
    "                sample_weight         = self.x_train['simulationPrice'].values,\n",
    "                eval_sample_weight    = [self.x_train['simulationPrice'].values, self.x_test['simulationPrice'].values],\n",
    "                **para['fit_params'])\n",
    "        \n",
    "        # inference\n",
    "        if target_transform:      \n",
    "            preds = postprocess_preds(reg.predict(self.x_test)**2)\n",
    "            reals = self.y_test**2\n",
    "        else:\n",
    "            preds = postprocess_preds(reg.predict(self.x_test))\n",
    "            reals = self.y_test\n",
    "        \n",
    "        # impute zeros\n",
    "        if train_on_positive:\n",
    "            preds[(self.x_test['order_sum_last_28'] == 0) & (self.x_test['promo_in_test'] == 0)] = 0\n",
    "\n",
    "        # compute loss [negative profit]\n",
    "        loss = np.round(-profit(reals, preds, price = self.x_test['simulationPrice'].values))\n",
    "                      \n",
    "        return {'loss': loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we aim to use the tuned hyper-parameter values for our model, we would need to perform tuning on a differet subset of data to prevent overfitting. To that end, we will simply go one day in the past compared to the training folds used in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "- train period days: 28 -- 143 (n = 1213708)\n",
      "- valid period days: 158 -- 158 (n = 10463)\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# collapse-hide\n",
    "\n",
    "# validation dates\n",
    "v_end   = 158          # 1 day before last validation fold in code_03_modeling\n",
    "v_start = v_end        # same as v_start\n",
    "\n",
    "# training dates\n",
    "t_start = 28           # first day in the data\n",
    "t_end   = v_start - 15 # validation day - two weeks\n",
    "\n",
    "# extract index\n",
    "train_idx = list(X[(X.day_of_year >= t_start) & (X.day_of_year <= t_end)].index)\n",
    "valid_idx = list(X[(X.day_of_year >= v_start) & (X.day_of_year <= v_end)].index)   \n",
    "\n",
    "# extract samples\n",
    "X_train, y_train = X.iloc[train_idx][features], y.iloc[train_idx]\n",
    "X_valid, y_valid = X.iloc[valid_idx][features], y.iloc[valid_idx]\n",
    "\n",
    "# keep positive cases\n",
    "if train_on_positive:\n",
    "    y_train = y_train.loc[(X_train['order_sum_last_28'] > 0) | (X_train['promo_in_test'] > 0)]\n",
    "    X_train = X_train.loc[(X_train['order_sum_last_28'] > 0) | (X_train['promo_in_test'] > 0)]\n",
    "    \n",
    "# target transformation\n",
    "if target_transform:\n",
    "    y_train = np.sqrt(y_train)\n",
    "    y_valid = np.sqrt(y_valid)\n",
    "\n",
    "# information\n",
    "print('-' * 65)\n",
    "print('- train period days: {} -- {} (n = {})'.format(t_start, t_end, len(train_idx)))\n",
    "print('- valid period days: {} -- {} (n = {})'.format(v_start, v_end, len(valid_idx)))\n",
    "print('-' * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to instantiate the `HPOpt` object and launch the tuning trials! Let's do that and then extract optimal values as a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best meta-parameters:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'goss',\n",
       " 'objective': 'rmse',\n",
       " 'metrics': 'rmse',\n",
       " 'n_estimators': 10000,\n",
       " 'learning_rate': 0.004012417857266637,\n",
       " 'max_depth': 10,\n",
       " 'num_leaves': 64,\n",
       " 'bagging_fraction': 0.9346881591116736,\n",
       " 'feature_fraction': 0.6680768850934483,\n",
       " 'lambda_l1': 0.28013320828944976,\n",
       " 'lambda_l2': 0.5896826524767101,\n",
       " 'silent': True,\n",
       " 'verbosity': -1,\n",
       " 'nthread': 4,\n",
       " 'random_state': 77}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collapse-show\n",
    "\n",
    "# instantiate objects\n",
    "hpo_obj = HPOpt(X_train, X_valid, y_train, y_valid)\n",
    "trials  = Trials() \n",
    "\n",
    "# perform tuning\n",
    "lgb_opt_params = hpo_obj.process(fn_name   = 'lgb_reg',\n",
    "                                 space     = lgb_space, \n",
    "                                 trials    = trials, \n",
    "                                 algo      = tpe.suggest, \n",
    "                                 max_evals = tuning_trials)  \n",
    "\n",
    "# merge best params to fixed params\n",
    "params = list(lgb_opt_params[0].keys())\n",
    "for par_id in range(len(params)):\n",
    "    lgb_reg_params[params[par_id]] = lgb_opt_params[0][params[par_id]]\n",
    "    \n",
    "# postprocess\n",
    "lgb_reg_params['boosting_type'] = boost_types[lgb_reg_params['boosting_type']]\n",
    "lgb_reg_params['max_depth']     = int(lgb_reg_params['max_depth'])\n",
    "lgb_reg_params['num_leaves']    = int(lgb_reg_params['num_leaves'])\n",
    "\n",
    "# print best params\n",
    "print('Best meta-parameters:')\n",
    "lgb_reg_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! Now we can simply save the hyper-parameter values as a `pickle` file and load them in the modeling script when setting up the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collapse-show\n",
    "\n",
    "par_file = open('../lgb_meta_params.pkl', 'wb')\n",
    "pickle.dump(lgb_reg_params, par_file)\n",
    "par_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Closing words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This blogpost has finally come to an end. Thank you for reading! We looked at important stages of our solution and covered steps such as data aggregation, feature engineering, custom loss functions, target transformation and hyper-parameter tuning in detail.\n",
    "\n",
    "Our final solution is an ensemble of multiple LightGBM models with different features and training options that we have developed during the competition. I am not covering the ensembling part in the blogpost, but you can find the blending and stacking codes along with the other notebooks in my [Github repo](https://github.com/kozodoi/DMC_2020).\n",
    "\n",
    "If you liked the task, stay tuned for the next editions of the [Data Mining Cup](https://www.data-mining-cup.com) and feel free to use the comment widnow below to ask questions if something is not clear."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
