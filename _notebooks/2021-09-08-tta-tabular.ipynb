{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Test-Time Augmentation for Tabular Data\n",
    "> Improving predictive performance during inference\n",
    "\n",
    "- toc: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Nikita Kozodoi\n",
    "- categories: [python, structured data, test-time augmentation]\n",
    "- image: images/posts/tta.png\n",
    "- cover: images/covers/tta.png"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Overview\n",
    "\n",
    "Test time augmentation (TTA) is a [popular technique in computer vision](https://towardsdatascience.com/test-time-augmentation-tta-and-how-to-perform-it-with-keras-4ac19b67fb4d\n",
    "). TTA aims at boosting the model accuracy by using data augmentation on the inference stage. The idea behind TTA is simple: for each test image, we create multiple versions that are a little different from the original (e.g., cropped or flipped). Next, we predict labels for the test images and created copies and average model predictions over multiple versions of each image. This usually helps to improve the accuracy irrespective of the underlying model.\n",
    "\n",
    "In many business settings, data comes in a tabular format. Can we use TTA with tabular data to enhance the accuracy of ML models in a way similar to computer vision models? How to define suitable transformations of test cases that do not affect the label? This blog post explores the opportunities for using TTA in tabular data environments. We will implement TTA for `scikit-learn` classifiers and test its performance on multiple credit scoring data sets. The preliminary results indicate that TTA might be a tiny bit helpful in some settings. \n",
    "\n",
    "*Note:* the results presented in this blog post are currently being extended within a scope of a working paper. The post will be updated once the paper is available on ArXiV."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Adapting TTA to tabular data\n",
    "\n",
    "TTA has been originally developed for deep learning applications in computer vision. In contrast to image data, tabular data poses a more challenging environment for using TTA. We will discuss two main challenges that we need to solve to apply TTA to structured data:\n",
    "- how to define transformations?\n",
    "- how to treat categorical features?\n",
    "\n",
    "\n",
    "## 2.1. How to define transformations?\n",
    "\n",
    "When working with image data, light transformations such as rotation, brightness adjustment, saturation and many others modify the underlying pixel values but do not affect the ground truth. That is, a rotated cat is still a cat. We can easily verify this by visually checking the transformed images and limiting the magnitude of transformations to make sure the cat is still recognizable. \n",
    "\n",
    "![](images/fig_cat.jpg)\n",
    "\n",
    "This is different for tabular data, where the underlying features represent different characteristics of the observed subjects. Let's consider a credit scoring example. In finance, banks use ML models to support loan allocation decisions. Consider a binary classification problem, where we predict whether the applicant will pay back the loan. The underlying features may describe the applicant's attributes (age, gender), loan parameters (amount, duration), macroeconomic indicators (inflation, growth). How to do transformations on these features? While there is no such thing as rotating a loan applicant (at least not within the scope of machine learning), we could do a somewhat similar exercise: create copies of each loan applicant and slightly modify feature values for each copy. A good starting point would be to add some random noise to each of the features.\n",
    "\n",
    "This procedure raises a question: how can we be sure that transformations do not alter the label? Would increasing the applicant's age by 10 years affect her repayment ability? Arguably, yes. What about increasing the age by 1 year? Or 1 day? These are challenging questions that we can not answer without more information. This implies that the magnitude of the added noise has to be carefully tuned. We need to take into account the variance of each specific feature as well as the overall data set variability. Adding too little noise will create synthetic cases that are too similar to the original applications, which is not very useful. On the other hand, adding too much noise risks changing the label of the corresponding application, which would harm the model accuracy. The trade-off between these two extremes is what can potentially bring us closer to discovering an accuracy boost.\n",
    "\n",
    "\n",
    "## 2.2. How to treat categorical features?\n",
    "\n",
    "It is rather straightforward to add noise to continuous features such as age or income. However, tabular data frequently contains special gifts: categorical features. From gender to zip code, these features present another challenge for the application of TTA. Adding noise to the zip code appears non-trivial and requires some further thinking. Ignoring categorical features and only altering the continuous ones sounds like an easy solution, but this might not work well on data sets that contain a lot of information in the form of categorical data.\n",
    "\n",
    "In this blog post, we will try a rather naive approach to deal with categorical features. Every categorical feature can be encoded as a set of dummy variables. Next, considering each dummy feature separately, we can occasionally flip the value, switching the person's gender, country of origin or education level with one click. This would introduce some variance in the categorical features and provide TTA with more diverse synthetic applications. This approach is imperfect and can be improved on, but we have to start somewhere, right?\n",
    "\n",
    "Now that we have some ideas about how TTA should work and what are the main challenges, let's actually try to implement it!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Implementing TTA\n",
    "\n",
    "This section implements a helper function `predict_proba_with_tta()` to extend the standard `predict_proba()` method in `scikit-learn` such that predictions take advantage of the TTA procedure. We focus on a binary classification task, but one could easily extend this framework to regression tasks as well.\n",
    "\n",
    "The function `predict_proba_with_tta()` requires specifying the underlying `scikit-learn` model and the test set with observations to be predicted. The function operates in four simple steps:\n",
    "1. Creating `num_tta` copies of the test set.\n",
    "2. Implementing random transformations of the synthetic copies. \n",
    "3. Predicting labels for the real and synthetic observations.\n",
    "4. Aggregating the predictions. \n",
    "\n",
    "Considering the challenges discussed in the previous section, we implement the following transformations for the continuous features:\n",
    "- compute STD of each continuous feature denoted as `std`\n",
    "- generate a random vector `n` using the standard normal distribution\n",
    "- add `alpha * n * std` to each feature , where `alpha` is a meta-parameter. \n",
    "\n",
    "And for the categorical features:\n",
    "- convert categorical features into a set of dummies\n",
    "- flip each dummy variable with a probability `beta`, where `beta` is a meta-parameter.\n",
    "\n",
    "By varying `alpha` and `beta`, we control the transformation magnitude, adjusting the noise scale in the synthetic copies. Higher values imply stronger transformations. The suitable values can be identified through some meta-parameter tuning."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "#collapse-show\n",
    "\n",
    "def predict_proba_with_tta(data, \n",
    "                           model, \n",
    "                           dummies = None, \n",
    "                           num_tta = 4, \n",
    "                           alpha   = 0.01, \n",
    "                           beta    = 0.01, \n",
    "                           seed    = 0):\n",
    "    '''\n",
    "    Predicts class probabilities using TTA.\n",
    "    \n",
    "    Arguments:\n",
    "    - data (numpy array): data set with the feature values \n",
    "    - model (sklearn model): machine learning model\n",
    "    - dummies (list): list of column names of dummy features\n",
    "    - num_tta (integer): number of test-time augmentations\n",
    "    - alpha (float): noise parameter for continuous features\n",
    "    - beta (float): noise parameter for dummy features\n",
    "    - seed (integer): random seed\n",
    "\n",
    "    Returns:\n",
    "    - array of predicted probabilities\n",
    "    '''\n",
    "    \n",
    "    # set random seed\n",
    "    np.random.seed(seed = seed)\n",
    "    \n",
    "    # original prediction\n",
    "    preds = model.predict_proba(data) / (num_tta + 1)\n",
    "     \n",
    "    # select numeric features\n",
    "    num_vars = [var for var in data.columns if data[var].dtype != 'object']\n",
    "        \n",
    "    # find dummies\n",
    "    if dummies != None:\n",
    "        num_vars = list(set(num_vars) - set(dummies))\n",
    "    \n",
    "    # synthetic predictions\n",
    "    for i in range(num_tta):\n",
    "        \n",
    "        # copy data\n",
    "        data_new = data.copy()\n",
    "    \n",
    "        # introduce noise to numeric vars\n",
    "        for var in num_vars:\n",
    "            data_new[var] = data_new[var] + alpha * np.random.normal(0, 1, size = len(data_new)) * data_new[var].std()\n",
    "            \n",
    "        # introduce noise to dummies\n",
    "        if dummies != None:\n",
    "            for var in dummies:\n",
    "                probs = np.random.binomial(1, (1 - beta), size = len(data_new))\n",
    "                data_new.loc[probs == 0, var] = 1 - data_new.loc[probs == 0, var]\n",
    "            \n",
    "        # predict probs\n",
    "        preds_new = model.predict_proba(data_new) \n",
    "        preds    += preds_new / (num_tta + 1)\n",
    "    \n",
    "    # return probs\n",
    "    return preds"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Empirical benchmark\n",
    "\n",
    "Let's test our TTA function! This section performs empirical experiment on multiple data sets to check whether TTA can improve the model performance. First, we import relevant modules and load the list of prepared data sets. All data sets come from a credit scoring environment, which represents a binary classification setup. Some of the data sets are publically available, whereas the others are subject to NDA. The public data sets include [australian](http://archive.ics.uci.edu/ml/datasets/statlog+(australian+credit+approval)), [german](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)), [pakdd](https://www.kdnuggets.com/2010/03/f-pakdd-2010-data-mining-competition.html), [gmsc](https://www.kaggle.com/c/GiveMeSomeCredit), [homecredit](https://www.kaggle.com/c/home-credit-default-risk/) and [lendingclub](https://www.kaggle.com/wendykan/lending-club-loan-data). The sample sizes and the number of features vary greatly across the datasets. This allows us to test the TTA framework in different conditions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "source": [
    "#collapse-hide\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import os\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "#collapse-show\n",
    "datasets = os.listdir('../data')\n",
    "datasets"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['thomas.csv',\n",
       " 'german.csv',\n",
       " 'hmeq.csv',\n",
       " 'bene2.csv',\n",
       " 'lendingclub.csv',\n",
       " 'bene1.csv',\n",
       " 'cashbus.csv',\n",
       " 'uk.csv',\n",
       " 'australian.csv',\n",
       " 'pakdd.csv',\n",
       " 'gmsc.csv',\n",
       " 'paipaidai.csv']"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apart from the data sets, TTA needs an underlying ML model. In our experiment, on each data set, we will use a Random Forest classifier with 500 trees, which is a good trade-off between good performance and computational resources. We will not go deep into tuning the classifier and keep the parameters fixed for all data sets. We will then use stratified 5-fold cross-validation to train and test models with and without TTA."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "#collapse-show\n",
    "\n",
    "# classifier\n",
    "clf = RandomForestClassifier(n_estimators = 500, random_state = 1, n_jobs = 4)\n",
    "\n",
    "# settings\n",
    "folds = StratifiedKFold(n_splits     = 5, \n",
    "                        shuffle      = True, \n",
    "                        random_state = 23)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The cell below implements the following experiment:\n",
    "1. We loop through the datasets and perform cross-validation, training Random Forest on each fold combination. \n",
    "2. Next, we predict labels of the validation cases and calculate the AUC of the model predictions. This is our benchmark.\n",
    "3. We predict labels of the validation cases with the same model but now implement TTA to adjust the predictions.  \n",
    "4. By comparing the average AUC difference before and after TTA, we can judge whether TTA actually helps to boost the predictive performance."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "source": [
    "#collapse-show\n",
    "\n",
    "# placeholders\n",
    "auc_change = []\n",
    "\n",
    "# timer\n",
    "start = time.time()\n",
    "\n",
    "# modeling loop\n",
    "for data in datasets:\n",
    "\n",
    "    ##### DATA PREPARATION\n",
    "\n",
    "    # import data\n",
    "    X = pd.read_csv('../data/' + data)\n",
    "\n",
    "    # convert target to integer\n",
    "    X.loc[X.BAD == 'BAD',  'BAD'] = 1\n",
    "    X.loc[X.BAD == 'GOOD', 'BAD'] = 0\n",
    "\n",
    "    # extract X and y\n",
    "    y = X['BAD']\n",
    "    del X['BAD']\n",
    "\n",
    "    # create dummies\n",
    "    X = pd.get_dummies(X, prefix_sep = '_dummy_')\n",
    "\n",
    "    # data information\n",
    "    print('-------------------------------------')\n",
    "    print('Dataset:', data, X.shape)\n",
    "    print('-------------------------------------')\n",
    "\n",
    "    \n",
    "    ##### CROSS-VALIDATION\n",
    "    \n",
    "    # create objects\n",
    "    oof_preds_raw = np.zeros((len(X), y.nunique()))\n",
    "    oof_preds_tta = np.zeros((len(X), y.nunique()))\n",
    "\n",
    "    # modeling loop\n",
    "    for fold_, (trn_, val_) in enumerate(folds.split(y, y)):\n",
    "\n",
    "        # data partitioning\n",
    "        trn_x, trn_y = X.iloc[trn_], y.iloc[trn_]\n",
    "        val_x, val_y = X.iloc[val_], y.iloc[val_]\n",
    "\n",
    "        # train the model\n",
    "        clf.fit(trn_x, trn_y)\n",
    "        \n",
    "        # identify dummies\n",
    "        dummies = list(X.filter(like = '_dummy_').columns)\n",
    "\n",
    "        # predictions\n",
    "        oof_preds_raw[val_, :] =  clf.predict_proba(val_x)\n",
    "        oof_preds_tta[val_, :] =  predict_proba_with_tta(data    = val_x, \n",
    "                                                         model   = clf, \n",
    "                                                         dummies = dummies,\n",
    "                                                         num_tta = 5, \n",
    "                                                         alpha   = np.sqrt(len(trn_x)) / 3000,\n",
    "                                                         beta    = np.sqrt(len(trn_x)) / 30000,\n",
    "                                                         seed    = 1)\n",
    "\n",
    "    # print performance\n",
    "    print('- AUC before TTA = %.6f ' % roc_auc_score(y, oof_preds_raw[:,1]))\n",
    "    print('- AUC with TTA   = %.6f ' % roc_auc_score(y, oof_preds_tta[:,1]))\n",
    "    print('-------------------------------------')\n",
    "    print('')\n",
    "    \n",
    "    # save the AUC delta\n",
    "    delta = roc_auc_score(y, oof_preds_tta[:,1]) - roc_auc_score(y, oof_preds_raw[:,1])\n",
    "    auc_change.append(delta)\n",
    "\n",
    "# display results\n",
    "print('-------------------------------------')\n",
    "print('Finished in %.1f minutes' % ((time.time() - start) / 60))\n",
    "print('-------------------------------------')\n",
    "print('TTA improves AUC in %.0f/%.0f cases' % (np.sum(np.array(auc_change) > 0), len(datasets)))\n",
    "print('Mean AUC change = %.6f' % np.mean(auc_change))\n",
    "print('-------------------------------------')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-------------------------------------\n",
      "Dataset: thomas.csv (1225, 28)\n",
      "-------------------------------------\n",
      "- AUC before TTA = 0.612322 \n",
      "- AUC with TTA   = 0.613617 \n",
      "-------------------------------------\n",
      "\n",
      "-------------------------------------\n",
      "Dataset: german.csv (1000, 61)\n",
      "-------------------------------------\n",
      "- AUC before TTA = 0.796233 \n",
      "- AUC with TTA   = 0.796300 \n",
      "-------------------------------------\n",
      "\n",
      "-------------------------------------\n",
      "Dataset: hmeq.csv (5960, 20)\n",
      "-------------------------------------\n",
      "- AUC before TTA = 0.975995 \n",
      "- AUC with TTA   = 0.976805 \n",
      "-------------------------------------\n",
      "\n",
      "-------------------------------------\n",
      "Dataset: bene2.csv (7190, 28)\n",
      "-------------------------------------\n",
      "- AUC before TTA = 0.801193 \n",
      "- AUC with TTA   = 0.799387 \n",
      "-------------------------------------\n",
      "\n",
      "-------------------------------------\n",
      "Dataset: lendingclub.csv (43344, 114)\n",
      "-------------------------------------\n",
      "- AUC before TTA = 0.625029 \n",
      "- AUC with TTA   = 0.628207 \n",
      "-------------------------------------\n",
      "\n",
      "-------------------------------------\n",
      "Dataset: bene1.csv (3123, 84)\n",
      "-------------------------------------\n",
      "- AUC before TTA = 0.788607 \n",
      "- AUC with TTA   = 0.789447 \n",
      "-------------------------------------\n",
      "\n",
      "-------------------------------------\n",
      "Dataset: cashbus.csv (15000, 642)\n",
      "-------------------------------------\n",
      "- AUC before TTA = 0.629648 \n",
      "- AUC with TTA   = 0.624874 \n",
      "-------------------------------------\n",
      "\n",
      "-------------------------------------\n",
      "Dataset: uk.csv (30000, 51)\n",
      "-------------------------------------\n",
      "- AUC before TTA = 0.712042 \n",
      "- AUC with TTA   = 0.723359 \n",
      "-------------------------------------\n",
      "\n",
      "-------------------------------------\n",
      "Dataset: australian.csv (690, 42)\n",
      "-------------------------------------\n",
      "- AUC before TTA = 0.931787 \n",
      "- AUC with TTA   = 0.931958 \n",
      "-------------------------------------\n",
      "\n",
      "-------------------------------------\n",
      "Dataset: pakdd.csv (50000, 373)\n",
      "-------------------------------------\n",
      "- AUC before TTA = 0.620081 \n",
      "- AUC with TTA   = 0.623080 \n",
      "-------------------------------------\n",
      "\n",
      "-------------------------------------\n",
      "Dataset: gmsc.csv (150000, 68)\n",
      "-------------------------------------\n",
      "- AUC before TTA = 0.846187 \n",
      "- AUC with TTA   = 0.855176 \n",
      "-------------------------------------\n",
      "\n",
      "-------------------------------------\n",
      "Dataset: paipaidai.csv (60000, 1934)\n",
      "-------------------------------------\n",
      "- AUC before TTA = 0.716398 \n",
      "- AUC with TTA   = 0.721679 \n",
      "-------------------------------------\n",
      "\n",
      "-------------------------------------\n",
      "Finished in 206.1 minutes\n",
      "-------------------------------------\n",
      "TTA improves AUC in 10/12 cases\n",
      "Mean AUC change = 0.002364\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "metadata": {
    "_uuid": "4e5e1ed34e0887b3c10b8e649abef15f44a34853",
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Looks like TTA is working! Overall, TTA improves the AUC in 10 out of 12 data sets. The observed performance gains are rather small: on average, TTA improves AUC by `0.00236`. The results are visualized in the barplot below: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#collapse-hide\n",
    "\n",
    "objects = list(range(len(datasets)))\n",
    "y_pos   = np.arange(len(objects))\n",
    "perf    = np.sort(auc_change2)\n",
    "\n",
    "plt.figure(figsize = (6, 8))\n",
    "plt.barh(y_pos, perf, align = 'center', color = 'blue', alpha = 0.66)\n",
    "\n",
    "plt.ylabel('Dataset')\n",
    "plt.yticks(y_pos, objects)\n",
    "plt.xlabel('AUC Gain')\n",
    "plt.title('')\n",
    "ax.plot([0, 0], [1, 12], 'k--')\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](images/fig_tta_gains.jpg)\n",
    "\n",
    "We should bear in mind that performance gains, although appearing rather small, come almost \"for free\". We don't need to train a new model and only require a relatively small amount of extra resources to create synthetic copies of the loan applications. Sounds good!\n",
    "\n",
    "It is possible that further fine-tuning of the TTA meta-parameters can uncover larger performance gains. Furthermore, a considerable variance of the average gains from TTA across the data sets indicates that TTA can be more helpful in specific settings. The important factors influencing the TTA performance may relate to both the data and the classifier used to produce predictions. More research is needed to identify and analyze such factors."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Closing words\n",
    "\n",
    "The purpose of this tutorial was to explore TTA applications for tabular data. We have discussed the corresponding challenges, developed a TTA wrapper function for `scikit-learn` and demonstrated that it could indeed be helpful on multiple credit scoring data sets. I hope you found this post interesting.\n",
    "\n",
    "The project described in this blog post is a work in progress. I will update the post once the working paper on the usage of TTA for tabular data is available. Stay tuned and happy learning!"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}