{
  
    
        "post0": {
            "title": "3D-UNET with BRATS-2020 Dataset",
            "content": "This post was originally published on my github, at here .",
            "url": "https://kozodoi.me/_notebook/20241117/3D-UNET",
            "relUrl": "/_notebook/20241117/3D-UNET",
            "date": " • Nov 17, 2024"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "import os import cv2 import glob import PIL import shutil import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt from skimage import data from skimage.util import montage import skimage.transform as skTrans from skimage.transform import rotate from skimage.transform import resize from PIL import Image, ImageOps import nilearn as nl import nibabel as nib import nilearn.plotting as nlplt !pip install git+https://github.com/miykael/gif_your_nifti # nifti to gif import gif_your_nifti.core as gif2nif import keras import keras.backend as K from keras.callbacks import CSVLogger import tensorflow as tf from tensorflow.keras.utils import plot_model from sklearn.preprocessing import MinMaxScaler from sklearn.model_selection import train_test_split from sklearn.metrics import classification_report from tensorflow.keras.models import * from tensorflow.keras.layers import * from tensorflow.keras.optimizers import * from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard np.set_printoptions(precision=3, suppress=True) . Collecting git+https://github.com/miykael/gif_your_nifti Cloning https://github.com/miykael/gif_your_nifti to /tmp/pip-req-build-rz8isjkt Running command git clone --filter=blob:none --quiet https://github.com/miykael/gif_your_nifti /tmp/pip-req-build-rz8isjkt Resolved https://github.com/miykael/gif_your_nifti to commit 55c09c42921f4871cf43f63a303ab420a92a4f03 Preparing metadata (setup.py) ... done Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from gif_your_nifti==0.2.2) (1.26.4) Requirement already satisfied: nibabel in /opt/conda/lib/python3.10/site-packages (from gif_your_nifti==0.2.2) (5.2.1) Requirement already satisfied: imageio&lt;3 in /opt/conda/lib/python3.10/site-packages (from gif_your_nifti==0.2.2) (2.34.1) Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from gif_your_nifti==0.2.2) (3.7.5) Requirement already satisfied: scikit-image in /opt/conda/lib/python3.10/site-packages (from gif_your_nifti==0.2.2) (0.23.2) Requirement already satisfied: pillow&gt;=8.3.2 in /opt/conda/lib/python3.10/site-packages (from imageio&lt;3-&gt;gif_your_nifti==0.2.2) (10.3.0) Requirement already satisfied: contourpy&gt;=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib-&gt;gif_your_nifti==0.2.2) (1.2.1) Requirement already satisfied: cycler&gt;=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib-&gt;gif_your_nifti==0.2.2) (0.12.1) Requirement already satisfied: fonttools&gt;=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib-&gt;gif_your_nifti==0.2.2) (4.53.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib-&gt;gif_your_nifti==0.2.2) (1.4.5) Requirement already satisfied: packaging&gt;=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib-&gt;gif_your_nifti==0.2.2) (21.3) Requirement already satisfied: pyparsing&gt;=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib-&gt;gif_your_nifti==0.2.2) (3.1.2) Requirement already satisfied: python-dateutil&gt;=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib-&gt;gif_your_nifti==0.2.2) (2.9.0.post0) Requirement already satisfied: scipy&gt;=1.9 in /opt/conda/lib/python3.10/site-packages (from scikit-image-&gt;gif_your_nifti==0.2.2) (1.14.1) Requirement already satisfied: networkx&gt;=2.8 in /opt/conda/lib/python3.10/site-packages (from scikit-image-&gt;gif_your_nifti==0.2.2) (3.3) Requirement already satisfied: tifffile&gt;=2022.8.12 in /opt/conda/lib/python3.10/site-packages (from scikit-image-&gt;gif_your_nifti==0.2.2) (2024.5.22) Requirement already satisfied: lazy-loader&gt;=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image-&gt;gif_your_nifti==0.2.2) (0.4) Requirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil&gt;=2.7-&gt;matplotlib-&gt;gif_your_nifti==0.2.2) (1.16.0) Building wheels for collected packages: gif_your_nifti Building wheel for gif_your_nifti (setup.py) ... - done Created wheel for gif_your_nifti: filename=gif_your_nifti-0.2.2-py3-none-any.whl size=6633 sha256=d54ed811fe56e8790dc8ae864a4b756a9917d6f71d387a9d8b2f1dfc1939913c Stored in directory: /tmp/pip-ephem-wheel-cache-757h3wwa/wheels/3a/c2/0b/c08f2425925519bb014e107d2919dadc2556ec5e7c205e4472 Successfully built gif_your_nifti Installing collected packages: gif_your_nifti Successfully installed gif_your_nifti-0.2.2 . SEGMENT_CLASSES = { 0 : &#39;NOT tumor&#39;, 1 : &#39;NECROTIC/CORE&#39;, 2 : &#39;EDEMA&#39;, 3 : &#39;ENHANCING&#39; } VOLUME_SLICES = 100 VOLUME_START_AT = 22 . TRAIN_DATASET_PATH = &#39;../input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/&#39; VALIDATION_DATASET_PATH = &#39;../input/brats20-dataset-training-validation/BraTS2020_ValidationData/MICCAI_BraTS2020_ValidationData&#39; test_image_flair=nib.load(TRAIN_DATASET_PATH + &#39;BraTS20_Training_001/BraTS20_Training_001_flair.nii&#39;).get_fdata() test_image_t1=nib.load(TRAIN_DATASET_PATH + &#39;BraTS20_Training_001/BraTS20_Training_001_t1.nii&#39;).get_fdata() test_image_t1ce=nib.load(TRAIN_DATASET_PATH + &#39;BraTS20_Training_001/BraTS20_Training_001_t1ce.nii&#39;).get_fdata() test_image_t2=nib.load(TRAIN_DATASET_PATH + &#39;BraTS20_Training_001/BraTS20_Training_001_t2.nii&#39;).get_fdata() test_mask=nib.load(TRAIN_DATASET_PATH + &#39;BraTS20_Training_001/BraTS20_Training_001_seg.nii&#39;).get_fdata() fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize = (20, 10)) slice_w = 25 ax1.imshow(test_image_flair[:,:,test_image_flair.shape[0]//2-slice_w], cmap = &#39;gray&#39;) ax1.set_title(&#39;Image flair&#39;) ax2.imshow(test_image_t1[:,:,test_image_t1.shape[0]//2-slice_w], cmap = &#39;gray&#39;) ax2.set_title(&#39;Image t1&#39;) ax3.imshow(test_image_t1ce[:,:,test_image_t1ce.shape[0]//2-slice_w], cmap = &#39;gray&#39;) ax3.set_title(&#39;Image t1ce&#39;) ax4.imshow(test_image_t2[:,:,test_image_t2.shape[0]//2-slice_w], cmap = &#39;gray&#39;) ax4.set_title(&#39;Image t2&#39;) ax5.imshow(test_mask[:,:,test_mask.shape[0]//2-slice_w]) ax5.set_title(&#39;Mask&#39;) . Text(0.5, 1.0, &#39;Mask&#39;) . fig, ax1 = plt.subplots(1, 1, figsize = (15,15)) ax1.imshow(rotate(montage(test_image_t1[50:-50,:,:]), 90, resize=True), cmap =&#39;gray&#39;) . &lt;matplotlib.image.AxesImage at 0x7dbc20f61de0&gt; . niimg = nl.image.load_img(TRAIN_DATASET_PATH + &#39;BraTS20_Training_001/BraTS20_Training_001_flair.nii&#39;) nimask = nl.image.load_img(TRAIN_DATASET_PATH + &#39;BraTS20_Training_001/BraTS20_Training_001_seg.nii&#39;) fig, axes = plt.subplots(nrows=4, figsize=(30, 40)) nlplt.plot_anat(niimg, title=&#39;BraTS20_Training_001_flair.nii plot_anat&#39;, axes=axes[0]) nlplt.plot_epi(niimg, title=&#39;BraTS20_Training_001_flair.nii plot_epi&#39;, axes=axes[1]) nlplt.plot_img(niimg, title=&#39;BraTS20_Training_001_flair.nii plot_img&#39;, axes=axes[2]) nlplt.plot_roi(nimask, title=&#39;BraTS20_Training_001_flair.nii with mask plot_roi&#39;, bg_img=niimg, axes=axes[3], cmap=&#39;Paired&#39;) plt.show() . IMG_SIZE=128 . import tensorflow as tf from tensorflow.keras import backend as K def dice_coef(y_true, y_pred, smooth=1.0): class_num = 4 total_dice = 0 for i in range(class_num): y_true_f = tf.reshape(y_true[:,:,:,i], [-1]) y_pred_f = tf.reshape(y_pred[:,:,:,i], [-1]) intersection = tf.reduce_sum(y_true_f * y_pred_f) dice = (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth) total_dice += dice return total_dice / class_num def dice_coef_necrotic(y_true, y_pred, smooth=1.0): y_true_f = tf.reshape(y_true[:,:,:,1], [-1]) y_pred_f = tf.reshape(y_pred[:,:,:,1], [-1]) intersection = tf.reduce_sum(y_true_f * y_pred_f) return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth) def dice_coef_edema(y_true, y_pred, smooth=1.0): y_true_f = tf.reshape(y_true[:,:,:,2], [-1]) y_pred_f = tf.reshape(y_pred[:,:,:,2], [-1]) intersection = tf.reduce_sum(y_true_f * y_pred_f) return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth) def dice_coef_enhancing(y_true, y_pred, smooth=1.0): y_true_f = tf.reshape(y_true[:,:,:,3], [-1]) y_pred_f = tf.reshape(y_pred[:,:,:,3], [-1]) intersection = tf.reduce_sum(y_true_f * y_pred_f) return (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth) def precision(y_true, y_pred): true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1))) predicted_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_pred, 0, 1))) precision = true_positives / (predicted_positives + K.epsilon()) return precision def sensitivity(y_true, y_pred): true_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true * y_pred, 0, 1))) possible_positives = tf.reduce_sum(tf.round(tf.clip_by_value(y_true, 0, 1))) return true_positives / (possible_positives + K.epsilon()) def specificity(y_true, y_pred): true_negatives = tf.reduce_sum(tf.round(tf.clip_by_value((1-y_true) * (1-y_pred), 0, 1))) possible_negatives = tf.reduce_sum(tf.round(tf.clip_by_value(1-y_true, 0, 1))) return true_negatives / (possible_negatives + K.epsilon()) . def build_unet(inputs, ker_init, dropout): conv1 = Conv3D(32, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(inputs) conv1 = Conv3D(32, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(conv1) pool = MaxPooling3D(pool_size=(2, 2, 2))(conv1) conv = Conv3D(64, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(pool) conv = Conv3D(64, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(conv) pool1 = MaxPooling3D(pool_size=(2, 2, 2))(conv) conv2 = Conv3D(128, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(pool1) conv2 = Conv3D(128, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(conv2) pool2 = MaxPooling3D(pool_size=(2, 2, 2))(conv2) conv3 = Conv3D(256, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(pool2) conv3 = Conv3D(256, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(conv3) pool4 = MaxPooling3D(pool_size=(2, 2, 2))(conv3) conv5 = Conv3D(512, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(pool4) conv5 = Conv3D(512, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(conv5) drop5 = Dropout(dropout)(conv5) up7 = Conv3D(256, (2,2,2), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(UpSampling3D(size = (2,2,2))(drop5)) merge7 = concatenate([conv3,up7]) conv7 = Conv3D(256, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(merge7) conv7 = Conv3D(256, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(conv7) up8 = Conv3D(128, (2,2,2), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(UpSampling3D(size = (2,2,2))(conv7)) merge8 = concatenate([conv2,up8]) conv8 = Conv3D(128, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(merge8) conv8 = Conv3D(128, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(conv8) up9 = Conv3D(64, (2,2,2), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(UpSampling3D(size = (2,2,2))(conv8)) merge9 = concatenate([conv,up9]) conv9 = Conv3D(64, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(merge9) conv9 = Conv3D(64, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(conv9) up = Conv3D(32, (2,2,2), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(UpSampling3D(size = (2,2,2))(conv9)) merge = concatenate([conv1,up]) conv = Conv3D(32, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(merge) conv = Conv3D(32, (3, 3, 3), activation = &#39;relu&#39;, padding = &#39;same&#39;, kernel_initializer = ker_init)(conv) conv10 = Conv3D(4, (1,1,1), activation = &#39;softmax&#39;)(conv) return Model(inputs = inputs, outputs = conv10) input_layer = Input((IMG_SIZE, IMG_SIZE, IMG_SIZE, 3)) model = build_unet(input_layer, &#39;he_normal&#39;, 0.2) model.compile(loss=&quot;categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics = [&#39;accuracy&#39;,tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema ,dice_coef_enhancing] ) . # Recompile the model with updated metrics input_layer = Input((IMG_SIZE, IMG_SIZE, IMG_SIZE, 3)) model = build_unet(input_layer, &#39;he_normal&#39;, 0.2) model.compile( loss=&quot;categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=[ &#39;accuracy&#39;, tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema, dice_coef_enhancing ] ) . plot_model(model, show_shapes = True, show_dtype=False, show_layer_names = True, rankdir = &#39;TB&#39;, expand_nested = False, dpi = 70) . train_and_val_directories = [f.path for f in os.scandir(TRAIN_DATASET_PATH) if f.is_dir()] train_and_val_directories.remove(TRAIN_DATASET_PATH+&#39;BraTS20_Training_355&#39;) def pathListIntoIds(dirList): x = [] for i in range(0,len(dirList)): x.append(dirList[i][dirList[i].rfind(&#39;/&#39;)+1:]) return x train_and_test_ids = pathListIntoIds(train_and_val_directories); train_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2) train_ids, test_ids = train_test_split(train_test_ids,test_size=0.15) . import numpy as np import tensorflow as tf import keras import cv2 import nibabel as nib import os class DataGenerator(keras.utils.Sequence): def __init__(self, list_IDs, batch_size=1, dim=(240, 240, 155), n_channels=3, shuffle=True, data_path=&#39;&#39;): &quot;&quot;&quot;Initialize DataGenerator. Args: list_IDs: List of sample IDs batch_size: Size of each batch dim: Dimensions of input image n_channels: Number of input channels shuffle: Whether to shuffle data between epochs data_path: Base path to dataset &quot;&quot;&quot; super().__init__() self.dim = dim self.batch_size = batch_size self.list_IDs = list_IDs self.n_channels = n_channels self.shuffle = shuffle self.data_path = data_path self.on_epoch_end() def on_epoch_end(self): &quot;&quot;&quot;Updates indexes after each epoch.&quot;&quot;&quot; self.indexes = np.arange(len(self.list_IDs)) if self.shuffle: np.random.shuffle(self.indexes) def __len__(self): &quot;&quot;&quot;Returns the number of batches per epoch.&quot;&quot;&quot; return int(np.floor(len(self.list_IDs) / self.batch_size)) def __getitem__(self, index): &quot;&quot;&quot;Generate one batch of data.&quot;&quot;&quot; # Generate indexes of the batch indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size] # Find list of IDs list_IDs_temp = [self.list_IDs[k] for k in indexes] # Generate data X, y = self.__data_generation(list_IDs_temp) return X, y def __data_generation(self, batch_ids): &quot;&quot;&quot;Generates data containing batch_size samples.&quot;&quot;&quot; X = np.zeros((self.batch_size, *self.dim, self.n_channels), dtype=np.float32) y = np.zeros((self.batch_size, *self.dim, 4), dtype=np.float32) for c, ID in enumerate(batch_ids): case_path = os.path.join(self.data_path, ID) # Load the different modalities flair = nib.load(os.path.join(case_path, f&#39;{ID}_flair.nii&#39;)).get_fdata() t1ce = nib.load(os.path.join(case_path, f&#39;{ID}_t1ce.nii&#39;)).get_fdata() t1 = nib.load(os.path.join(case_path, f&#39;{ID}_t1.nii&#39;)).get_fdata() seg = nib.load(os.path.join(case_path, f&#39;{ID}_seg.nii&#39;)).get_fdata() # Resize and normalize each slice for j in range(self.dim[2]): # Assuming dim[2] is the number of slices X[c, :, :, j, 0] = cv2.resize(flair[:, :, j], (self.dim[0], self.dim[1])) X[c, :, :, j, 1] = cv2.resize(t1ce[:, :, j], (self.dim[0], self.dim[1])) X[c, :, :, j, 2] = cv2.resize(t1[:, :, j], (self.dim[0], self.dim[1])) # Handle segmentation mask temp_seg = cv2.resize(seg[:, :, j], (self.dim[0], self.dim[1]), interpolation=cv2.INTER_NEAREST) temp_seg[temp_seg == 4] = 3 # Convert label 4 to 3 mask = tf.cast(tf.one_hot(temp_seg.astype(np.int32), 4), tf.float32) y[c, :, :, j] = mask # Normalize input X = X / np.max(X) return X, y #training_generator = DataGenerator(train_ids) #valid_generator = DataGenerator(val_ids) #test_generator = DataGenerator(test_ids) . import tensorflow as tf physical_devices = tf.config.list_physical_devices(&#39;GPU&#39;) print(physical_devices) . [PhysicalDevice(name=&#39;/physical_device:GPU:0&#39;, device_type=&#39;GPU&#39;), PhysicalDevice(name=&#39;/physical_device:GPU:1&#39;, device_type=&#39;GPU&#39;)] . callbacks = [ # keras.callbacks.EarlyStopping(monitor=&#39;loss&#39;, min_delta=0, # patience=2, verbose=1, mode=&#39;auto&#39;), keras.callbacks.ReduceLROnPlateau(monitor=&#39;val_loss&#39;, factor=0.2, patience=2, min_lr=0.000001, verbose=1), # keras.callbacks.ModelCheckpoint(filepath = &#39;model_.{epoch:02d}-{val_loss:.6f}.m5&#39;, # verbose=1, save_best_only=True, save_weights_only = True) ] . strategy=tf.distribute.MirroredStrategy() . import tensorflow as tf strategy = tf.distribute.MirroredStrategy() print(f&#39;Number of devices: {strategy.num_replicas_in_sync}&#39;) # Wrap model creation and compilation in strategy scope with strategy.scope(): input_layer = Input((IMG_SIZE, IMG_SIZE, IMG_SIZE, 3)) model = build_unet(input_layer, &#39;he_normal&#39;, 0.2) model.compile( loss=&quot;categorical_crossentropy&quot;, optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=[ &#39;accuracy&#39;, tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema, dice_coef_enhancing ] ) BATCH_SIZE_PER_GPU = 5 global_batch_size = BATCH_SIZE_PER_GPU * strategy.num_replicas_in_sync # Initialize generators with adjusted batch size training_generator = DataGenerator( list_IDs=train_ids, batch_size=global_batch_size, dim=(IMG_SIZE, IMG_SIZE, IMG_SIZE), n_channels=3, data_path=TRAIN_DATASET_PATH ) valid_generator = DataGenerator( list_IDs=val_ids, batch_size=global_batch_size, dim=(IMG_SIZE, IMG_SIZE, IMG_SIZE), n_channels=3, data_path=VALIDATION_DATASET_PATH ) test_generator = DataGenerator( list_IDs=test_ids, batch_size=global_batch_size, dim=(IMG_SIZE, IMG_SIZE, IMG_SIZE), n_channels=3, data_path=VALIDATION_DATASET_PATH ) # Train the model history = model.fit( training_generator, epochs=500, callbacks=callbacks, validation_data=valid_generator ) model.save(&#39;/kaggle/output/final_model.h5&#39;) . Number of devices: 2 Epoch 1/500 . ResourceExhaustedError Traceback (most recent call last) Cell In[16], line 56 47 test_generator = DataGenerator( 48 list_IDs=test_ids, 49 batch_size=global_batch_size, (...) 52 data_path=VALIDATION_DATASET_PATH 53 ) 55 # Train the model &gt; 56 history = model.fit( 57 training_generator, 58 epochs=500, 59 callbacks=callbacks, 60 validation_data=valid_generator 61 ) 63 model.save(&#39;/kaggle/output/final_model.h5&#39;) File /opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122, in filter_traceback.&lt;locals&gt;.error_handler(*args, **kwargs) 119 filtered_tb = _process_traceback_frames(e.__traceback__) 120 # To get the full stack trace, call: 121 # `keras.config.disable_traceback_filtering()` --&gt; 122 raise e.with_traceback(filtered_tb) from None 123 finally: 124 del filtered_tb File /opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53, in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name) 51 try: 52 ctx.ensure_initialized() &gt; 53 tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name, 54 inputs, attrs, num_outputs) 55 except core._NotOkStatusException as e: 56 if name is not None: ResourceExhaustedError: Graph execution error: Detected at node replica_1/functional_5_1/up_sampling3d_11_1/Repeat_2/Tile defined at (most recent call last): File &#34;/opt/conda/lib/python3.10/threading.py&#34;, line 973, in _bootstrap File &#34;/opt/conda/lib/python3.10/threading.py&#34;, line 1016, in _bootstrap_inner File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py&#34;, line 104, in one_step_on_data File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py&#34;, line 51, in train_step File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py&#34;, line 117, in error_handler File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py&#34;, line 846, in __call__ File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py&#34;, line 117, in error_handler File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/ops/operation.py&#34;, line 48, in __call__ File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py&#34;, line 156, in error_handler File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/models/functional.py&#34;, line 202, in call File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/ops/function.py&#34;, line 155, in _run_through_graph File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/models/functional.py&#34;, line 592, in call File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py&#34;, line 117, in error_handler File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py&#34;, line 846, in __call__ File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py&#34;, line 117, in error_handler File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/ops/operation.py&#34;, line 48, in __call__ File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py&#34;, line 156, in error_handler File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/layers/reshaping/up_sampling3d.py&#34;, line 99, in call File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/layers/reshaping/up_sampling3d.py&#34;, line 131, in _resize_volumes File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/ops/numpy.py&#34;, line 4381, in repeat File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py&#34;, line 1764, in repeat Detected at node replica_1/functional_5_1/up_sampling3d_11_1/Repeat_2/Tile defined at (most recent call last): File &#34;/opt/conda/lib/python3.10/threading.py&#34;, line 973, in _bootstrap File &#34;/opt/conda/lib/python3.10/threading.py&#34;, line 1016, in _bootstrap_inner File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py&#34;, line 104, in one_step_on_data File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py&#34;, line 51, in train_step File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py&#34;, line 117, in error_handler File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py&#34;, line 846, in __call__ File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py&#34;, line 117, in error_handler File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/ops/operation.py&#34;, line 48, in __call__ File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py&#34;, line 156, in error_handler File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/models/functional.py&#34;, line 202, in call File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/ops/function.py&#34;, line 155, in _run_through_graph File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/models/functional.py&#34;, line 592, in call File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py&#34;, line 117, in error_handler File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/layers/layer.py&#34;, line 846, in __call__ File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py&#34;, line 117, in error_handler File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/ops/operation.py&#34;, line 48, in __call__ File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py&#34;, line 156, in error_handler File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/layers/reshaping/up_sampling3d.py&#34;, line 99, in call File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/layers/reshaping/up_sampling3d.py&#34;, line 131, in _resize_volumes File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/ops/numpy.py&#34;, line 4381, in repeat File &#34;/opt/conda/lib/python3.10/site-packages/keras/src/backend/tensorflow/numpy.py&#34;, line 1764, in repeat 2 root error(s) found. (0) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[5,128,128,64,2,64] and type float on /job:localhost/replica:0/task:0/device:GPU:1 by allocator GPU_1_bfc [[{{node replica_1/functional_5_1/up_sampling3d_11_1/Repeat_2/Tile}}]] Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn&#39;t available when running in Eager mode. [[StatefulPartitionedCall/cond/else/_380/cond/StatefulPartitionedCall/replica_1/div_no_nan_1/_1454]] Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn&#39;t available when running in Eager mode. (1) RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[5,128,128,64,2,64] and type float on /job:localhost/replica:0/task:0/device:GPU:1 by allocator GPU_1_bfc [[{{node replica_1/functional_5_1/up_sampling3d_11_1/Repeat_2/Tile}}]] Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn&#39;t available when running in Eager mode. 0 successful operations. 0 derived errors ignored. [Op:__inference_one_step_on_iterator_35656] . import numpy as np import matplotlib.pyplot as plt import nibabel as nib import cv2 import os def predictByPath(model, case_path, case_id): &quot;&quot;&quot; Predict tumor segmentation for a given case Args: model: Trained 3D U-Net model case_path: Path to the case directory case_id: Case identifier &quot;&quot;&quot; # Initialize input volume X = np.zeros((1, IMG_SIZE, IMG_SIZE, IMG_SIZE, 3)) # Load the different modalities flair = nib.load(os.path.join(case_path, f&#39;{case_id}_flair.nii&#39;)).get_fdata() t1ce = nib.load(os.path.join(case_path, f&#39;{case_id}_t1ce.nii&#39;)).get_fdata() t1 = nib.load(os.path.join(case_path, f&#39;{case_id}_t1.nii&#39;)).get_fdata() # Resize and normalize each slice for j in range(IMG_SIZE): X[0, :, :, j, 0] = cv2.resize(flair[:, :, j], (IMG_SIZE, IMG_SIZE)) X[0, :, :, j, 1] = cv2.resize(t1ce[:, :, j], (IMG_SIZE, IMG_SIZE)) X[0, :, :, j, 2] = cv2.resize(t1[:, :, j], (IMG_SIZE, IMG_SIZE)) # Normalize input X = X / np.max(X) # Make prediction return model.predict(X, verbose=1) def showPredictsById(model, case_id, data_path, start_slice=64): &quot;&quot;&quot; Show predictions for a given case Args: model: Trained 3D U-Net model case_id: Case identifier (e.g., &#39;BraTS20_Training_001&#39;) data_path: Path to the dataset start_slice: Slice index to visualize &quot;&quot;&quot; # Construct full path case_path = os.path.join(data_path, case_id) # Load ground truth and original image gt = nib.load(os.path.join(case_path, f&#39;{case_id}_seg.nii&#39;)).get_fdata() flair = nib.load(os.path.join(case_path, f&#39;{case_id}_flair.nii&#39;)).get_fdata() # Get predictions predictions = predictByPath(model, case_path, case_id) # Extract individual class predictions background = predictions[0, :, :, :, 0] necrotic = predictions[0, :, :, :, 1] edema = predictions[0, :, :, :, 2] enhancing = predictions[0, :, :, :, 3] # Create visualization plt.figure(figsize=(20, 4)) # Original FLAIR plt.subplot(161) plt.imshow(cv2.resize(flair[:, :, start_slice], (IMG_SIZE, IMG_SIZE)), cmap=&quot;gray&quot;) plt.title(&#39;Original FLAIR&#39;) plt.axis(&#39;off&#39;) # Ground Truth plt.subplot(162) gt_slice = cv2.resize(gt[:, :, start_slice], (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST) plt.imshow(cv2.resize(flair[:, :, start_slice], (IMG_SIZE, IMG_SIZE)), cmap=&quot;gray&quot;) plt.imshow(gt_slice, cmap=&quot;hot&quot;, interpolation=&#39;none&#39;, alpha=0.3) plt.title(&#39;Ground Truth&#39;) plt.axis(&#39;off&#39;) # Combined Prediction plt.subplot(163) combined_pred = np.zeros((IMG_SIZE, IMG_SIZE, 3)) combined_pred[:, :, 0] = necrotic[:, :, start_slice] combined_pred[:, :, 1] = edema[:, :, start_slice] combined_pred[:, :, 2] = enhancing[:, :, start_slice] plt.imshow(cv2.resize(flair[:, :, start_slice], (IMG_SIZE, IMG_SIZE)), cmap=&quot;gray&quot;) plt.imshow(combined_pred, interpolation=&#39;none&#39;, alpha=0.3) plt.title(&#39;Combined Prediction&#39;) plt.axis(&#39;off&#39;) # Individual class predictions plt.subplot(164) plt.imshow(cv2.resize(flair[:, :, start_slice], (IMG_SIZE, IMG_SIZE)), cmap=&quot;gray&quot;) plt.imshow(necrotic[:, :, start_slice], cmap=&quot;hot&quot;, interpolation=&#39;none&#39;, alpha=0.3) plt.title(f&#39;{SEGMENT_CLASSES[1]}&#39;) plt.axis(&#39;off&#39;) plt.subplot(165) plt.imshow(cv2.resize(flair[:, :, start_slice], (IMG_SIZE, IMG_SIZE)), cmap=&quot;gray&quot;) plt.imshow(edema[:, :, start_slice], cmap=&quot;hot&quot;, interpolation=&#39;none&#39;, alpha=0.3) plt.title(f&#39;{SEGMENT_CLASSES[2]}&#39;) plt.axis(&#39;off&#39;) plt.subplot(166) plt.imshow(cv2.resize(flair[:, :, start_slice], (IMG_SIZE, IMG_SIZE)), cmap=&quot;gray&quot;) plt.imshow(enhancing[:, :, start_slice], cmap=&quot;hot&quot;, interpolation=&#39;none&#39;, alpha=0.3) plt.title(f&#39;{SEGMENT_CLASSES[3]}&#39;) plt.axis(&#39;off&#39;) plt.tight_layout() plt.show() # Calculate Dice scores dice_scores = [] gt_slice = gt_slice.astype(int) pred_slice = np.argmax(predictions[0, :, :, start_slice], axis=-1) for i in range(1, 4): # Skip background class gt_binary = (gt_slice == i).astype(np.float32) pred_binary = (pred_slice == i).astype(np.float32) intersection = np.sum(gt_binary * pred_binary) union = np.sum(gt_binary) + np.sum(pred_binary) if union &gt; 0: dice = 2 * intersection / union else: dice = 1.0 if intersection == 0 else 0.0 dice_scores.append(dice) print(&quot; nDice Scores for slice {}:&quot;.format(start_slice)) print(f&quot;Necrotic/Core: {dice_scores[0]:.3f}&quot;) print(f&quot;Edema: {dice_scores[1]:.3f}&quot;) print(f&quot;Enhancing: {dice_scores[2]:.3f}&quot;) print(f&quot;Average: {np.mean(dice_scores):.3f}&quot;) . # Visualize predictions for multiple test cases for i in range(5): # Show first 5 test cases print(f&quot; nCase {test_ids[i]}&quot;) showPredictsById(model, test_ids[i], TRAIN_DATASET_PATH) # Or visualize a specific case with different slices case_id = test_ids[0] for slice_idx in [50, 60, 70, 80]: # View different slices showPredictsById(model, case_id, TRAIN_DATASET_PATH, start_slice=slice_idx) . import numpy as np import matplotlib.pyplot as plt import nibabel as nib import cv2 import os def showPredictsById(model, case_id, data_path, start_slice=64, show_3d=True): &quot;&quot;&quot; Enhanced visualization of tumor predictions matching the red colormap style &quot;&quot;&quot; # Construct full path case_path = os.path.join(data_path, case_id) # Load all required volumes gt = nib.load(os.path.join(case_path, f&#39;{case_id}_seg.nii&#39;)).get_fdata() flair = nib.load(os.path.join(case_path, f&#39;{case_id}_flair.nii&#39;)).get_fdata() t1ce = nib.load(os.path.join(case_path, f&#39;{case_id}_t1ce.nii&#39;)).get_fdata() # Get predictions predictions = predictByPath(model, case_path, case_id) if show_3d: fig = plt.figure(figsize=(20, 10)) gs = fig.add_gridspec(2, 6) else: fig = plt.figure(figsize=(20, 4)) gs = fig.add_gridspec(1, 6) # Main axial view ax1 = fig.add_subplot(gs[0, 0]) ax1.imshow(cv2.resize(flair[:, :, start_slice], (IMG_SIZE, IMG_SIZE)), cmap=&quot;gray&quot;) ax1.set_title(&#39;FLAIR&#39;) ax1.axis(&#39;off&#39;) # Ground Truth ax2 = fig.add_subplot(gs[0, 1]) gt_slice = cv2.resize(gt[:, :, start_slice], (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_NEAREST) ax2.imshow(cv2.resize(flair[:, :, start_slice], (IMG_SIZE, IMG_SIZE)), cmap=&quot;gray&quot;) ax2.imshow(gt_slice, cmap=&quot;Reds&quot;, interpolation=&#39;none&#39;, alpha=0.3) ax2.set_title(&#39;Ground Truth&#39;) ax2.axis(&#39;off&#39;) # Combined Prediction ax3 = fig.add_subplot(gs[0, 2]) ax3.imshow(cv2.resize(flair[:, :, start_slice], (IMG_SIZE, IMG_SIZE)), cmap=&quot;gray&quot;) # Combine predictions by taking maximum across channels pred_combined = np.max(predictions[0, :, :, start_slice, 1:], axis=-1) ax3.imshow(pred_combined, cmap=&quot;Reds&quot;, interpolation=&#39;none&#39;, alpha=0.3) ax3.set_title(&#39;Combined Prediction&#39;) ax3.axis(&#39;off&#39;) # Individual predictions class_names = [&#39;Necrotic&#39;, &#39;Edema&#39;, &#39;Enhancing&#39;] for idx in range(3): ax = fig.add_subplot(gs[0, idx+3]) ax.imshow(cv2.resize(flair[:, :, start_slice], (IMG_SIZE, IMG_SIZE)), cmap=&quot;gray&quot;) prediction = predictions[0, :, :, start_slice, idx+1] ax.imshow(prediction, cmap=&quot;OrRd&quot;, interpolation=&#39;none&#39;, alpha=0.3) ax.set_title(f&#39;{class_names[idx]}&#39;) ax.axis(&#39;off&#39;) if show_3d: # Sagittal view ax_sag = fig.add_subplot(gs[1, 0:2]) mid_sagittal = IMG_SIZE // 2 ax_sag.imshow(cv2.resize(flair[mid_sagittal, :, :].T, (IMG_SIZE, IMG_SIZE)), cmap=&quot;gray&quot;) # Combine predictions for sagittal view pred_sag = np.max(predictions[0, mid_sagittal, :, :, 1:], axis=-1) ax_sag.imshow(pred_sag.T, cmap=&quot;Reds&quot;, interpolation=&#39;none&#39;, alpha=0.3) ax_sag.axhline(y=start_slice, color=&#39;r&#39;, linestyle=&#39;--&#39;) ax_sag.set_title(&#39;Sagittal View&#39;) ax_sag.axis(&#39;off&#39;) # Coronal view ax_cor = fig.add_subplot(gs[1, 2:4]) mid_coronal = IMG_SIZE // 2 ax_cor.imshow(cv2.resize(flair[:, mid_coronal, :].T, (IMG_SIZE, IMG_SIZE)), cmap=&quot;gray&quot;) # Combine predictions for coronal view pred_cor = np.max(predictions[0, :, mid_coronal, :, 1:], axis=-1) ax_cor.imshow(pred_cor.T, cmap=&quot;Reds&quot;, interpolation=&#39;none&#39;, alpha=0.3) ax_cor.axhline(y=start_slice, color=&#39;r&#39;, linestyle=&#39;--&#39;) ax_cor.set_title(&#39;Coronal View&#39;) ax_cor.axis(&#39;off&#39;) # Dice score plot ax_dice = fig.add_subplot(gs[1, 4:]) dice_scores = [] labels = [] for i in range(1, 4): gt_binary = (gt_slice == i).astype(np.float32) pred_slice = np.argmax(predictions[0, :, :, start_slice], axis=-1) pred_binary = (pred_slice == i).astype(np.float32) intersection = np.sum(gt_binary * pred_binary) union = np.sum(gt_binary) + np.sum(pred_binary) dice = 2 * intersection / union if union &gt; 0 else 0.0 dice_scores.append(dice) labels.append(class_names[i-1]) bars = ax_dice.bar(labels, dice_scores) ax_dice.set_ylim(0, 1) ax_dice.set_title(&#39;Dice Scores&#39;) for bar in bars: height = bar.get_height() ax_dice.text(bar.get_x() + bar.get_width()/2., height, f&#39;{height:.3f}&#39;, ha=&#39;center&#39;, va=&#39;bottom&#39;) plt.tight_layout() plt.show() # Print numerical results print(&quot; nDice Scores for slice {}:&quot;.format(start_slice)) print(f&quot;Necrotic/Core: {dice_scores[0]:.3f}&quot;) print(f&quot;Edema: {dice_scores[1]:.3f}&quot;) print(f&quot;Enhancing: {dice_scores[2]:.3f}&quot;) print(f&quot;Average: {np.mean(dice_scores):.3f}&quot;) . showPredictsById(model, &#39;BraTS20_Training_001&#39;, TRAIN_DATASET_PATH, start_slice=64) . showPredictsById(model, &#39;BraTS20_Training_100&#39;, TRAIN_DATASET_PATH, start_slice=64, show_3d=True) .",
            "url": "https://kozodoi.me/2024/11/15/D-UNET.html",
            "relUrl": "/2024/11/15/D-UNET.html",
            "date": " • Nov 15, 2024"
        }
        
    
  
    
        ,"post2": {
            "title": "Directional Forecastion",
            "content": "This post was originally published on my github_repository . 1. Introduction This post consists of, how basic machine learning and python libraries can be tailored for most of Finance Data Science like in here we are using Random Forest for directional forecasting of a cryptocurrency data consisting of features &#39;open&#39;, &#39;high&#39;, &#39;low&#39;, &#39;close&#39;, &#39;volume&#39;, &#39;taker_buy_volume_ratio&#39; etc.. with their given &#39;timestamp&#39;s&#39;(useful for extracting month, day, date, hour, minutes) of minutes&#39; basis, containing 2.1 million rows. . { &quot;cells&quot;: [ { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 1, &quot;id&quot;: &quot;702cf6bf&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:31:51.702481Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:31:51.701947Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:31:54.729006Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:31:54.727637Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 3.041341, &quot;end_time&quot;: &quot;2024-10-31T20:31:54.732015&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:31:51.690674&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;import numpy as np n&quot;, &quot;import pandas as pd n&quot;, &quot;import matplotlib.pyplot as plt n&quot;, &quot;import seaborn as sns&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 2, &quot;id&quot;: &quot;0d1c7824&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:31:54.751056Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:31:54.750407Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:32:03.441069Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:32:03.439676Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 8.703387, &quot;end_time&quot;: &quot;2024-10-31T20:32:03.444011&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:31:54.740624&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [ { &quot;data&quot;: { &quot;text/html&quot;: [ &quot; n&quot;, &quot; n&quot;, &quot;&lt;table border= &quot;1 &quot; class= &quot;dataframe &quot;&gt; n&quot;, &quot; n&quot;, &quot; &lt;tr style= &quot;text-align: right; &quot;&gt; n&quot;, &quot; n&quot;, &quot; timestamp n&quot;, &quot; open n&quot;, &quot; high n&quot;, &quot; low n&quot;, &quot; close n&quot;, &quot; volume n&quot;, &quot; quote_asset_volume n&quot;, &quot; number_of_trades n&quot;, &quot; taker_buy_base_volume n&quot;, &quot; taker_buy_quote_volume n&quot;, &quot; target n&quot;, &quot; &lt;/tr&gt; n&quot;, &quot; n&quot;, &quot; n&quot;, &quot; n&quot;, &quot; 0 n&quot;, &quot; 1525471260 | n&quot;, &quot; 0.90120 | n&quot;, &quot; 0.90130 | n&quot;, &quot; 0.90120 | n&quot;, &quot; 0.90130 | n&quot;, &quot; 134.98 | n&quot;, &quot; 121.646459 | n&quot;, &quot; 4.0 | n&quot;, &quot; 125.08 | n&quot;, &quot; 112.723589 | n&quot;, &quot; 1.0 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 1 n&quot;, &quot; 1525471320 | n&quot;, &quot; 0.90185 | n&quot;, &quot; 0.90195 | n&quot;, &quot; 0.90185 | n&quot;, &quot; 0.90195 | n&quot;, &quot; 1070.54 | n&quot;, &quot; 965.505313 | n&quot;, &quot; 12.0 | n&quot;, &quot; 879.94 | n&quot;, &quot; 793.612703 | n&quot;, &quot; 0.0 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 2 n&quot;, &quot; 1525471380 | n&quot;, &quot; 0.90140 | n&quot;, &quot; 0.90140 | n&quot;, &quot; 0.90139 | n&quot;, &quot; 0.90139 | n&quot;, &quot; 2293.06 | n&quot;, &quot; 2066.963991 | n&quot;, &quot; 5.0 | n&quot;, &quot; 0.00 | n&quot;, &quot; 0.000000 | n&quot;, &quot; 0.0 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 3 n&quot;, &quot; 1525471440 | n&quot;, &quot; 0.90139 | n&quot;, &quot; 0.90140 | n&quot;, &quot; 0.90138 | n&quot;, &quot; 0.90139 | n&quot;, &quot; 6850.59 | n&quot;, &quot; 6175.000909 | n&quot;, &quot; 19.0 | n&quot;, &quot; 1786.30 | n&quot;, &quot; 1610.149485 | n&quot;, &quot; 0.0 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 4 n&quot;, &quot; 1525471500 | n&quot;, &quot; 0.90139 | n&quot;, &quot; 0.90139 | n&quot;, &quot; 0.90130 | n&quot;, &quot; 0.90130 | n&quot;, &quot; 832.30 | n&quot;, &quot; 750.222624 | n&quot;, &quot; 3.0 | n&quot;, &quot; 784.82 | n&quot;, &quot; 707.428900 | n&quot;, &quot; 0.0 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; ... n&quot;, &quot; ... | n&quot;, &quot; ... | n&quot;, &quot; ... | n&quot;, &quot; ... | n&quot;, &quot; ... | n&quot;, &quot; ... | n&quot;, &quot; ... | n&quot;, &quot; ... | n&quot;, &quot; ... | n&quot;, &quot; ... | n&quot;, &quot; ... | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 2122433 n&quot;, &quot; 1652817240 | n&quot;, &quot; 0.43060 | n&quot;, &quot; 0.43060 | n&quot;, &quot; 0.42990 | n&quot;, &quot; 0.43040 | n&quot;, &quot; 136274.00 | n&quot;, &quot; 58630.162800 | n&quot;, &quot; 144.0 | n&quot;, &quot; 54216.00 | n&quot;, &quot; 23325.927700 | n&quot;, &quot; 1.0 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 2122434 n&quot;, &quot; 1652817300 | n&quot;, &quot; 0.43030 | n&quot;, &quot; 0.43070 | n&quot;, &quot; 0.43030 | n&quot;, &quot; 0.43050 | n&quot;, &quot; 104478.00 | n&quot;, &quot; 44967.837600 | n&quot;, &quot; 99.0 | n&quot;, &quot; 52232.00 | n&quot;, &quot; 22484.030400 | n&quot;, &quot; 1.0 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 2122435 n&quot;, &quot; 1652817360 | n&quot;, &quot; 0.43050 | n&quot;, &quot; 0.43120 | n&quot;, &quot; 0.43050 | n&quot;, &quot; 0.43090 | n&quot;, &quot; 212396.00 | n&quot;, &quot; 91526.987200 | n&quot;, &quot; 177.0 | n&quot;, &quot; 108324.00 | n&quot;, &quot; 46673.061600 | n&quot;, &quot; 0.0 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 2122436 n&quot;, &quot; 1652817420 | n&quot;, &quot; 0.43110 | n&quot;, &quot; 0.43110 | n&quot;, &quot; 0.43040 | n&quot;, &quot; 0.43060 | n&quot;, &quot; 131047.00 | n&quot;, &quot; 56443.003800 | n&quot;, &quot; 107.0 | n&quot;, &quot; 32713.00 | n&quot;, &quot; 14097.148900 | n&quot;, &quot; 0.0 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 2122437 n&quot;, &quot; 1652817480 | n&quot;, &quot; 0.43060 | n&quot;, &quot; 0.43080 | n&quot;, &quot; 0.43010 | n&quot;, &quot; 0.43010 | n&quot;, &quot; 101150.00 | n&quot;, &quot; 43542.262900 | n&quot;, &quot; 105.0 | n&quot;, &quot; 46109.00 | n&quot;, &quot; 19851.723700 | n&quot;, &quot; 1.0 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot;&lt;/table&gt; n&quot;, &quot;2122438 rows × 11 columns . n&quot;, &quot;&quot; ], &quot;text/plain&quot;: [ &quot; timestamp open high low close volume n&quot;, &quot;0 1525471260 0.90120 0.90130 0.90120 0.90130 134.98 n&quot;, &quot;1 1525471320 0.90185 0.90195 0.90185 0.90195 1070.54 n&quot;, &quot;2 1525471380 0.90140 0.90140 0.90139 0.90139 2293.06 n&quot;, &quot;3 1525471440 0.90139 0.90140 0.90138 0.90139 6850.59 n&quot;, &quot;4 1525471500 0.90139 0.90139 0.90130 0.90130 832.30 n&quot;, &quot;... ... ... ... ... ... ... n&quot;, &quot;2122433 1652817240 0.43060 0.43060 0.42990 0.43040 136274.00 n&quot;, &quot;2122434 1652817300 0.43030 0.43070 0.43030 0.43050 104478.00 n&quot;, &quot;2122435 1652817360 0.43050 0.43120 0.43050 0.43090 212396.00 n&quot;, &quot;2122436 1652817420 0.43110 0.43110 0.43040 0.43060 131047.00 n&quot;, &quot;2122437 1652817480 0.43060 0.43080 0.43010 0.43010 101150.00 n&quot;, &quot; n&quot;, &quot; quote_asset_volume number_of_trades taker_buy_base_volume n&quot;, &quot;0 121.646459 4.0 125.08 n&quot;, &quot;1 965.505313 12.0 879.94 n&quot;, &quot;2 2066.963991 5.0 0.00 n&quot;, &quot;3 6175.000909 19.0 1786.30 n&quot;, &quot;4 750.222624 3.0 784.82 n&quot;, &quot;... ... ... ... n&quot;, &quot;2122433 58630.162800 144.0 54216.00 n&quot;, &quot;2122434 44967.837600 99.0 52232.00 n&quot;, &quot;2122435 91526.987200 177.0 108324.00 n&quot;, &quot;2122436 56443.003800 107.0 32713.00 n&quot;, &quot;2122437 43542.262900 105.0 46109.00 n&quot;, &quot; n&quot;, &quot; taker_buy_quote_volume target n&quot;, &quot;0 112.723589 1.0 n&quot;, &quot;1 793.612703 0.0 n&quot;, &quot;2 0.000000 0.0 n&quot;, &quot;3 1610.149485 0.0 n&quot;, &quot;4 707.428900 0.0 n&quot;, &quot;... ... ... n&quot;, &quot;2122433 23325.927700 1.0 n&quot;, &quot;2122434 22484.030400 1.0 n&quot;, &quot;2122435 46673.061600 0.0 n&quot;, &quot;2122436 14097.148900 0.0 n&quot;, &quot;2122437 19851.723700 1.0 n&quot;, &quot; n&quot;, &quot;[2122438 rows x 11 columns]&quot; ] }, &quot;execution_count&quot;: 2, &quot;metadata&quot;: {}, &quot;output_type&quot;: &quot;execute_result&quot; } ], &quot;source&quot;: [ &quot;train_df = pd.read_csv(&#39;/kaggle/input/directional-forecasting-in-cryptocurrencies/train.csv&#39;) n&quot;, &quot;test_df = pd.read_csv(&#39;/kaggle/input/directional-forecasting-in-cryptocurrencies/test.csv&#39;) n&quot;, &quot;train_df&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 3, &quot;id&quot;: &quot;a403554a&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:32:03.462146Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:32:03.461677Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:32:05.162956Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:32:05.161864Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 1.715081, &quot;end_time&quot;: &quot;2024-10-31T20:32:05.167262&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:32:03.452181&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [ { &quot;data&quot;: { &quot;image/png&quot;: &quot;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&quot;, &quot;text/plain&quot;: [ &quot;&lt;Figure size 2000x1000 with 2 Axes&gt;&quot; ] }, &quot;metadata&quot;: {}, &quot;output_type&quot;: &quot;display_data&quot; } ], &quot;source&quot;: [ &quot;import seaborn as sns n&quot;, &quot;plt.figure(figsize=(20,10)) n&quot;, &quot;sns.heatmap(train_df.corr(), annot=True, cmap=&#39;coolwarm&#39;, linewidths=0.5) n&quot;, &quot;plt.title(&#39;Correlation Heatmap of Features&#39;) n&quot;, &quot;plt.show()&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 4, &quot;id&quot;: &quot;e0758621&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:32:05.189548Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:32:05.189085Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:32:06.262793Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:32:06.261578Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 1.088047, &quot;end_time&quot;: &quot;2024-10-31T20:32:06.265478&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:32:05.177431&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [ { &quot;data&quot;: { &quot;image/png&quot;: &quot;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&quot;, &quot;text/plain&quot;: [ &quot;&lt;Figure size 700x50 with 1 Axes&gt;&quot; ] }, &quot;metadata&quot;: {}, &quot;output_type&quot;: &quot;display_data&quot; } ], &quot;source&quot;: [ &quot;import seaborn as sns n&quot;, &quot;import matplotlib.pyplot as plt n&quot;, &quot; n&quot;, &quot;correlation_matrix = train_df.corr() n&quot;, &quot; n&quot;, &quot;signal_corr = correlation_matrix[[&#39;target&#39;]].sort_values(by=&#39;target&#39;, ascending=False) n&quot;, &quot; n&quot;, &quot;plt.figure(figsize=(7, 0.5)) n&quot;, &quot;sns.heatmap(signal_corr.T, annot=True, cmap=&#39;coolwarm&#39;, cbar=False) n&quot;, &quot;plt.title( &quot;Correlation of Features with &#39;target&#39; &quot;) n&quot;, &quot;plt.show() n&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 5, &quot;id&quot;: &quot;67916f46&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:32:06.288943Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:32:06.288460Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:32:06.798921Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:32:06.797751Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 0.525722, &quot;end_time&quot;: &quot;2024-10-31T20:32:06.802021&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:32:06.276299&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;from datetime import datetime, timezone n&quot;, &quot; n&quot;, &quot;train_df[&#39;timestamp&#39;] = pd.to_datetime(train_df[&#39;timestamp&#39;], unit=&#39;s&#39;) n&quot;, &quot;train_df[&#39;minute&#39;] = train_df[&#39;timestamp&#39;].dt.minute n&quot;, &quot;train_df.drop(&#39;timestamp&#39;, axis=1, inplace=True) n&quot;, &quot; n&quot;, &quot;test_df[&#39;timestamp&#39;] = pd.to_datetime(test_df[&#39;timestamp&#39;], unit=&#39;s&#39;) n&quot;, &quot;test_df[&#39;minute&#39;] = test_df[&#39;timestamp&#39;].dt.minute n&quot;, &quot;test_df.drop(&#39;timestamp&#39;, axis=1, inplace=True)&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 6, &quot;id&quot;: &quot;db578539&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:32:06.826757Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:32:06.826268Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:32:07.480443Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:32:07.479229Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 0.670707, &quot;end_time&quot;: &quot;2024-10-31T20:32:07.483545&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:32:06.812838&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [ { &quot;name&quot;: &quot;stdout&quot;, &quot;output_type&quot;: &quot;stream&quot;, &quot;text&quot;: [ &quot;Outliers: n&quot;, &quot; open high low close volume quote_asset_volume n&quot;, &quot;107836 0.50566 0.50631 0.48507 0.50008 1822170.0 9.088897e+05 n&quot;, &quot;108201 0.47580 0.47615 0.47500 0.47500 1145163.6 5.439815e+05 n&quot;, &quot;197296 0.31937 0.32362 0.31937 0.32278 1811583.1 5.802121e+05 n&quot;, &quot;199657 0.36803 0.37200 0.36778 0.37152 1346806.7 4.980468e+05 n&quot;, &quot;200189 0.48499 0.49021 0.48356 0.49000 2875628.5 1.396135e+06 n&quot;, &quot;... ... ... ... ... ... ... n&quot;, &quot;2120984 0.42000 0.42800 0.41990 0.42790 2374477.0 1.006244e+06 n&quot;, &quot;2120985 0.42790 0.44260 0.42790 0.43930 12363632.0 5.401453e+06 n&quot;, &quot;2120986 0.43940 0.43940 0.43300 0.43450 3054547.0 1.332590e+06 n&quot;, &quot;2120987 0.43450 0.43600 0.42480 0.42650 6367198.0 2.730539e+06 n&quot;, &quot;2120988 0.42650 0.42750 0.42270 0.42270 2871307.0 1.220657e+06 n&quot;, &quot; n&quot;, &quot; number_of_trades taker_buy_base_volume taker_buy_quote_volume n&quot;, &quot;107836 865.0 1151145.0 5.747437e+05 n&quot;, &quot;108201 42.0 1107813.4 5.262225e+05 n&quot;, &quot;197296 261.0 1675449.4 5.364960e+05 n&quot;, &quot;199657 459.0 1186219.9 4.387031e+05 n&quot;, &quot;200189 728.0 2623470.3 1.273743e+06 n&quot;, &quot;... ... ... ... n&quot;, &quot;2120984 1285.0 1749782.0 7.408540e+05 n&quot;, &quot;2120985 7547.0 6577021.0 2.870867e+06 n&quot;, &quot;2120986 2296.0 1430104.0 6.235863e+05 n&quot;, &quot;2120987 3992.0 2906838.0 1.246762e+06 n&quot;, &quot;2120988 1939.0 1315831.0 5.595667e+05 n&quot;, &quot; n&quot;, &quot; target minute n&quot;, &quot;107836 0.0 17 n&quot;, &quot;108201 1.0 22 n&quot;, &quot;197296 0.0 17 n&quot;, &quot;199657 0.0 38 n&quot;, &quot;200189 0.0 30 n&quot;, &quot;... ... ... n&quot;, &quot;2120984 1.0 45 n&quot;, &quot;2120985 0.0 46 n&quot;, &quot;2120986 0.0 47 n&quot;, &quot;2120987 0.0 48 n&quot;, &quot;2120988 1.0 49 n&quot;, &quot; n&quot;, &quot;[74452 rows x 11 columns] n&quot; ] } ], &quot;source&quot;: [ &quot;#USING Z-SCORE METHOD TO GET RID OF OUTLIERS n&quot;, &quot; n&quot;, &quot;z_threshold = 3 n&quot;, &quot; n&quot;, &quot;z_scores = np.abs((train_df - train_df.mean()) / train_df.std()) n&quot;, &quot; n&quot;, &quot;outliers = (z_scores &gt; z_threshold) n&quot;, &quot; n&quot;, &quot;print( &quot;Outliers: n &quot;, train_df[outliers.any(axis=1)]) n&quot;, &quot; n&quot;, &quot;train_df = train_df[~outliers.any(axis=1)]&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 7, &quot;id&quot;: &quot;6e53f833&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:32:07.507920Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:32:07.507268Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:32:08.196401Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:32:08.195211Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 0.704715, &quot;end_time&quot;: &quot;2024-10-31T20:32:08.199334&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:32:07.494619&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [ { &quot;name&quot;: &quot;stderr&quot;, &quot;output_type&quot;: &quot;stream&quot;, &quot;text&quot;: [ &quot;/tmp/ipykernel_18/3596558217.py:1: SettingWithCopyWarning: n&quot;, &quot;A value is trying to be set on a copy of a slice from a DataFrame. n&quot;, &quot;Try using .loc[row_indexer,col_indexer] = value instead n&quot;, &quot; n&quot;, &quot;See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy n&quot;, &quot; train_df[&#39;close_shift_1&#39;] = train_df.shift(-1)[&#39;close&#39;] n&quot;, &quot;/tmp/ipykernel_18/3596558217.py:2: SettingWithCopyWarning: n&quot;, &quot;A value is trying to be set on a copy of a slice from a DataFrame. n&quot;, &quot;Try using .loc[row_indexer,col_indexer] = value instead n&quot;, &quot; n&quot;, &quot;See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy n&quot;, &quot; train_df[&#39;close_shift_2&#39;] = train_df.shift(-2)[&#39;close&#39;] n&quot;, &quot;/tmp/ipykernel_18/3596558217.py:3: SettingWithCopyWarning: n&quot;, &quot;A value is trying to be set on a copy of a slice from a DataFrame. n&quot;, &quot;Try using .loc[row_indexer,col_indexer] = value instead n&quot;, &quot; n&quot;, &quot;See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy n&quot;, &quot; train_df[&#39;close_shift_3&#39;] = train_df.shift(-3)[&#39;close&#39;] n&quot;, &quot;/tmp/ipykernel_18/3596558217.py:4: SettingWithCopyWarning: n&quot;, &quot;A value is trying to be set on a copy of a slice from a DataFrame. n&quot;, &quot;Try using .loc[row_indexer,col_indexer] = value instead n&quot;, &quot; n&quot;, &quot;See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy n&quot;, &quot; train_df[&#39;close_shift_4&#39;] = train_df.shift(-4)[&#39;close&#39;] n&quot;, &quot;/tmp/ipykernel_18/3596558217.py:5: SettingWithCopyWarning: n&quot;, &quot;A value is trying to be set on a copy of a slice from a DataFrame. n&quot;, &quot;Try using .loc[row_indexer,col_indexer] = value instead n&quot;, &quot; n&quot;, &quot;See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy n&quot;, &quot; train_df[&#39;close_shift_5&#39;] = train_df.shift(-5)[&#39;close&#39;] n&quot; ] } ], &quot;source&quot;: [ &quot;train_df[&#39;close_shift_1&#39;] = train_df.shift(-1)[&#39;close&#39;] n&quot;, &quot;train_df[&#39;close_shift_2&#39;] = train_df.shift(-2)[&#39;close&#39;] n&quot;, &quot;train_df[&#39;close_shift_3&#39;] = train_df.shift(-3)[&#39;close&#39;] n&quot;, &quot;train_df[&#39;close_shift_4&#39;] = train_df.shift(-4)[&#39;close&#39;] n&quot;, &quot;train_df[&#39;close_shift_5&#39;] = train_df.shift(-5)[&#39;close&#39;] n&quot;, &quot; n&quot;, &quot; n&quot;, &quot;test_df[&#39;close_shift_1&#39;] = test_df.shift(-1)[&#39;close&#39;] n&quot;, &quot;test_df[&#39;close_shift_2&#39;] = test_df.shift(-2)[&#39;close&#39;] n&quot;, &quot;test_df[&#39;close_shift_3&#39;] = test_df.shift(-3)[&#39;close&#39;] n&quot;, &quot;test_df[&#39;close_shift_4&#39;] = test_df.shift(-4)[&#39;close&#39;] n&quot;, &quot;test_df[&#39;close_shift_5&#39;] = test_df.shift(-5)[&#39;close&#39;] n&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 8, &quot;id&quot;: &quot;e8490815&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:32:08.223974Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:32:08.223058Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:32:08.239007Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:32:08.237754Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 0.031178, &quot;end_time&quot;: &quot;2024-10-31T20:32:08.241846&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:32:08.210668&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;def Feature_Engineering(df): n&quot;, &quot; train_df = pd.DataFrame(df) n&quot;, &quot; n&quot;, &quot; n&quot;, &quot;# train_df[&#39;true_range&#39;] = train_df[[&#39;high&#39;, &#39;low&#39;, &#39;close&#39;]].max(axis=1) - train_df[[&#39;high&#39;, &#39;low&#39;, &#39;close&#39;]].min(axis=1) --&gt;BAD INDICATORS n&quot;, &quot;# train_df[&#39;atr&#39;] = (train_df[[&#39;high&#39;, &#39;low&#39;, &#39;close&#39;]].max(axis=1) - train_df[[&#39;high&#39;, &#39;low&#39;, &#39;close&#39;]].min(axis=1)).rolling(window=14).mean() --&gt;BAD INDICATOR n&quot;, &quot; n&quot;, &quot; train_df[&#39;sma_5&#39;] = train_df[&#39;close&#39;].rolling(window=5).mean() n&quot;, &quot; train_df[&#39;sma_10&#39;] = train_df[&#39;close&#39;].rolling(window=10).mean() n&quot;, &quot; train_df[&#39;sma_shift_5&#39;] = train_df[&#39;close_shift_1&#39;].rolling(window=5).mean() n&quot;, &quot; train_df[&#39;sma_shiftmedian_5&#39;] = train_df[&#39;close_shift_1&#39;].rolling(window=5).median() n&quot;, &quot; train_df[&#39;sma_shiftmedian_10&#39;] = train_df[&#39;close_shift_1&#39;].rolling(window=10).median() n&quot;, &quot; n&quot;, &quot; n&quot;, &quot; train_df[&#39;sma_20&#39;] = train_df[&#39;close&#39;].rolling(window=20).mean() n&quot;, &quot; n&quot;, &quot; train_df[&#39;bb_up&#39;] = train_df[&#39;sma_20&#39;] + ((train_df[&#39;close&#39;].rolling(window=20).std()) * 2) n&quot;, &quot; train_df[&#39;bb_low&#39;] = train_df[&#39;sma_20&#39;] - ((train_df[&#39;close&#39;].rolling(window=20).std()) * 2) n&quot;, &quot; n&quot;, &quot; train_df[&#39;ema_5&#39;] = train_df[&#39;close&#39;].ewm(span=5, adjust=False).mean() n&quot;, &quot; train_df[&#39;ema_10&#39;] = train_df[&#39;close&#39;].ewm(span=10, adjust=False).mean() n&quot;, &quot; train_df[&#39;ema_shift_5&#39;] = train_df[&#39;close_shift_1&#39;].ewm(span=5, adjust=False).mean() n&quot;, &quot; train_df[&#39;ema_crossover&#39;] = train_df[&#39;ema_5&#39;] - train_df[&#39;ema_10&#39;] n&quot;, &quot; train_df[&#39;close_diff&#39;] = train_df[&#39;close&#39;] - train_df[&#39;close_shift_1&#39;] n&quot;, &quot; train_df[&#39;close_shift_1_volatility&#39;] = train_df[&#39;close_shift_1&#39;].rolling(window=10).std() n&quot;, &quot; n&quot;, &quot; train_df[&#39;close_shift_1_ma20&#39;] = train_df[&#39;close_shift_1&#39;].rolling(window=20).mean() n&quot;, &quot; train_df[&#39;close_shift_1_upper_bb&#39;] = train_df[&#39;close_shift_1_ma20&#39;] + 2 * train_df[&#39;close_shift_1_volatility&#39;] n&quot;, &quot; train_df[&#39;close_shift_1_lower_bb&#39;] = train_df[&#39;close_shift_1_ma20&#39;] - 2 * train_df[&#39;close_shift_1_volatility&#39;] n&quot;, &quot; train_df[&#39;close_momentum_5&#39;] = train_df[&#39;close&#39;] - train_df[&#39;close&#39;].shift(5) n&quot;, &quot; train_df[&#39;close_to_shift_ratio&#39;] = train_df[&#39;close&#39;] / train_df[&#39;close_shift_1&#39;] n&quot;, &quot; n&quot;, &quot; n&quot;, &quot; return train_df n&quot;, &quot; n&quot;, &quot; n&quot;, &quot; n&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 9, &quot;id&quot;: &quot;b1b5dd67&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:32:08.267367Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:32:08.266966Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:32:11.399354Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:32:11.398023Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 3.148227, &quot;end_time&quot;: &quot;2024-10-31T20:32:11.402263&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:32:08.254036&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;train_df = Feature_Engineering(train_df) n&quot;, &quot;test_df = Feature_Engineering(test_df) n&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 10, &quot;id&quot;: &quot;56e70f67&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:32:11.426147Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:32:11.425672Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:32:12.115318Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:32:12.113820Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 0.705285, &quot;end_time&quot;: &quot;2024-10-31T20:32:12.118464&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:32:11.413179&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;train_df = train_df.dropna()&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 11, &quot;id&quot;: &quot;658b8ed6&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:32:12.143519Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:32:12.143104Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:32:12.151788Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:32:12.150566Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 0.024403, &quot;end_time&quot;: &quot;2024-10-31T20:32:12.154645&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:32:12.130242&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [ { &quot;data&quot;: { &quot;text/plain&quot;: [ &quot;Index([&#39;open&#39;, &#39;high&#39;, &#39;low&#39;, &#39;close&#39;, &#39;volume&#39;, &#39;quote_asset_volume&#39;, n&quot;, &quot; &#39;number_of_trades&#39;, &#39;taker_buy_base_volume&#39;, &#39;taker_buy_quote_volume&#39;, n&quot;, &quot; &#39;target&#39;, &#39;minute&#39;, &#39;close_shift_1&#39;, &#39;close_shift_2&#39;, &#39;close_shift_3&#39;, n&quot;, &quot; &#39;close_shift_4&#39;, &#39;close_shift_5&#39;, &#39;sma_5&#39;, &#39;sma_10&#39;, &#39;sma_shift_5&#39;, n&quot;, &quot; &#39;sma_shiftmedian_5&#39;, &#39;sma_shiftmedian_10&#39;, &#39;sma_20&#39;, &#39;bb_up&#39;, &#39;bb_low&#39;, n&quot;, &quot; &#39;ema_5&#39;, &#39;ema_10&#39;, &#39;ema_shift_5&#39;, &#39;ema_crossover&#39;, &#39;close_diff&#39;, n&quot;, &quot; &#39;close_shift_1_volatility&#39;, &#39;close_shift_1_ma20&#39;, n&quot;, &quot; &#39;close_shift_1_upper_bb&#39;, &#39;close_shift_1_lower_bb&#39;, &#39;close_momentum_5&#39;, n&quot;, &quot; &#39;close_to_shift_ratio&#39;], n&quot;, &quot; dtype=&#39;object&#39;)&quot; ] }, &quot;execution_count&quot;: 11, &quot;metadata&quot;: {}, &quot;output_type&quot;: &quot;execute_result&quot; } ], &quot;source&quot;: [ &quot;train_df.columns&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 12, &quot;id&quot;: &quot;32cb3c9c&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:32:12.179047Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:32:12.178573Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:32:16.294470Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:32:16.293360Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 4.131584, &quot;end_time&quot;: &quot;2024-10-31T20:32:16.297537&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:32:12.165953&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;from sklearn.model_selection import train_test_split n&quot;, &quot; n&quot;, &quot;X, y = train_df.drop(columns=[&#39;target&#39;]), train_df[&#39;target&#39;] n&quot;, &quot;X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) n&quot;, &quot; n&quot;, &quot;from sklearn.preprocessing import MinMaxScaler n&quot;, &quot; n&quot;, &quot;scaler = MinMaxScaler() n&quot;, &quot;X_train_scaled = scaler.fit_transform(X_train) n&quot;, &quot;X_test_scaled = scaler.transform(X_test)&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 13, &quot;id&quot;: &quot;d7b97b55&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:32:16.322228Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:32:16.321771Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:45:01.488068Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:45:01.486822Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 765.204331, &quot;end_time&quot;: &quot;2024-10-31T20:45:01.513183&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:32:16.308852&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [ { &quot;name&quot;: &quot;stderr&quot;, &quot;output_type&quot;: &quot;stream&quot;, &quot;text&quot;: [ &quot;[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. n&quot; ] }, { &quot;name&quot;: &quot;stdout&quot;, &quot;output_type&quot;: &quot;stream&quot;, &quot;text&quot;: [ &quot;building tree 1 of 110 n&quot;, &quot;building tree 2 of 110 n&quot;, &quot;building tree 3 of 110 n&quot;, &quot;building tree 4 of 110 n&quot;, &quot;building tree 5 of 110 n&quot;, &quot;building tree 6 of 110 n&quot;, &quot;building tree 7 of 110 n&quot;, &quot;building tree 8 of 110 n&quot;, &quot;building tree 9 of 110 n&quot;, &quot;building tree 10 of 110 n&quot;, &quot;building tree 11 of 110 n&quot;, &quot;building tree 12 of 110 n&quot;, &quot;building tree 13 of 110 n&quot;, &quot;building tree 14 of 110 n&quot;, &quot;building tree 15 of 110 n&quot;, &quot;building tree 16 of 110 n&quot;, &quot;building tree 17 of 110 n&quot;, &quot;building tree 18 of 110 n&quot;, &quot;building tree 19 of 110 n&quot;, &quot;building tree 20 of 110 n&quot;, &quot;building tree 21 of 110 n&quot;, &quot;building tree 22 of 110 n&quot;, &quot;building tree 23 of 110 n&quot;, &quot;building tree 24 of 110 n&quot;, &quot;building tree 25 of 110 n&quot;, &quot;building tree 26 of 110 n&quot;, &quot;building tree 27 of 110 n&quot;, &quot;building tree 28 of 110 n&quot;, &quot;building tree 29 of 110 n&quot;, &quot;building tree 30 of 110 n&quot;, &quot;building tree 31 of 110 n&quot;, &quot;building tree 32 of 110 n&quot;, &quot;building tree 33 of 110 n&quot;, &quot;building tree 34 of 110 n&quot;, &quot;building tree 35 of 110 n&quot;, &quot;building tree 36 of 110 n&quot;, &quot;building tree 37 of 110 n&quot; ] }, { &quot;name&quot;: &quot;stderr&quot;, &quot;output_type&quot;: &quot;stream&quot;, &quot;text&quot;: [ &quot;[Parallel(n_jobs=-1)]: Done 33 tasks | elapsed: 4.0min n&quot; ] }, { &quot;name&quot;: &quot;stdout&quot;, &quot;output_type&quot;: &quot;stream&quot;, &quot;text&quot;: [ &quot;building tree 38 of 110 n&quot;, &quot;building tree 39 of 110 n&quot;, &quot;building tree 40 of 110 n&quot;, &quot;building tree 41 of 110 n&quot;, &quot;building tree 42 of 110 n&quot;, &quot;building tree 43 of 110 n&quot;, &quot;building tree 44 of 110 n&quot;, &quot;building tree 45 of 110 n&quot;, &quot;building tree 46 of 110 n&quot;, &quot;building tree 47 of 110 n&quot;, &quot;building tree 48 of 110 n&quot;, &quot;building tree 49 of 110 n&quot;, &quot;building tree 50 of 110 n&quot;, &quot;building tree 51 of 110 n&quot;, &quot;building tree 52 of 110 n&quot;, &quot;building tree 53 of 110 n&quot;, &quot;building tree 54 of 110 n&quot;, &quot;building tree 55 of 110 n&quot;, &quot;building tree 56 of 110 n&quot;, &quot;building tree 57 of 110 n&quot;, &quot;building tree 58 of 110 n&quot;, &quot;building tree 59 of 110 n&quot;, &quot;building tree 60 of 110 n&quot;, &quot;building tree 61 of 110 n&quot;, &quot;building tree 62 of 110 n&quot;, &quot;building tree 63 of 110 n&quot;, &quot;building tree 64 of 110 n&quot;, &quot;building tree 65 of 110 n&quot;, &quot;building tree 66 of 110 n&quot;, &quot;building tree 67 of 110 n&quot;, &quot;building tree 68 of 110 n&quot;, &quot;building tree 69 of 110 n&quot;, &quot;building tree 70 of 110 n&quot;, &quot;building tree 71 of 110 n&quot;, &quot;building tree 72 of 110 n&quot;, &quot;building tree 73 of 110 n&quot;, &quot;building tree 74 of 110 n&quot;, &quot;building tree 75 of 110 n&quot;, &quot;building tree 76 of 110 n&quot;, &quot;building tree 77 of 110 n&quot;, &quot;building tree 78 of 110 n&quot;, &quot;building tree 79 of 110 n&quot;, &quot;building tree 80 of 110 n&quot;, &quot;building tree 81 of 110 n&quot;, &quot;building tree 82 of 110 n&quot;, &quot;building tree 83 of 110 n&quot;, &quot;building tree 84 of 110 n&quot;, &quot;building tree 85 of 110 n&quot;, &quot;building tree 86 of 110 n&quot;, &quot;building tree 87 of 110 n&quot;, &quot;building tree 88 of 110 n&quot;, &quot;building tree 89 of 110 n&quot;, &quot;building tree 90 of 110 n&quot;, &quot;building tree 91 of 110 n&quot;, &quot;building tree 92 of 110 n&quot;, &quot;building tree 93 of 110 n&quot;, &quot;building tree 94 of 110 n&quot;, &quot;building tree 95 of 110 n&quot;, &quot;building tree 96 of 110 n&quot;, &quot;building tree 97 of 110 n&quot;, &quot;building tree 98 of 110 n&quot;, &quot;building tree 99 of 110 n&quot;, &quot;building tree 100 of 110 n&quot;, &quot;building tree 101 of 110 n&quot;, &quot;building tree 102 of 110 n&quot;, &quot;building tree 103 of 110 n&quot;, &quot;building tree 104 of 110 n&quot;, &quot;building tree 105 of 110 n&quot;, &quot;building tree 106 of 110 n&quot;, &quot;building tree 107 of 110 n&quot;, &quot;building tree 108 of 110 n&quot;, &quot;building tree 109 of 110 n&quot;, &quot;building tree 110 of 110 n&quot; ] }, { &quot;name&quot;: &quot;stderr&quot;, &quot;output_type&quot;: &quot;stream&quot;, &quot;text&quot;: [ &quot;[Parallel(n_jobs=-1)]: Done 110 out of 110 | elapsed: 12.7min finished n&quot; ] }, { &quot;data&quot;: { &quot;text/html&quot;: [ &quot;&lt;div id= &quot;sk-container-id-1 &quot; class= &quot;sk-top-container &quot;&gt;&lt;div class= &quot;sk-text-repr-fallback &quot;&gt;RandomForestClassifier(max_depth=40, n_estimators=110, n_jobs=-1, n&quot;, &quot; random_state=42, verbose=2) . In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.&lt;/div&gt;&lt;div class= &quot;sk-container &quot; hidden&gt;&lt;div class= &quot;sk-item &quot;&gt;&lt;div class= &quot;sk-estimator sk-toggleable &quot;&gt;&lt;input class= &quot;sk-toggleable__control sk-hidden--visually &quot; id= &quot;sk-estimator-id-1 &quot; type= &quot;checkbox &quot; checked&gt;&lt;label for= &quot;sk-estimator-id-1 &quot; class= &quot;sk-toggleable__label sk-toggleable__label-arrow &quot;&gt;RandomForestClassifier&lt;/label&gt;&lt;div class= &quot;sk-toggleable__content &quot;&gt;RandomForestClassifier(max_depth=40, n_estimators=110, n_jobs=-1, n&quot;, &quot; random_state=42, verbose=2) . &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&quot; ], &quot;text/plain&quot;: [ &quot;RandomForestClassifier(max_depth=40, n_estimators=110, n_jobs=-1, n&quot;, &quot; random_state=42, verbose=2)&quot; ] }, &quot;execution_count&quot;: 13, &quot;metadata&quot;: {}, &quot;output_type&quot;: &quot;execute_result&quot; } ], &quot;source&quot;: [ &quot;from sklearn.ensemble import RandomForestClassifier n&quot;, &quot; n&quot;, &quot;model = RandomForestClassifier(random_state=42, n_estimators=110, n_jobs=-1, verbose=2, max_depth=40) n&quot;, &quot;model.fit(X_train_scaled, y_train)&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 14, &quot;id&quot;: &quot;7530f756&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:45:01.559190Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:45:01.558697Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:45:04.333449Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:45:04.332325Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 2.801454, &quot;end_time&quot;: &quot;2024-10-31T20:45:04.336213&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:45:01.534759&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [ { &quot;name&quot;: &quot;stderr&quot;, &quot;output_type&quot;: &quot;stream&quot;, &quot;text&quot;: [ &quot;[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers. n&quot;, &quot;[Parallel(n_jobs=4)]: Done 33 tasks | elapsed: 0.9s n&quot;, &quot;[Parallel(n_jobs=4)]: Done 110 out of 110 | elapsed: 2.7s finished n&quot; ] } ], &quot;source&quot;: [ &quot;y_predictions = model.predict(X_test_scaled) n&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 15, &quot;id&quot;: &quot;640f4149&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:45:04.381941Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:45:04.381470Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:45:05.567134Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:45:05.566008Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 1.211725, &quot;end_time&quot;: &quot;2024-10-31T20:45:05.569931&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:45:04.358206&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [ { &quot;name&quot;: &quot;stdout&quot;, &quot;output_type&quot;: &quot;stream&quot;, &quot;text&quot;: [ &quot;0.9983227252418865 n&quot;, &quot; precision recall f1-score support n&quot;, &quot; n&quot;, &quot; 0.0 1.00 1.00 1.00 215042 n&quot;, &quot; 1.0 1.00 1.00 1.00 194551 n&quot;, &quot; n&quot;, &quot; accuracy 1.00 409593 n&quot;, &quot; macro avg 1.00 1.00 1.00 409593 n&quot;, &quot;weighted avg 1.00 1.00 1.00 409593 n&quot;, &quot; n&quot; ] } ], &quot;source&quot;: [ &quot;from sklearn.metrics import accuracy_score, classification_report n&quot;, &quot; n&quot;, &quot;print(accuracy_score(y_test, y_predictions)) n&quot;, &quot; n&quot;, &quot;print(classification_report(y_test, y_predictions))&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 16, &quot;id&quot;: &quot;e46c234c&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:45:05.615368Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:45:05.614941Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:45:05.671265Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:45:05.670263Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 0.082169, &quot;end_time&quot;: &quot;2024-10-31T20:45:05.673908&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:45:05.591739&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [ { &quot;data&quot;: { &quot;text/html&quot;: [ &quot; n&quot;, &quot; n&quot;, &quot;&lt;table border= &quot;1 &quot; class= &quot;dataframe &quot;&gt; n&quot;, &quot; n&quot;, &quot; &lt;tr style= &quot;text-align: right; &quot;&gt; n&quot;, &quot; n&quot;, &quot; feature n&quot;, &quot; importance n&quot;, &quot; &lt;/tr&gt; n&quot;, &quot; n&quot;, &quot; n&quot;, &quot; n&quot;, &quot; 27 n&quot;, &quot; close_diff | n&quot;, &quot; 0.536047 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 33 n&quot;, &quot; close_to_shift_ratio | n&quot;, &quot; 0.454401 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 28 n&quot;, &quot; close_shift_1_volatility | n&quot;, &quot; 0.000878 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 32 n&quot;, &quot; close_momentum_5 | n&quot;, &quot; 0.000794 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 26 n&quot;, &quot; ema_crossover | n&quot;, &quot; 0.000621 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 6 n&quot;, &quot; number_of_trades | n&quot;, &quot; 0.000494 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 4 n&quot;, &quot; volume | n&quot;, &quot; 0.000389 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 5 n&quot;, &quot; quote_asset_volume | n&quot;, &quot; 0.000388 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 8 n&quot;, &quot; taker_buy_quote_volume | n&quot;, &quot; 0.000362 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 7 n&quot;, &quot; taker_buy_base_volume | n&quot;, &quot; 0.000345 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 10 n&quot;, &quot; close_shift_1 | n&quot;, &quot; 0.000315 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 3 n&quot;, &quot; close | n&quot;, &quot; 0.000279 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 11 n&quot;, &quot; close_shift_2 | n&quot;, &quot; 0.000250 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 9 n&quot;, &quot; minute | n&quot;, &quot; 0.000236 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 12 n&quot;, &quot; close_shift_3 | n&quot;, &quot; 0.000233 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 13 n&quot;, &quot; close_shift_4 | n&quot;, &quot; 0.000228 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 31 n&quot;, &quot; close_shift_1_lower_bb | n&quot;, &quot; 0.000224 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 1 n&quot;, &quot; high | n&quot;, &quot; 0.000224 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 24 n&quot;, &quot; ema_10 | n&quot;, &quot; 0.000221 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 22 n&quot;, &quot; bb_low | n&quot;, &quot; 0.000221 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 23 n&quot;, &quot; ema_5 | n&quot;, &quot; 0.000217 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 2 n&quot;, &quot; low | n&quot;, &quot; 0.000216 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 15 n&quot;, &quot; sma_5 | n&quot;, &quot; 0.000215 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 21 n&quot;, &quot; bb_up | n&quot;, &quot; 0.000214 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 14 n&quot;, &quot; close_shift_5 | n&quot;, &quot; 0.000214 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 30 n&quot;, &quot; close_shift_1_upper_bb | n&quot;, &quot; 0.000207 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 0 n&quot;, &quot; open | n&quot;, &quot; 0.000206 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 16 n&quot;, &quot; sma_10 | n&quot;, &quot; 0.000201 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 25 n&quot;, &quot; ema_shift_5 | n&quot;, &quot; 0.000200 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 20 n&quot;, &quot; sma_20 | n&quot;, &quot; 0.000198 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 17 n&quot;, &quot; sma_shift_5 | n&quot;, &quot; 0.000196 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 29 n&quot;, &quot; close_shift_1_ma20 | n&quot;, &quot; 0.000195 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 19 n&quot;, &quot; sma_shiftmedian_10 | n&quot;, &quot; 0.000185 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot; 18 n&quot;, &quot; sma_shiftmedian_5 | n&quot;, &quot; 0.000184 | n&quot;, &quot; . n&quot;, &quot; n&quot;, &quot;&lt;/table&gt; n&quot;, &quot;&quot; ], &quot;text/plain&quot;: [ &quot; feature importance n&quot;, &quot;27 close_diff 0.536047 n&quot;, &quot;33 close_to_shift_ratio 0.454401 n&quot;, &quot;28 close_shift_1_volatility 0.000878 n&quot;, &quot;32 close_momentum_5 0.000794 n&quot;, &quot;26 ema_crossover 0.000621 n&quot;, &quot;6 number_of_trades 0.000494 n&quot;, &quot;4 volume 0.000389 n&quot;, &quot;5 quote_asset_volume 0.000388 n&quot;, &quot;8 taker_buy_quote_volume 0.000362 n&quot;, &quot;7 taker_buy_base_volume 0.000345 n&quot;, &quot;10 close_shift_1 0.000315 n&quot;, &quot;3 close 0.000279 n&quot;, &quot;11 close_shift_2 0.000250 n&quot;, &quot;9 minute 0.000236 n&quot;, &quot;12 close_shift_3 0.000233 n&quot;, &quot;13 close_shift_4 0.000228 n&quot;, &quot;31 close_shift_1_lower_bb 0.000224 n&quot;, &quot;1 high 0.000224 n&quot;, &quot;24 ema_10 0.000221 n&quot;, &quot;22 bb_low 0.000221 n&quot;, &quot;23 ema_5 0.000217 n&quot;, &quot;2 low 0.000216 n&quot;, &quot;15 sma_5 0.000215 n&quot;, &quot;21 bb_up 0.000214 n&quot;, &quot;14 close_shift_5 0.000214 n&quot;, &quot;30 close_shift_1_upper_bb 0.000207 n&quot;, &quot;0 open 0.000206 n&quot;, &quot;16 sma_10 0.000201 n&quot;, &quot;25 ema_shift_5 0.000200 n&quot;, &quot;20 sma_20 0.000198 n&quot;, &quot;17 sma_shift_5 0.000196 n&quot;, &quot;29 close_shift_1_ma20 0.000195 n&quot;, &quot;19 sma_shiftmedian_10 0.000185 n&quot;, &quot;18 sma_shiftmedian_5 0.000184&quot; ] }, &quot;execution_count&quot;: 16, &quot;metadata&quot;: {}, &quot;output_type&quot;: &quot;execute_result&quot; } ], &quot;source&quot;: [ &quot;importance_df = pd.DataFrame({ n&quot;, &quot; &#39;feature&#39;: X_train.columns, n&quot;, &quot; &#39;importance&#39;: model.feature_importances_ n&quot;, &quot;}).sort_values(&#39;importance&#39;, ascending=False) n&quot;, &quot; n&quot;, &quot;importance_df&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 17, &quot;id&quot;: &quot;7eef3532&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:45:05.721663Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:45:05.721221Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:45:05.731865Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:45:05.730403Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 0.037311, &quot;end_time&quot;: &quot;2024-10-31T20:45:05.734419&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:45:05.697108&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [ { &quot;data&quot;: { &quot;text/plain&quot;: [ &quot;[36, n&quot;, &quot; 36, n&quot;, &quot; 38, n&quot;, &quot; 36, n&quot;, &quot; 40, n&quot;, &quot; 34, n&quot;, &quot; 39, n&quot;, &quot; 36, n&quot;, &quot; 40, n&quot;, &quot; 40, n&quot;, &quot; 37, n&quot;, &quot; 40, n&quot;, &quot; 39, n&quot;, &quot; 32, n&quot;, &quot; 39, n&quot;, &quot; 40, n&quot;, &quot; 35, n&quot;, &quot; 35, n&quot;, &quot; 38, n&quot;, &quot; 34, n&quot;, &quot; 38, n&quot;, &quot; 40, n&quot;, &quot; 39, n&quot;, &quot; 35, n&quot;, &quot; 33, n&quot;, &quot; 37, n&quot;, &quot; 37, n&quot;, &quot; 40, n&quot;, &quot; 34, n&quot;, &quot; 34, n&quot;, &quot; 38, n&quot;, &quot; 36, n&quot;, &quot; 32, n&quot;, &quot; 34, n&quot;, &quot; 36, n&quot;, &quot; 38, n&quot;, &quot; 34, n&quot;, &quot; 36, n&quot;, &quot; 39, n&quot;, &quot; 35, n&quot;, &quot; 38, n&quot;, &quot; 40, n&quot;, &quot; 34, n&quot;, &quot; 39, n&quot;, &quot; 40, n&quot;, &quot; 34, n&quot;, &quot; 32, n&quot;, &quot; 36, n&quot;, &quot; 38, n&quot;, &quot; 37, n&quot;, &quot; 40, n&quot;, &quot; 35, n&quot;, &quot; 40, n&quot;, &quot; 34, n&quot;, &quot; 36, n&quot;, &quot; 29, n&quot;, &quot; 40, n&quot;, &quot; 40, n&quot;, &quot; 35, n&quot;, &quot; 35, n&quot;, &quot; 38, n&quot;, &quot; 34, n&quot;, &quot; 33, n&quot;, &quot; 40, n&quot;, &quot; 35, n&quot;, &quot; 35, n&quot;, &quot; 33, n&quot;, &quot; 38, n&quot;, &quot; 36, n&quot;, &quot; 40, n&quot;, &quot; 40, n&quot;, &quot; 36, n&quot;, &quot; 36, n&quot;, &quot; 39, n&quot;, &quot; 40, n&quot;, &quot; 37, n&quot;, &quot; 38, n&quot;, &quot; 36, n&quot;, &quot; 40, n&quot;, &quot; 40, n&quot;, &quot; 40, n&quot;, &quot; 32, n&quot;, &quot; 40, n&quot;, &quot; 40, n&quot;, &quot; 36, n&quot;, &quot; 39, n&quot;, &quot; 36, n&quot;, &quot; 38, n&quot;, &quot; 35, n&quot;, &quot; 37, n&quot;, &quot; 34, n&quot;, &quot; 38, n&quot;, &quot; 40, n&quot;, &quot; 34, n&quot;, &quot; 40, n&quot;, &quot; 37, n&quot;, &quot; 40, n&quot;, &quot; 38, n&quot;, &quot; 35, n&quot;, &quot; 37, n&quot;, &quot; 38, n&quot;, &quot; 38, n&quot;, &quot; 32, n&quot;, &quot; 37, n&quot;, &quot; 40, n&quot;, &quot; 34, n&quot;, &quot; 36, n&quot;, &quot; 37, n&quot;, &quot; 38, n&quot;, &quot; 33]&quot; ] }, &quot;execution_count&quot;: 17, &quot;metadata&quot;: {}, &quot;output_type&quot;: &quot;execute_result&quot; } ], &quot;source&quot;: [ &quot;tree_depths = [estimator.tree_.max_depth for estimator in model.estimators_] n&quot;, &quot;tree_depths&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 18, &quot;id&quot;: &quot;4f67e2aa&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:45:05.782716Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:45:05.782289Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:45:05.789381Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:45:05.788107Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 0.034491, &quot;end_time&quot;: &quot;2024-10-31T20:45:05.791868&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:45:05.757377&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;input_cols=[&#39;open&#39;, &#39;high&#39;, &#39;low&#39;, &#39;close&#39;, &#39;volume&#39;, &#39;quote_asset_volume&#39;, n&quot;, &quot; &#39;number_of_trades&#39;, &#39;taker_buy_base_volume&#39;, &#39;taker_buy_quote_volume&#39;, n&quot;, &quot; &#39;minute&#39;, &#39;close_shift_1&#39;, &#39;close_shift_2&#39;, &#39;close_shift_3&#39;, n&quot;, &quot; &#39;close_shift_4&#39;, &#39;close_shift_5&#39;, &#39;sma_5&#39;, &#39;sma_10&#39;, &#39;sma_shift_5&#39;, n&quot;, &quot; &#39;sma_shiftmedian_5&#39;, &#39;sma_shiftmedian_10&#39;, &#39;sma_20&#39;, &#39;bb_up&#39;, &#39;bb_low&#39;, n&quot;, &quot; &#39;ema_5&#39;, &#39;ema_10&#39;, &#39;ema_shift_5&#39;, &#39;ema_crossover&#39;, &#39;close_diff&#39;, n&quot;, &quot; &#39;close_shift_1_volatility&#39;, &#39;close_shift_1_ma20&#39;, n&quot;, &quot; &#39;close_shift_1_upper_bb&#39;, &#39;close_shift_1_lower_bb&#39;, &#39;close_momentum_5&#39;, n&quot;, &quot; &#39;close_to_shift_ratio&#39;]&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 19, &quot;id&quot;: &quot;da789fb8&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:45:05.841418Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:45:05.840478Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:45:06.947900Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:45:06.946119Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 1.135731, &quot;end_time&quot;: &quot;2024-10-31T20:45:06.951335&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:45:05.815604&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;from sklearn.impute import SimpleImputer n&quot;, &quot; n&quot;, &quot;imputer = SimpleImputer(strategy=&#39;mean&#39;) n&quot;, &quot;test_df[input_cols] = imputer.fit_transform(test_df[input_cols]) n&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 20, &quot;id&quot;: &quot;6bc6563f&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:45:07.005478Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:45:07.005033Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:45:07.493093Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:45:07.491981Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 0.516448, &quot;end_time&quot;: &quot;2024-10-31T20:45:07.495809&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:45:06.979361&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;test_df[input_cols] = scaler.transform(test_df[input_cols])&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 21, &quot;id&quot;: &quot;76065c4c&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:45:07.543623Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:45:07.543191Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:45:07.720891Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:45:07.719676Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 0.204867, &quot;end_time&quot;: &quot;2024-10-31T20:45:07.723641&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:45:07.518774&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;test_df = test_df.drop([&#39;row_id&#39;], axis=1)&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 22, &quot;id&quot;: &quot;880319bd&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:45:07.772214Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:45:07.771118Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:45:07.779574Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:45:07.778175Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 0.035428, &quot;end_time&quot;: &quot;2024-10-31T20:45:07.782170&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:45:07.746742&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [ { &quot;data&quot;: { &quot;text/plain&quot;: [ &quot;Index([&#39;open&#39;, &#39;high&#39;, &#39;low&#39;, &#39;close&#39;, &#39;volume&#39;, &#39;quote_asset_volume&#39;, n&quot;, &quot; &#39;number_of_trades&#39;, &#39;taker_buy_base_volume&#39;, &#39;taker_buy_quote_volume&#39;, n&quot;, &quot; &#39;minute&#39;, &#39;close_shift_1&#39;, &#39;close_shift_2&#39;, &#39;close_shift_3&#39;, n&quot;, &quot; &#39;close_shift_4&#39;, &#39;close_shift_5&#39;, &#39;sma_5&#39;, &#39;sma_10&#39;, &#39;sma_shift_5&#39;, n&quot;, &quot; &#39;sma_shiftmedian_5&#39;, &#39;sma_shiftmedian_10&#39;, &#39;sma_20&#39;, &#39;bb_up&#39;, &#39;bb_low&#39;, n&quot;, &quot; &#39;ema_5&#39;, &#39;ema_10&#39;, &#39;ema_shift_5&#39;, &#39;ema_crossover&#39;, &#39;close_diff&#39;, n&quot;, &quot; &#39;close_shift_1_volatility&#39;, &#39;close_shift_1_ma20&#39;, n&quot;, &quot; &#39;close_shift_1_upper_bb&#39;, &#39;close_shift_1_lower_bb&#39;, &#39;close_momentum_5&#39;, n&quot;, &quot; &#39;close_to_shift_ratio&#39;], n&quot;, &quot; dtype=&#39;object&#39;)&quot; ] }, &quot;execution_count&quot;: 22, &quot;metadata&quot;: {}, &quot;output_type&quot;: &quot;execute_result&quot; } ], &quot;source&quot;: [ &quot;test_df.columns&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 23, &quot;id&quot;: &quot;9071ceda&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:45:07.830823Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:45:07.830344Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:45:12.960616Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:45:12.959531Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 5.157599, &quot;end_time&quot;: &quot;2024-10-31T20:45:12.963195&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:45:07.805596&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [ { &quot;name&quot;: &quot;stderr&quot;, &quot;output_type&quot;: &quot;stream&quot;, &quot;text&quot;: [ &quot;/opt/conda/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names n&quot;, &quot; warnings.warn( n&quot;, &quot;[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers. n&quot;, &quot;[Parallel(n_jobs=4)]: Done 33 tasks | elapsed: 1.5s n&quot;, &quot;[Parallel(n_jobs=4)]: Done 110 out of 110 | elapsed: 5.0s finished n&quot; ] } ], &quot;source&quot;: [ &quot;test_target=model.predict(test_df)&quot; ] }, { &quot;cell_type&quot;: &quot;code&quot;, &quot;execution_count&quot;: 24, &quot;id&quot;: &quot;6b648597&quot;, &quot;metadata&quot;: { &quot;execution&quot;: { &quot;iopub.execute_input&quot;: &quot;2024-10-31T20:45:13.014240Z&quot;, &quot;iopub.status.busy&quot;: &quot;2024-10-31T20:45:13.013357Z&quot;, &quot;iopub.status.idle&quot;: &quot;2024-10-31T20:45:14.655387Z&quot;, &quot;shell.execute_reply&quot;: &quot;2024-10-31T20:45:14.654163Z&quot; }, &quot;papermill&quot;: { &quot;duration&quot;: 1.67004, &quot;end_time&quot;: &quot;2024-10-31T20:45:14.658337&quot;, &quot;exception&quot;: false, &quot;start_time&quot;: &quot;2024-10-31T20:45:12.988297&quot;, &quot;status&quot;: &quot;completed&quot; }, &quot;tags&quot;: [] }, &quot;outputs&quot;: [], &quot;source&quot;: [ &quot;submission = pd.DataFrame({ n&quot;, &quot; &#39;row_id&#39;: range(0, 909617), n&quot;, &quot; &#39;target&#39;: test_target n&quot;, &quot;}) n&quot;, &quot; n&quot;, &quot;submission.to_csv(&#39;submission.csv&#39;, index=False)&quot; ] } ], &quot;metadata&quot;: { &quot;kaggle&quot;: { &quot;accelerator&quot;: &quot;none&quot;, &quot;dataSources&quot;: [ { &quot;databundleVersionId&quot;: 9630059, &quot;sourceId&quot;: 85340, &quot;sourceType&quot;: &quot;competition&quot; } ], &quot;isGpuEnabled&quot;: false, &quot;isInternetEnabled&quot;: true, &quot;language&quot;: &quot;python&quot;, &quot;sourceType&quot;: &quot;notebook&quot; }, &quot;kernelspec&quot;: { &quot;display_name&quot;: &quot;Python 3&quot;, &quot;language&quot;: &quot;python&quot;, &quot;name&quot;: &quot;python3&quot; }, &quot;language_info&quot;: { &quot;codemirror_mode&quot;: { &quot;name&quot;: &quot;ipython&quot;, &quot;version&quot;: 3 }, &quot;file_extension&quot;: &quot;.py&quot;, &quot;mimetype&quot;: &quot;text/x-python&quot;, &quot;name&quot;: &quot;python&quot;, &quot;nbconvert_exporter&quot;: &quot;python&quot;, &quot;pygments_lexer&quot;: &quot;ipython3&quot;, &quot;version&quot;: &quot;3.10.14&quot; }, &quot;papermill&quot;: { &quot;default_parameters&quot;: {}, &quot;duration&quot;: 807.11308, &quot;end_time&quot;: &quot;2024-10-31T20:45:15.506152&quot;, &quot;environment_variables&quot;: {}, &quot;exception&quot;: null, &quot;input_path&quot;: &quot;__notebook__.ipynb&quot;, &quot;output_path&quot;: &quot;__notebook__.ipynb&quot;, &quot;parameters&quot;: {}, &quot;start_time&quot;: &quot;2024-10-31T20:31:48.393072&quot;, &quot;version&quot;: &quot;2.6.0&quot; } }, &quot;nbformat&quot;: 4, &quot;nbformat_minor&quot;: 5 } .",
            "url": "https://kozodoi.me/blog/20241002/Directional-Forecasting",
            "relUrl": "/blog/20241002/Directional-Forecasting",
            "date": " • Oct 2, 2024"
        }
        
    
  
    
        ,"post3": {
            "title": "Disease Recognizer",
            "content": "Overview . . Disease Recognizer uses sentence embeddings generated by the sentence-transformers/all-MiniLM-L6-v2 model to encode patient symptoms into a high-dimensional space. Machine learning algorithms, including Logistic Regression and KMeans Clustering, are employed to classify and group symptoms, ultimately predicting the associated disease. . Features . . Symptom Embedding: Converts text-based symptoms into embeddings using a pre-trained transformer model. | Disease Prediction: Classifies symptoms into disease categories using Logistic Regression. | Clustering: Groups similar symptoms using KMeans Clustering. | Data Visualization: Visualizes the embedded symptom data using t-SNE plots. | Interactive Prediction: Allows for real-time disease prediction based on new symptom inputs. | . Direct Run . Go to this URL: https://disease-recogniser-nlp-team-ais.streamlit.app/ . Repo Link . LINK -&gt; https://github.com/Abhi2april/Disease-Recogniser .",
            "url": "https://kozodoi.me/blog/20240801/disease-detection",
            "relUrl": "/blog/20240801/disease-detection",
            "date": " • Aug 1, 2024"
        }
        
    
  
    
        ,"post4": {
            "title": "Title",
            "content": "import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns . train_df = pd.read_csv(&#39;/kaggle/input/directional-forecasting-in-cryptocurrencies/train.csv&#39;) test_df = pd.read_csv(&#39;/kaggle/input/directional-forecasting-in-cryptocurrencies/test.csv&#39;) train_df . timestamp open high low close volume quote_asset_volume number_of_trades taker_buy_base_volume taker_buy_quote_volume target . 0 1525471260 | 0.90120 | 0.90130 | 0.90120 | 0.90130 | 134.98 | 121.646459 | 4.0 | 125.08 | 112.723589 | 1.0 | . 1 1525471320 | 0.90185 | 0.90195 | 0.90185 | 0.90195 | 1070.54 | 965.505313 | 12.0 | 879.94 | 793.612703 | 0.0 | . 2 1525471380 | 0.90140 | 0.90140 | 0.90139 | 0.90139 | 2293.06 | 2066.963991 | 5.0 | 0.00 | 0.000000 | 0.0 | . 3 1525471440 | 0.90139 | 0.90140 | 0.90138 | 0.90139 | 6850.59 | 6175.000909 | 19.0 | 1786.30 | 1610.149485 | 0.0 | . 4 1525471500 | 0.90139 | 0.90139 | 0.90130 | 0.90130 | 832.30 | 750.222624 | 3.0 | 784.82 | 707.428900 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 2122433 1652817240 | 0.43060 | 0.43060 | 0.42990 | 0.43040 | 136274.00 | 58630.162800 | 144.0 | 54216.00 | 23325.927700 | 1.0 | . 2122434 1652817300 | 0.43030 | 0.43070 | 0.43030 | 0.43050 | 104478.00 | 44967.837600 | 99.0 | 52232.00 | 22484.030400 | 1.0 | . 2122435 1652817360 | 0.43050 | 0.43120 | 0.43050 | 0.43090 | 212396.00 | 91526.987200 | 177.0 | 108324.00 | 46673.061600 | 0.0 | . 2122436 1652817420 | 0.43110 | 0.43110 | 0.43040 | 0.43060 | 131047.00 | 56443.003800 | 107.0 | 32713.00 | 14097.148900 | 0.0 | . 2122437 1652817480 | 0.43060 | 0.43080 | 0.43010 | 0.43010 | 101150.00 | 43542.262900 | 105.0 | 46109.00 | 19851.723700 | 1.0 | . 2122438 rows × 11 columns . import seaborn as sns plt.figure(figsize=(20,10)) sns.heatmap(train_df.corr(), annot=True, cmap=&#39;coolwarm&#39;, linewidths=0.5) plt.title(&#39;Correlation Heatmap of Features&#39;) plt.show() . import seaborn as sns import matplotlib.pyplot as plt correlation_matrix = train_df.corr() signal_corr = correlation_matrix[[&#39;target&#39;]].sort_values(by=&#39;target&#39;, ascending=False) plt.figure(figsize=(7, 0.5)) sns.heatmap(signal_corr.T, annot=True, cmap=&#39;coolwarm&#39;, cbar=False) plt.title(&quot;Correlation of Features with &#39;target&#39;&quot;) plt.show() . from datetime import datetime, timezone train_df[&#39;timestamp&#39;] = pd.to_datetime(train_df[&#39;timestamp&#39;], unit=&#39;s&#39;) train_df[&#39;minute&#39;] = train_df[&#39;timestamp&#39;].dt.minute train_df.drop(&#39;timestamp&#39;, axis=1, inplace=True) test_df[&#39;timestamp&#39;] = pd.to_datetime(test_df[&#39;timestamp&#39;], unit=&#39;s&#39;) test_df[&#39;minute&#39;] = test_df[&#39;timestamp&#39;].dt.minute test_df.drop(&#39;timestamp&#39;, axis=1, inplace=True) . #USING Z-SCORE METHOD TO GET RID OF OUTLIERS z_threshold = 3 z_scores = np.abs((train_df - train_df.mean()) / train_df.std()) outliers = (z_scores &gt; z_threshold) print(&quot;Outliers: n&quot;, train_df[outliers.any(axis=1)]) train_df = train_df[~outliers.any(axis=1)] . Outliers: open high low close volume quote_asset_volume 107836 0.50566 0.50631 0.48507 0.50008 1822170.0 9.088897e+05 108201 0.47580 0.47615 0.47500 0.47500 1145163.6 5.439815e+05 197296 0.31937 0.32362 0.31937 0.32278 1811583.1 5.802121e+05 199657 0.36803 0.37200 0.36778 0.37152 1346806.7 4.980468e+05 200189 0.48499 0.49021 0.48356 0.49000 2875628.5 1.396135e+06 ... ... ... ... ... ... ... 2120984 0.42000 0.42800 0.41990 0.42790 2374477.0 1.006244e+06 2120985 0.42790 0.44260 0.42790 0.43930 12363632.0 5.401453e+06 2120986 0.43940 0.43940 0.43300 0.43450 3054547.0 1.332590e+06 2120987 0.43450 0.43600 0.42480 0.42650 6367198.0 2.730539e+06 2120988 0.42650 0.42750 0.42270 0.42270 2871307.0 1.220657e+06 number_of_trades taker_buy_base_volume taker_buy_quote_volume 107836 865.0 1151145.0 5.747437e+05 108201 42.0 1107813.4 5.262225e+05 197296 261.0 1675449.4 5.364960e+05 199657 459.0 1186219.9 4.387031e+05 200189 728.0 2623470.3 1.273743e+06 ... ... ... ... 2120984 1285.0 1749782.0 7.408540e+05 2120985 7547.0 6577021.0 2.870867e+06 2120986 2296.0 1430104.0 6.235863e+05 2120987 3992.0 2906838.0 1.246762e+06 2120988 1939.0 1315831.0 5.595667e+05 target minute 107836 0.0 17 108201 1.0 22 197296 0.0 17 199657 0.0 38 200189 0.0 30 ... ... ... 2120984 1.0 45 2120985 0.0 46 2120986 0.0 47 2120987 0.0 48 2120988 1.0 49 [74452 rows x 11 columns] . train_df[&#39;close_shift_1&#39;] = train_df.shift(-1)[&#39;close&#39;] train_df[&#39;close_shift_2&#39;] = train_df.shift(-2)[&#39;close&#39;] train_df[&#39;close_shift_3&#39;] = train_df.shift(-3)[&#39;close&#39;] train_df[&#39;close_shift_4&#39;] = train_df.shift(-4)[&#39;close&#39;] train_df[&#39;close_shift_5&#39;] = train_df.shift(-5)[&#39;close&#39;] test_df[&#39;close_shift_1&#39;] = test_df.shift(-1)[&#39;close&#39;] test_df[&#39;close_shift_2&#39;] = test_df.shift(-2)[&#39;close&#39;] test_df[&#39;close_shift_3&#39;] = test_df.shift(-3)[&#39;close&#39;] test_df[&#39;close_shift_4&#39;] = test_df.shift(-4)[&#39;close&#39;] test_df[&#39;close_shift_5&#39;] = test_df.shift(-5)[&#39;close&#39;] . /tmp/ipykernel_18/3596558217.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy train_df[&#39;close_shift_1&#39;] = train_df.shift(-1)[&#39;close&#39;] /tmp/ipykernel_18/3596558217.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy train_df[&#39;close_shift_2&#39;] = train_df.shift(-2)[&#39;close&#39;] /tmp/ipykernel_18/3596558217.py:3: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy train_df[&#39;close_shift_3&#39;] = train_df.shift(-3)[&#39;close&#39;] /tmp/ipykernel_18/3596558217.py:4: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy train_df[&#39;close_shift_4&#39;] = train_df.shift(-4)[&#39;close&#39;] /tmp/ipykernel_18/3596558217.py:5: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy train_df[&#39;close_shift_5&#39;] = train_df.shift(-5)[&#39;close&#39;] . def Feature_Engineering(df): train_df = pd.DataFrame(df) # train_df[&#39;true_range&#39;] = train_df[[&#39;high&#39;, &#39;low&#39;, &#39;close&#39;]].max(axis=1) - train_df[[&#39;high&#39;, &#39;low&#39;, &#39;close&#39;]].min(axis=1) --&gt;BAD INDICATORS # train_df[&#39;atr&#39;] = (train_df[[&#39;high&#39;, &#39;low&#39;, &#39;close&#39;]].max(axis=1) - train_df[[&#39;high&#39;, &#39;low&#39;, &#39;close&#39;]].min(axis=1)).rolling(window=14).mean() --&gt;BAD INDICATOR train_df[&#39;sma_5&#39;] = train_df[&#39;close&#39;].rolling(window=5).mean() train_df[&#39;sma_10&#39;] = train_df[&#39;close&#39;].rolling(window=10).mean() train_df[&#39;sma_shift_5&#39;] = train_df[&#39;close_shift_1&#39;].rolling(window=5).mean() train_df[&#39;sma_shiftmedian_5&#39;] = train_df[&#39;close_shift_1&#39;].rolling(window=5).median() train_df[&#39;sma_shiftmedian_10&#39;] = train_df[&#39;close_shift_1&#39;].rolling(window=10).median() train_df[&#39;sma_20&#39;] = train_df[&#39;close&#39;].rolling(window=20).mean() train_df[&#39;bb_up&#39;] = train_df[&#39;sma_20&#39;] + ((train_df[&#39;close&#39;].rolling(window=20).std()) * 2) train_df[&#39;bb_low&#39;] = train_df[&#39;sma_20&#39;] - ((train_df[&#39;close&#39;].rolling(window=20).std()) * 2) train_df[&#39;ema_5&#39;] = train_df[&#39;close&#39;].ewm(span=5, adjust=False).mean() train_df[&#39;ema_10&#39;] = train_df[&#39;close&#39;].ewm(span=10, adjust=False).mean() train_df[&#39;ema_shift_5&#39;] = train_df[&#39;close_shift_1&#39;].ewm(span=5, adjust=False).mean() train_df[&#39;ema_crossover&#39;] = train_df[&#39;ema_5&#39;] - train_df[&#39;ema_10&#39;] train_df[&#39;close_diff&#39;] = train_df[&#39;close&#39;] - train_df[&#39;close_shift_1&#39;] train_df[&#39;close_shift_1_volatility&#39;] = train_df[&#39;close_shift_1&#39;].rolling(window=10).std() train_df[&#39;close_shift_1_ma20&#39;] = train_df[&#39;close_shift_1&#39;].rolling(window=20).mean() train_df[&#39;close_shift_1_upper_bb&#39;] = train_df[&#39;close_shift_1_ma20&#39;] + 2 * train_df[&#39;close_shift_1_volatility&#39;] train_df[&#39;close_shift_1_lower_bb&#39;] = train_df[&#39;close_shift_1_ma20&#39;] - 2 * train_df[&#39;close_shift_1_volatility&#39;] train_df[&#39;close_momentum_5&#39;] = train_df[&#39;close&#39;] - train_df[&#39;close&#39;].shift(5) train_df[&#39;close_to_shift_ratio&#39;] = train_df[&#39;close&#39;] / train_df[&#39;close_shift_1&#39;] return train_df . train_df = Feature_Engineering(train_df) test_df = Feature_Engineering(test_df) . train_df = train_df.dropna() . train_df.columns . Index([&#39;open&#39;, &#39;high&#39;, &#39;low&#39;, &#39;close&#39;, &#39;volume&#39;, &#39;quote_asset_volume&#39;, &#39;number_of_trades&#39;, &#39;taker_buy_base_volume&#39;, &#39;taker_buy_quote_volume&#39;, &#39;target&#39;, &#39;minute&#39;, &#39;close_shift_1&#39;, &#39;close_shift_2&#39;, &#39;close_shift_3&#39;, &#39;close_shift_4&#39;, &#39;close_shift_5&#39;, &#39;sma_5&#39;, &#39;sma_10&#39;, &#39;sma_shift_5&#39;, &#39;sma_shiftmedian_5&#39;, &#39;sma_shiftmedian_10&#39;, &#39;sma_20&#39;, &#39;bb_up&#39;, &#39;bb_low&#39;, &#39;ema_5&#39;, &#39;ema_10&#39;, &#39;ema_shift_5&#39;, &#39;ema_crossover&#39;, &#39;close_diff&#39;, &#39;close_shift_1_volatility&#39;, &#39;close_shift_1_ma20&#39;, &#39;close_shift_1_upper_bb&#39;, &#39;close_shift_1_lower_bb&#39;, &#39;close_momentum_5&#39;, &#39;close_to_shift_ratio&#39;], dtype=&#39;object&#39;) . from sklearn.model_selection import train_test_split X, y = train_df.drop(columns=[&#39;target&#39;]), train_df[&#39;target&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) . from sklearn.ensemble import RandomForestClassifier model = RandomForestClassifier(random_state=42, n_estimators=110, n_jobs=-1, verbose=2, max_depth=40) model.fit(X_train_scaled, y_train) . [Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers. . building tree 1 of 110 building tree 2 of 110 building tree 3 of 110 building tree 4 of 110 building tree 5 of 110 building tree 6 of 110 building tree 7 of 110 building tree 8 of 110 building tree 9 of 110 building tree 10 of 110 building tree 11 of 110 building tree 12 of 110 building tree 13 of 110 building tree 14 of 110 building tree 15 of 110 building tree 16 of 110 building tree 17 of 110 building tree 18 of 110 building tree 19 of 110 building tree 20 of 110 building tree 21 of 110 building tree 22 of 110 building tree 23 of 110 building tree 24 of 110 building tree 25 of 110 building tree 26 of 110 building tree 27 of 110 building tree 28 of 110 building tree 29 of 110 building tree 30 of 110 building tree 31 of 110 building tree 32 of 110 building tree 33 of 110 building tree 34 of 110 building tree 35 of 110 building tree 36 of 110 building tree 37 of 110 . [Parallel(n_jobs=-1)]: Done 33 tasks | elapsed: 4.0min . building tree 38 of 110 building tree 39 of 110 building tree 40 of 110 building tree 41 of 110 building tree 42 of 110 building tree 43 of 110 building tree 44 of 110 building tree 45 of 110 building tree 46 of 110 building tree 47 of 110 building tree 48 of 110 building tree 49 of 110 building tree 50 of 110 building tree 51 of 110 building tree 52 of 110 building tree 53 of 110 building tree 54 of 110 building tree 55 of 110 building tree 56 of 110 building tree 57 of 110 building tree 58 of 110 building tree 59 of 110 building tree 60 of 110 building tree 61 of 110 building tree 62 of 110 building tree 63 of 110 building tree 64 of 110 building tree 65 of 110 building tree 66 of 110 building tree 67 of 110 building tree 68 of 110 building tree 69 of 110 building tree 70 of 110 building tree 71 of 110 building tree 72 of 110 building tree 73 of 110 building tree 74 of 110 building tree 75 of 110 building tree 76 of 110 building tree 77 of 110 building tree 78 of 110 building tree 79 of 110 building tree 80 of 110 building tree 81 of 110 building tree 82 of 110 building tree 83 of 110 building tree 84 of 110 building tree 85 of 110 building tree 86 of 110 building tree 87 of 110 building tree 88 of 110 building tree 89 of 110 building tree 90 of 110 building tree 91 of 110 building tree 92 of 110 building tree 93 of 110 building tree 94 of 110 building tree 95 of 110 building tree 96 of 110 building tree 97 of 110 building tree 98 of 110 building tree 99 of 110 building tree 100 of 110 building tree 101 of 110 building tree 102 of 110 building tree 103 of 110 building tree 104 of 110 building tree 105 of 110 building tree 106 of 110 building tree 107 of 110 building tree 108 of 110 building tree 109 of 110 building tree 110 of 110 . [Parallel(n_jobs=-1)]: Done 110 out of 110 | elapsed: 12.7min finished . RandomForestClassifier(max_depth=40, n_estimators=110, n_jobs=-1, random_state=42, verbose=2) . In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(max_depth=40, n_estimators=110, n_jobs=-1, random_state=42, verbose=2) . y_predictions = model.predict(X_test_scaled) . [Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=4)]: Done 33 tasks | elapsed: 0.9s [Parallel(n_jobs=4)]: Done 110 out of 110 | elapsed: 2.7s finished . from sklearn.metrics import accuracy_score, classification_report print(accuracy_score(y_test, y_predictions)) print(classification_report(y_test, y_predictions)) . 0.9983227252418865 precision recall f1-score support 0.0 1.00 1.00 1.00 215042 1.0 1.00 1.00 1.00 194551 accuracy 1.00 409593 macro avg 1.00 1.00 1.00 409593 weighted avg 1.00 1.00 1.00 409593 . importance_df = pd.DataFrame({ &#39;feature&#39;: X_train.columns, &#39;importance&#39;: model.feature_importances_ }).sort_values(&#39;importance&#39;, ascending=False) importance_df . feature importance . 27 close_diff | 0.536047 | . 33 close_to_shift_ratio | 0.454401 | . 28 close_shift_1_volatility | 0.000878 | . 32 close_momentum_5 | 0.000794 | . 26 ema_crossover | 0.000621 | . 6 number_of_trades | 0.000494 | . 4 volume | 0.000389 | . 5 quote_asset_volume | 0.000388 | . 8 taker_buy_quote_volume | 0.000362 | . 7 taker_buy_base_volume | 0.000345 | . 10 close_shift_1 | 0.000315 | . 3 close | 0.000279 | . 11 close_shift_2 | 0.000250 | . 9 minute | 0.000236 | . 12 close_shift_3 | 0.000233 | . 13 close_shift_4 | 0.000228 | . 31 close_shift_1_lower_bb | 0.000224 | . 1 high | 0.000224 | . 24 ema_10 | 0.000221 | . 22 bb_low | 0.000221 | . 23 ema_5 | 0.000217 | . 2 low | 0.000216 | . 15 sma_5 | 0.000215 | . 21 bb_up | 0.000214 | . 14 close_shift_5 | 0.000214 | . 30 close_shift_1_upper_bb | 0.000207 | . 0 open | 0.000206 | . 16 sma_10 | 0.000201 | . 25 ema_shift_5 | 0.000200 | . 20 sma_20 | 0.000198 | . 17 sma_shift_5 | 0.000196 | . 29 close_shift_1_ma20 | 0.000195 | . 19 sma_shiftmedian_10 | 0.000185 | . 18 sma_shiftmedian_5 | 0.000184 | . tree_depths = [estimator.tree_.max_depth for estimator in model.estimators_] tree_depths . [36, 36, 38, 36, 40, 34, 39, 36, 40, 40, 37, 40, 39, 32, 39, 40, 35, 35, 38, 34, 38, 40, 39, 35, 33, 37, 37, 40, 34, 34, 38, 36, 32, 34, 36, 38, 34, 36, 39, 35, 38, 40, 34, 39, 40, 34, 32, 36, 38, 37, 40, 35, 40, 34, 36, 29, 40, 40, 35, 35, 38, 34, 33, 40, 35, 35, 33, 38, 36, 40, 40, 36, 36, 39, 40, 37, 38, 36, 40, 40, 40, 32, 40, 40, 36, 39, 36, 38, 35, 37, 34, 38, 40, 34, 40, 37, 40, 38, 35, 37, 38, 38, 32, 37, 40, 34, 36, 37, 38, 33] . input_cols=[&#39;open&#39;, &#39;high&#39;, &#39;low&#39;, &#39;close&#39;, &#39;volume&#39;, &#39;quote_asset_volume&#39;, &#39;number_of_trades&#39;, &#39;taker_buy_base_volume&#39;, &#39;taker_buy_quote_volume&#39;, &#39;minute&#39;, &#39;close_shift_1&#39;, &#39;close_shift_2&#39;, &#39;close_shift_3&#39;, &#39;close_shift_4&#39;, &#39;close_shift_5&#39;, &#39;sma_5&#39;, &#39;sma_10&#39;, &#39;sma_shift_5&#39;, &#39;sma_shiftmedian_5&#39;, &#39;sma_shiftmedian_10&#39;, &#39;sma_20&#39;, &#39;bb_up&#39;, &#39;bb_low&#39;, &#39;ema_5&#39;, &#39;ema_10&#39;, &#39;ema_shift_5&#39;, &#39;ema_crossover&#39;, &#39;close_diff&#39;, &#39;close_shift_1_volatility&#39;, &#39;close_shift_1_ma20&#39;, &#39;close_shift_1_upper_bb&#39;, &#39;close_shift_1_lower_bb&#39;, &#39;close_momentum_5&#39;, &#39;close_to_shift_ratio&#39;] . from sklearn.impute import SimpleImputer imputer = SimpleImputer(strategy=&#39;mean&#39;) test_df[input_cols] = imputer.fit_transform(test_df[input_cols]) . test_df[input_cols] = scaler.transform(test_df[input_cols]) . test_df = test_df.drop([&#39;row_id&#39;], axis=1) . test_df.columns . Index([&#39;open&#39;, &#39;high&#39;, &#39;low&#39;, &#39;close&#39;, &#39;volume&#39;, &#39;quote_asset_volume&#39;, &#39;number_of_trades&#39;, &#39;taker_buy_base_volume&#39;, &#39;taker_buy_quote_volume&#39;, &#39;minute&#39;, &#39;close_shift_1&#39;, &#39;close_shift_2&#39;, &#39;close_shift_3&#39;, &#39;close_shift_4&#39;, &#39;close_shift_5&#39;, &#39;sma_5&#39;, &#39;sma_10&#39;, &#39;sma_shift_5&#39;, &#39;sma_shiftmedian_5&#39;, &#39;sma_shiftmedian_10&#39;, &#39;sma_20&#39;, &#39;bb_up&#39;, &#39;bb_low&#39;, &#39;ema_5&#39;, &#39;ema_10&#39;, &#39;ema_shift_5&#39;, &#39;ema_crossover&#39;, &#39;close_diff&#39;, &#39;close_shift_1_volatility&#39;, &#39;close_shift_1_ma20&#39;, &#39;close_shift_1_upper_bb&#39;, &#39;close_shift_1_lower_bb&#39;, &#39;close_momentum_5&#39;, &#39;close_to_shift_ratio&#39;], dtype=&#39;object&#39;) . test_target=model.predict(test_df) . /opt/conda/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names warnings.warn( [Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers. [Parallel(n_jobs=4)]: Done 33 tasks | elapsed: 1.5s [Parallel(n_jobs=4)]: Done 110 out of 110 | elapsed: 5.0s finished . submission = pd.DataFrame({ &#39;row_id&#39;: range(0, 909617), &#39;target&#39;: test_target }) submission.to_csv(&#39;submission.csv&#39;, index=False) .",
            "url": "https://kozodoi.me/2024/02/10/Directional-Forecasting.html",
            "relUrl": "/2024/02/10/Directional-Forecasting.html",
            "date": " • Feb 10, 2024"
        }
        
    
  
    
        ,"post5": {
            "title": "Implementing PCA from Scratch",
            "content": "Last update: 26.03.2023. All opinions are my own. . 1. Overview . This blog post provides a tutorial on implementing the Principal Component Analysis algorithm using Python and NumPy. We will set up a simple class object, implement relevant methods to perform the decomposition, and illustrate how it works on a toy dataset. . Why are we implementing PCA from scratch if the algorithm is already available in scikit-learn? First, coding something from scratch is the best way to understand it. You may know many ML algorithms, but being able to write it down indicates that you have really mastered it. Second, implementing algorithms from scratch is a common task in ML interviews in tech companies, which makes it a useful skill that a job candidate should practice. Last but not least, it&#39;s a fun exercise, right? :) . This post is part of &quot;ML from Scratch&quot; series, where we implement established ML algorithms in Python. Check out other posts to see further implementations. . 2. How PCA works . Before jumping to implementation, let&#39;s quickly refresh our minds. How does PCA work? . PCA is a popular unsupervised algorithm used for dimensionality reduction. In a nutshell, PCA helps you to reduce the number of feature in your dataset by combining the features without loosing too much information. More specifically, PCA finds a linear data transformation that projects the data into a new coordinate system with a fewer dimensions. To capture the most variation in the original data, this projection is done by finding the so-called principal components - eigenvectors of the data&#39;s covariance matrix - and multiplying the actual data matrix with a subset of the components. This procedure is what we are going to implement. . P.S. If you need a more detailed summary of how PCA works, check out this Wiki page. . 3. Implementing PCA . Let&#39;s start the implementation! The only library we need to import is numpy: . import numpy as np . In line with object-oriented programming practices, we will implement PCA as a class with a set of methods. We will need the following five: . __init__(): initialize the class object. | fit(): center the data and identify principal components. | transform(): transform new data into the identified components. | Let&#39;s sketch a class object template. Since we implement functions as class methods, we include self argument for each method: . class PCA: def __init__(self): pass def fit(self): &quot;&quot;&quot; Find principal components &quot;&quot;&quot; pass def transform(self): &quot;&quot;&quot; Transform new data &quot;&quot;&quot; pass . Now let&#39;s go through each method one by one. . The __init__() method is run once when the initialize the PCA class object. . One thing need to do on the initialization step is to store meta-parameters of our algorithm. For PCA, there is only one meta-parameter we will specify: the number of components. We will save it as self.num_components. . Apart from the meta-parameters, we will create three placeholders that we will use to store important class attributes: . self.components: array with the principal component weights | self.mean: mean variable values observed in the training data | self.variance_share: proportion of variance explained by principal components | . def __init__(self, num_components): self.num_components = num_components self.components = None self.mean = None self.variance_share = None . Next, let&#39;s implement the fit() method - the heart of our PCA class. This method will be applied to a provided dataset to identify and memorize principal components. . We will do the following steps: . Center the data by subtracting the mean values for each variable. Normalizing variables is important to make sure that their impact in the data variation is similar and does not depend on the range of that variable. We will also memorize the mean values as self.mean as we will need it later for the data transformation. | Calculate eigenvectors of the covariance matrix. First, we will use np.cov() to get the covariance matrix of the data. Next, we will leverage np.linalg.eig() to do the eigenvalue decomposition and obtain both eigenvalues and eigenvectors. | Sort eigenvalues and eigenvectors in the decreasing order. Since we will use a smaller number of components compared to the number of variables in the original data, we would like to focus on components that reflect more data variation. In our case, eigenvectors that correspond to larger eigenvalues capture more variation. | Store an array with the top num_components components as self.components. | Finally, we will calculate and memorize the data variation explained by the selected components as self.variance_share. This can be computed as a cumulative sum of the corresponding eigenvalues divided by the total sum of eigenvalues. . def fit(self, X): &quot;&quot;&quot; Find principal components &quot;&quot;&quot; # data centering self.mean = np.mean(X, axis = 0) X -= self.mean # calculate eigenvalues &amp; vectors cov_matrix = np.cov(X.T) values, vectors = np.linalg.eig(cov_matrix) # sort eigenvalues &amp; vectors sort_idx = np.argsort(values)[::-1] values = values[sort_idx] vectors = vectors[:, sort_idx] # store principal components &amp; variance self.components = vectors[:self.num_components] self.variance_share = np.sum(values[:self.num_components]) / np.sum(values) . The most difficult part is over! Last but not least, we will implement a method to perform the data transformation. . This will be run after calling the fit() method on the training data, so we only need to implement two steps: . Center the new data using the same mean values that we used on the fitting stage. | Multiply the data matrix with the matrix of the selected components. Note that we will need to transpose the components matrix to ensure the right dimensionality. | def transform(self, X): &quot;&quot;&quot; Transform data &quot;&quot;&quot; # data centering X -= self.mean # decomposition return np.dot(X, self.components.T) . Putting everything together, this is how our implementation looks like: . class PCA: def __init__(self, num_components): self.num_components = num_components self.components = None self.mean = None self.variance_share = None def fit(self, X): &quot;&quot;&quot; Find principal components &quot;&quot;&quot; # data centering self.mean = np.mean(X, axis = 0) X -= self.mean # calculate eigenvalues &amp; vectors cov_matrix = np.cov(X.T) values, vectors = np.linalg.eig(cov_matrix) # sort eigenvalues &amp; vectors sort_idx = np.argsort(values)[::-1] values = values[sort_idx] vectors = vectors[:, sort_idx] # store principal components &amp; variance self.components = vectors[:self.num_components] self.variance_share = np.sum(values[:self.num_components]) / np.sum(values) def transform(self, X): &quot;&quot;&quot; Transform data &quot;&quot;&quot; # data centering X -= self.mean # decomposition return np.dot(X, self.components.T) . 4. Testing the implementation . Now that we have our implementation, let&#39;s check whether it actually works. We will generate two toy data samples with 10 features using the np.random module to draw feature values from a random Normal distribution: . X_old = np.random.normal(loc = 0, scale = 1, size = (1000, 10)) X_new = np.random.normal(loc = 0, scale = 1, size = (500, 10)) print(X_old.shape, X_new.shape) . (1000, 10) (500, 10) . Now, let&#39;s instantiate our PCA class, fit it on the old data and transform both datasets! . To see if the algorithm works properly, we will generate four new examples as X_new, gradually increasing the feature values from 1 to 5. We expect the label predicted by KNN to increase from 0 to 1, since we are getting closer to examples in X1. Let&#39;s check! . # initialize PCA object pca = PCA(num_components = 8) # fit PCA on old data pca.fit(X_old) # check explained variance print(f&quot;Explained variance: {pca.variance_share:.4f}&quot;) . Explained variance: 0.8325 . Eight components explain more than 83% of the data variation. Not bad! Let&#39;s transform the data: . # transform datasets X_old = pca.transform(X_old) X_new = pca.transform(X_new) print(X_old.shape, X_new.shape) . (1000, 8) (500, 8) . Yay! Everything works as expected. The new datasets have eight features instead of the original ten features. . 5. Closing words . This is it! I hope this tutorial helps you to refresh your memory on how PCA works and gives you a good idea on how to implement it yourself. You are now well-equipped to do this exercise on your own! . If you liked this tutorial, feel free to share it on social media and buy me a coffee :) Don&#39;t forget to check out other posts in the &quot;ML from Scratch&quot; series. Happy learning! .",
            "url": "https://kozodoi.me/blog/20230326/pca-from-scratch",
            "relUrl": "/blog/20230326/pca-from-scratch",
            "date": " • Mar 26, 2023"
        }
        
    
  
    
        ,"post6": {
            "title": "Implementing KNN from Scratch",
            "content": "Last update: 26.03.2023. All opinions are my own. . 1. Overview . This blog post provides a tutorial on implementing the K Nearest Neighbors algorithm using Python and NumPy. We will set up a simple class object, implement relevant methods to perform the prediction, and illustrate how it works on a toy dataset. . Why are we implementing KNN from scratch if the algorithm is already available in scikit-learn? First, coding something from scratch is the best way to understand it. You may know many ML algorithms, but being able to write it down indicates that you have really mastered it. Second, implementing algorithms from scratch is a common task in ML interviews in tech companies, which makes it a useful skill that a job candidate should practice. Last but not least, it&#39;s a fun exercise, right? :) . This post is part of &quot;ML from Scratch&quot; series, where we implement established ML algorithms in Python. Check out other posts to see further implementations. . 2. How KNN works . Before we jump to the implementation, let&#39;s quickly refresh our minds. How does KNN work? . KNN is one of the so-called lazy algorithms, which means that there is no actual training step. Instead, KNN memorizes the training data by storing the feature values of training examples. Given a new example to be predicted, KNN calculates distances between the new example and each of the examples in the training set. The prediction returned by the KNN algorithm is simply the average value of the target variable across the K nearest neighbors of the new example. . P.S. If you need a more detailed summary of how KNN works, check out this Wiki page. . 3. Implementing KNN . Let&#39;s start the implementation! The only library we need to import is numpy: . for size in layer_sizes: x = tf.keras.layers.Dense( size, kernel_initializer=&quot;he_uniform&quot;, activation=activation_fn, )(x) if size &lt; layer_sizes - 1: x = tf.keras.layers.BatchNormalization()(x) x = tf.keras.layers.Dropout(dropout_rate)(x) x = tf.keras.layers.Dense( n_outputs, kernel_initializer=&quot;he_uniform&quot;, activation=&quot;sigmoid&quot;, name=&quot;events_predictions&quot; )(x) . import numpy as np . In line with object-oriented programming practices, we will implement KNN as a class with a set of methods. We will need the following five: . __init__(): initialize the class object. | fit(): memorize the training data and store it as a class variable. | predict(): predict label for a new example. | get_distance(): helper function to calculate distance between two examples. | get_neighbors(): helper function to find and rank neighbors by distance. | The last two functions are optional: we can implement the logic inside the predict() method, but it will be easier to split the steps. . Let&#39;s sketch a class object template. Since we implement functions as class methods, we include self argument for each method: . class KNN: def __init__(self): pass def fit(self): &quot;&quot;&quot; Memorize training data &quot;&quot;&quot; pass def predict(self): &quot;&quot;&quot; Predict labels &quot;&quot;&quot; pass def get_distance(self): &quot;&quot;&quot; Calculate distance between two examples &quot;&quot;&quot; pass def get_neighbors(self): &quot;&quot;&quot; Find nearest neighbors &quot;&quot;&quot; pass . Now let&#39;s go through each method one by one. . The __init__() method is run once when the initialize the KNN class object. The only thing we need to do on the initialization step is to store meta-parameters of our algorithm. For KNN, there is only one key meta-parameter we specify: the number of neighbors. We will save it as self.num_neighbors: . def __init__(self, num_neighbors: int = 5): self.num_neighbors = num_neighbors . Next, let&#39;s implement the fit() method. As we mentioned above, on the training stage, KNN needs to memorize the training data. To simplify further calculations, we will provide the input data as two numpy arrays: features saved as self.X and labels saved as self.y: . def fit(self, X: np.array, y: np.array): &quot;&quot;&quot; Memorize training data &quot;&quot;&quot; self.X = X self.y = y . Now, let&#39;s write down a helper function to calculate distance between two examples, which are two numpy arrays with feature values. For simplicity, we will assume that all features are numeric. One of the most commonly used distance metrics is the Euclidean distance, which is calculated as a root of the sum of the squared differences between feature values. If the last sentence sounds complicated, here is how simple it looks in Python: . def get_distance(self, a: np.array, b: np.array): &quot;&quot;&quot; Calculate Euclidean distance between two examples &quot;&quot;&quot; return np.sum((a - b) ** 2) ** 0.5 . Now we are getting to the most difficult part of the KNN implementation! Below, we will write a helper function that finds nearest neighbors for a given example. For that, we will do several steps: . Calculate distance between the provided example and each example in the memorized dataset self.X. | Sort examples in self.X by their distances to the provided example. | Return indices of the nearest neighbors based on the self.num_neighbors meta-parameter. | For step 1, we will leverage the get_distance() function defined above. The trick to implement step 2 is two save a tuple (example ID, distance) when going through the training data. This will allow to sort the examples by distance and return the relevant IDs at the same time: . def get_neighbors(self, example: np.array): &quot;&quot;&quot; Find and rank nearest neighbors of example &quot;&quot;&quot; # placeholder distances = [] # calculate distances as tuples (id, distance) for i in range(len(self.X)): distances.append((i, self.get_distance(self.X[i], example))) # sort by distance distances.sort(key = lambda x: x[1]) # return IDs and distances top neighbors return distances[:self.num_neighbors] . The final step is to do the prediction! For this purpose, we implement the predict() method that expects a new dataset as a numpy array and provides an array with predictions. For each example in the new dataset, the method will go through its nearest neighbors identified using the get_neighbors() helper, and average labels across the neighbors. That&#39;s it! . def predict(self, X: np.array): &quot;&quot;&quot; Predict labels &quot;&quot;&quot; # placeholder predictions = [] # go through examples for idx in range(len(X)): example = X[idx] k_neighbors = self.get_neighbors(example) k_y_values = [self.y[item[0]] for item in k_neighbors] prediction = sum(k_y_values) / self.num_neighbors predictions.append(prediction) # return predictions return np.array(predictions) . Putting everything together, this is how our implementation looks like: . ### END-TO-END KNN CLASS class KNN: def __init__(self, num_neighbors: int = 5): self.num_neighbors = num_neighbors def fit(self, X: np.array, y: np.array): &quot;&quot;&quot; Memorize training data &quot;&quot;&quot; self.X = X self.y = y def get_distance(self, a: np.array, b: np.array): &quot;&quot;&quot; Calculate Euclidean distance between two examples &quot;&quot;&quot; return np.sum((a - b) ** 2) ** 0.5 def get_neighbors(self, example: np.array): &quot;&quot;&quot; Find and rank nearest neighbors of example &quot;&quot;&quot; # placeholder distances = [] # calculate distances as tuples (id, distance) for i in range(len(self.X)): distances.append((i, self.get_distance(self.X[i], example))) # sort by distance distances.sort(key = lambda x: x[1]) # return IDs and distances top neighbors return distances[:self.num_neighbors] def predict(self, X: np.array): &quot;&quot;&quot; Predict labels &quot;&quot;&quot; # placeholder predictions = [] # go through examples for idx in range(len(X)): example = X[idx] k_neighbors = self.get_neighbors(example) k_y_values = [self.y[item[0]] for item in k_neighbors] prediction = sum(k_y_values) / self.num_neighbors predictions.append(prediction) # return predictions return np.array(predictions) . 4. Testing the implementation . Now that we have our implementation, let&#39;s check whether it actually works. We will generate toy data using numpy. The gen_data() function below uses the np.random module to draw feature values from a random Normal distribution and assign a 0/1 label. . ### HELPER FUNCTION def gen_data( mu: float = 0, sigma: float = 1, y: int = 0, size: tuple = (1000, 10), ): &quot;&quot;&quot; Generate random data &quot;&quot;&quot; X = np.random.normal(loc = mu, scale = sigma, size = size) y = np.repeat(y, repeats = size[0]) return X, y . To simulate a simple ML problem, we will generate a dataset consisting of two samples: . 30 examples with mean features value of 1 and a label of 0. | 20 examples with mean features value of 5 and a label of 1. | ### TOY DATA GENERATION X0, y0 = gen_data(mu = 1, sigma = 3, y = 0, size = (30, 10)) X1, y1 = gen_data(mu = 5, sigma = 3, y = 1, size = (20, 10)) X = np.concatenate((X0, X1), axis = 0) y = np.concatenate((y0, y1), axis = 0) . Now, let&#39;s instantiate our KNN class, fit it on the training data and provide predictions for some new examples! . To see if the algorithm works properly, we will generate four new examples as X_new, gradually increasing the feature values from 1 to 5. We expect the label predicted by KNN to increase from 0 to 1, since we are getting closer to examples in X1. Let&#39;s check! . ### PREDICTION # fit KNN clf = KNN(num_neighbors = 5) clf.fit(X, y) # generate new examples X_new = np.stack(( np.repeat(1, 10), np.repeat(2, 10), np.repeat(4, 10), np.repeat(5, 10), )) # predict new examples clf.predict(X_new) . array([0. , 0.2, 0.8, 1. ]) . Yay! Everything works as expected. Our KNN algorithm provides four predictions for the new examples, and the prediction goes up with the increase in feature values. Our job is done! . 5. Closing words . This is it! I hope this tutorial helps you to refresh your memory on how KNN works and gives you a good idea on how to implement it yourself. You are now well-equipped to do this exercise on your own! . If you liked this tutorial, feel free to share it on social media and buy me a coffee :) Don&#39;t forget to check out other posts in the &quot;ML from Scratch&quot; series. Happy learning! .",
            "url": "https://kozodoi.me/blog/20230319/knn-from-scratch",
            "relUrl": "/blog/20230319/knn-from-scratch",
            "date": " • Mar 19, 2023"
        }
        
    
  
    
        ,"post7": {
            "title": "Layer-Wise Learning Rate in PyTorch",
            "content": "Last update: 29.03.2022. All opinions are my own. . 1. Overview . In deep learning tasks, we often use transfer learning to take advantage of the available pre-trained models. Fine-tuning such models is a careful process. On the one hand, we want to adjust the model to the new data set. On the other hand, we also want to retain and leverage as much knowledge learned during pre-training as possible. . Discriminative learning rate is one of the tricks that can help us guide fine-tuning. By using lower learning rates on deeper layers of the network, we make sure we are not tempering too much with the model blocks that have already learned general patterns and concentrate fine-tuning on further layers. . This blog post provides a tutorial on implementing discriminative layer-wise learning rates in PyTorch. We will see how to specify individual learning rates for each of the model parameter blocks and set up the training process. . 2. Implementation . The implementation of layer-wise learning rates is rather straightforward. It consists of three simple steps: . Identifying a list of trainable layers in the neural net. | Setting up a list of model parameter blocks together with the corresponding learning rates. | Supplying the list with this information to the model optimizer. | Let&#39;s go through each of these steps one by one and see how it works! . 2.1. Identifying network layers . The first step in our journey is to instantiate a model and retrieve the list of its layers. This step is essential to figure out how exactly to adjust the learning rate as we go through different parts of the network. . As an example, we will load one of the CNNs from the timm library and print out its parameter groups by iterating through model.named_parameters() and saving their names in a list called layer_names. Note that the framework discussed in this post is model-agnostic. It will work with any architecture, including CNNs, RNNs and transformers. . # instantiate model import timm model = timm.create_model(&#39;resnet18&#39;, num_classes = 2) # save layer names layer_names = [] for idx, (name, param) in enumerate(model.named_parameters()): layer_names.append(name) print(f&#39;{idx}: {name}&#39;) . 0: conv1.weight 1: bn1.weight 2: bn1.bias 3: layer1.0.conv1.weight 4: layer1.0.bn1.weight 5: layer1.0.bn1.bias ... 58: layer4.1.bn2.weight 59: layer4.1.bn2.bias 60: fc.weight 61: fc.bias . As the output suggests, our model has 62 parameter groups. When doing a forward pass, an image is fed to the first convolutional layer named conv1, whose parameters are stored as conv1.weight. Next, the output travels through the batch normalization layer bn1, which has weights and biases stored as bn1.weight and bn1.bias. From that point, the output goes through the network blocks grouped into four big chunks labeled as layer1, ..., layer4. Finally, extracted features are fed into the fully connected part of the network denoted as fc. . In the cell below, we reverse the list of parameter group names to have the deepest layer in the end of the list. This will be useful on the next step. . # reverse layers layer_names.reverse() layer_names[0:5] . [&#39;fc.bias&#39;, &#39;fc.weight&#39;, &#39;layer4.1.bn2.bias&#39;, &#39;layer4.1.bn2.weight&#39;, &#39;layer4.1.conv2.weight&#39;] . 2.2. Specifying learning rates . Knowing the architecture of our network, we can reason about the appropriate learning rates. . There is some flexibility in how to approach this step. The key idea is to gradually reduce the learning rate when going deeper into the network. The first layers should already have a pretty good understanding of general domain-agnostic patterns after pre-training. In a computer vision setting, the first layers may have learned to distinguish simple shapes and edges; in natural language processing, the first layers may be responsible for general word relationships. We don&#39;t want to update parameters on the first layers too much, so it makes sense to reduce the corresponding learning rates. In contrast, we would like to set a higher learning rate for the final layers, especially for the fully-connected classifier part of the network. Those layers usually focus on domain-specific information and need to be trained on new data. . The easiest approach to incorporate this logic is to incrementally reduce the learning rate when going deeper into the network. Let&#39;s simply multiply it by a certain coefficient between 0 and 1 after each parameter group. In our example, this would gives us 62 gradually diminishing learning rate values for 62 model blocks. . Let&#39;s implement it in code! Below, we set up a list of dictionaries called parameters that stores model parameters and learning rates. We will simply go through all parameter blocks and iteratively reduce and assign the appropriate learning rate. In our example, we start with lr = 0.01 and multiply it by 0.9 at each step. Each item in parameters becomes a dictionary with two elements: . params: tensor with the model parameters | lr: corresponding learning rate | . # learning rate lr = 1e-2 lr_mult = 0.9 # placeholder parameters = [] # store params &amp; learning rates for idx, name in enumerate(layer_names): # display info print(f&#39;{idx}: lr = {lr:.6f}, {name}&#39;) # append layer parameters parameters += [{&#39;params&#39;: [p for n, p in model.named_parameters() if n == name and p.requires_grad], &#39;lr&#39;: lr}] # update learning rate lr *= lr_mult . 0: lr = 0.010000, fc.bias 1: lr = 0.009000, fc.weight 2: lr = 0.008100, layer4.1.bn2.bias 3: lr = 0.007290, layer4.1.bn2.weight 4: lr = 0.006561, layer4.1.conv2.weight 5: lr = 0.005905, layer4.1.bn1.bias ... 58: lr = 0.000022, layer1.0.conv1.weight 59: lr = 0.000020, bn1.bias 60: lr = 0.000018, bn1.weight 61: lr = 0.000016, conv1.weight . As you can see, we gradually reduce our learning rate from 0.01 for the bias on the classification layer to 0.00001 on the first convolutional layer. Looks good, right?! . Well, if you look closely, you will notice that we are setting different learning rates for parameter groups from the same layer. For example, having different learning rates for fc.bias and fc.weight does not really make that much sense. To address that, we can increment the learning rate only when going from one group of layers to another. The cell below provides an improved implementation. . #collapse-hide # learning rate lr = 1e-2 lr_mult = 0.9 # placeholder parameters = [] prev_group_name = layer_names[0].split(&#39;.&#39;)[0] # store params &amp; learning rates for idx, name in enumerate(layer_names): # parameter group name cur_group_name = name.split(&#39;.&#39;)[0] # update learning rate if cur_group_name != prev_group_name: lr *= lr_mult prev_group_name = cur_group_name # display info print(f&#39;{idx}: lr = {lr:.6f}, {name}&#39;) # append layer parameters parameters += [{&#39;params&#39;: [p for n, p in model.named_parameters() if n == name and p.requires_grad], &#39;lr&#39;: lr}] . . 0: lr = 0.010000, fc.bias 1: lr = 0.010000, fc.weight 2: lr = 0.009000, layer4.1.bn2.bias 3: lr = 0.009000, layer4.1.bn2.weight 4: lr = 0.009000, layer4.1.conv2.weight 5: lr = 0.009000, layer4.1.bn1.bias ... 58: lr = 0.006561, layer1.0.conv1.weight 59: lr = 0.005905, bn1.bias 60: lr = 0.005905, bn1.weight 61: lr = 0.005314, conv1.weight . This looks more interesting! . Note that we can become very creative in customizing the learning rates and the decay speed. There is no fixed rule that always works well. In my experience, simple linear decay with a multiplier between 0.9 and 1 is a good starting point. Still, the framework provides a lot of space for experimentation, so feel free to test out your ideas and see what works best on your data! . 2.3. Setting up the optimizer . We are almost done. The last and the easiest step is to supply our list of model parameters together with the selected learning rates to the optimizer. In the cell below, we provide parameters to the Adam optimizer, which is one of the most frequently used ones in the field. . Note that we don&#39;t need to supply the learning rate to Adam() as we have already done it in our parameters object. As long as individual learning rates are available, optimizer will prioritize them over the single learning rate supplied to the Adam() call. . # set up optimizer import torch.optim as optim optimizer = optim.Adam(parameters) . This is it! Now we can proceed to training our model as usual. When calling optimizer.step() inside the training loop, the optimizer will update model parameters by subtracting the gradient multiplied by the corresponding group-wise learning rates. This implies that there is no need to adjust the training loop, which usually looks something like this: . #collapse-hide # loop through batches for (inputs, labels) in data_loader: # extract inputs and labels inputs = inputs.to(device) labels = labels.to(device) # passes and weights update with torch.set_grad_enabled(True): # forward pass preds = model(inputs) loss = criterion(preds, labels) # backward pass loss.backward() # weights update optimizer.step() optimizer.zero_grad() . . 3. Closing words . In this post, we went through the steps of implementing a layer-wise discriminative learning rate in PyTorch. I hope this brief tutorial will help you set up your transfer learning pipeline and squeeze out the maximum of your pre-trained model. If you are interested, check out my other blog posts on tips on deep learning and PyTorch. Happy learning! .",
            "url": "https://kozodoi.me/blog/20220329/discriminative-lr",
            "relUrl": "/blog/20220329/discriminative-lr",
            "date": " • Mar 29, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Test-Time Augmentation for Tabular Data",
            "content": "Last update: 08.09.2021. All opinions are my own. . 1. Overview . Test time augmentation (TTA) is a popular technique in computer vision. TTA aims at boosting the model accuracy by using data augmentation on the inference stage. The idea behind TTA is simple: for each test image, we create multiple versions that are a little different from the original (e.g., cropped or flipped). Next, we predict labels for the test images and created copies and average model predictions over multiple versions of each image. This usually helps to improve the accuracy irrespective of the underlying model. . In many business settings, data comes in a tabular format. Can we use TTA with tabular data to enhance the accuracy of ML models in a way similar to computer vision models? How to define suitable transformations of test cases that do not affect the label? This blog post explores the opportunities for using TTA in tabular data environments. We will implement TTA for scikit-learn classifiers and test its performance on multiple credit scoring data sets. The preliminary results indicate that TTA might be a tiny bit helpful in some settings. . Note: the results presented in this blog post are currently being extended within a scope of a working paper. The post will be updated once the paper is available on ArXiV. . 2. Adapting TTA to tabular data . TTA has been originally developed for deep learning applications in computer vision. In contrast to image data, tabular data poses a more challenging environment for using TTA. We will discuss two main challenges that we need to solve to apply TTA to structured data: . how to define transformations? | how to treat categorical features? | . 2.1. How to define transformations? . When working with image data, light transformations such as rotation, brightness adjustment, saturation and many others modify the underlying pixel values but do not affect the ground truth. That is, a rotated cat is still a cat. We can easily verify this by visually checking the transformed images and limiting the magnitude of transformations to make sure the cat is still recognizable. . . This is different for tabular data, where the underlying features represent different characteristics of the observed subjects. Let&#39;s consider a credit scoring example. In finance, banks use ML models to support loan allocation decisions. Consider a binary classification problem, where we predict whether the applicant will pay back the loan. The underlying features may describe the applicant&#39;s attributes (age, gender), loan parameters (amount, duration), macroeconomic indicators (inflation, growth). How to do transformations on these features? While there is no such thing as rotating a loan applicant (at least not within the scope of machine learning), we could do a somewhat similar exercise: create copies of each loan applicant and slightly modify feature values for each copy. A good starting point would be to add some random noise to each of the features. . This procedure raises a question: how can we be sure that transformations do not alter the label? Would increasing the applicant&#39;s age by 10 years affect her repayment ability? Arguably, yes. What about increasing the age by 1 year? Or 1 day? These are challenging questions that we can not answer without more information. This implies that the magnitude of the added noise has to be carefully tuned. We need to take into account the variance of each specific feature as well as the overall data set variability. Adding too little noise will create synthetic cases that are too similar to the original applications, which is not very useful. On the other hand, adding too much noise risks changing the label of the corresponding application, which would harm the model accuracy. The trade-off between these two extremes is what can potentially bring us closer to discovering an accuracy boost. . 2.2. How to treat categorical features? . It is rather straightforward to add noise to continuous features such as age or income. However, tabular data frequently contains special gifts: categorical features. From gender to zip code, these features present another challenge for the application of TTA. Adding noise to the zip code appears non-trivial and requires some further thinking. Ignoring categorical features and only altering the continuous ones sounds like an easy solution, but this might not work well on data sets that contain a lot of information in the form of categorical data. . In this blog post, we will try a rather naive approach to deal with categorical features. Every categorical feature can be encoded as a set of dummy variables. Next, considering each dummy feature separately, we can occasionally flip the value, switching the person&#39;s gender, country of origin or education level with one click. This would introduce some variance in the categorical features and provide TTA with more diverse synthetic applications. This approach is imperfect and can be improved on, but we have to start somewhere, right? . Now that we have some ideas about how TTA should work and what are the main challenges, let&#39;s actually try to implement it! . 3. Implementing TTA . This section implements a helper function predict_proba_with_tta() to extend the standard predict_proba() method in scikit-learn such that predictions take advantage of the TTA procedure. We focus on a binary classification task, but one could easily extend this framework to regression tasks as well. . The function predict_proba_with_tta() requires specifying the underlying scikit-learn model and the test set with observations to be predicted. The function operates in four simple steps: . Creating num_tta copies of the test set. | Implementing random transformations of the synthetic copies. | Predicting labels for the real and synthetic observations. | Aggregating the predictions. | Considering the challenges discussed in the previous section, we implement the following transformations for the continuous features: . compute STD of each continuous feature denoted as std | generate a random vector n using the standard normal distribution | add alpha * n * std to each feature , where alpha is a meta-parameter. | . And for the categorical features: . convert categorical features into a set of dummies | flip each dummy variable with a probability beta, where beta is a meta-parameter. | . By varying alpha and beta, we control the transformation magnitude, adjusting the noise scale in the synthetic copies. Higher values imply stronger transformations. The suitable values can be identified through some meta-parameter tuning. . #collapse-show def predict_proba_with_tta(data, model, dummies = None, num_tta = 4, alpha = 0.01, beta = 0.01, seed = 0): &#39;&#39;&#39; Predicts class probabilities using TTA. Arguments: - data (numpy array): data set with the feature values - model (sklearn model): machine learning model - dummies (list): list of column names of dummy features - num_tta (integer): number of test-time augmentations - alpha (float): noise parameter for continuous features - beta (float): noise parameter for dummy features - seed (integer): random seed Returns: - array of predicted probabilities &#39;&#39;&#39; # set random seed np.random.seed(seed = seed) # original prediction preds = model.predict_proba(data) / (num_tta + 1) # select numeric features num_vars = [var for var in data.columns if data[var].dtype != &#39;object&#39;] # find dummies if dummies != None: num_vars = list(set(num_vars) - set(dummies)) # synthetic predictions for i in range(num_tta): # copy data data_new = data.copy() # introduce noise to numeric vars for var in num_vars: data_new[var] = data_new[var] + alpha * np.random.normal(0, 1, size = len(data_new)) * data_new[var].std() # introduce noise to dummies if dummies != None: for var in dummies: probs = np.random.binomial(1, (1 - beta), size = len(data_new)) data_new.loc[probs == 0, var] = 1 - data_new.loc[probs == 0, var] # predict probs preds_new = model.predict_proba(data_new) preds += preds_new / (num_tta + 1) # return probs return preds . . 4. Empirical benchmark . Let&#39;s test our TTA function! This section performs empirical experiment on multiple data sets to check whether TTA can improve the model performance. First, we import relevant modules and load the list of prepared data sets. All data sets come from a credit scoring environment, which represents a binary classification setup. Some of the data sets are publically available, whereas the others are subject to NDA. The public data sets include australian), german), pakdd, gmsc, homecredit and lendingclub. The sample sizes and the number of features vary greatly across the datasets. This allows us to test the TTA framework in different conditions. . #collapse-hide import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from sklearn.ensemble import RandomForestClassifier from sklearn.model_selection import StratifiedKFold from sklearn.metrics import roc_auc_score import os import time . . #collapse-show datasets = os.listdir(&#39;../data&#39;) datasets . . [&#39;thomas.csv&#39;, &#39;german.csv&#39;, &#39;hmeq.csv&#39;, &#39;bene2.csv&#39;, &#39;lendingclub.csv&#39;, &#39;bene1.csv&#39;, &#39;cashbus.csv&#39;, &#39;uk.csv&#39;, &#39;australian.csv&#39;, &#39;pakdd.csv&#39;, &#39;gmsc.csv&#39;, &#39;paipaidai.csv&#39;] . Apart from the data sets, TTA needs an underlying ML model. In our experiment, on each data set, we will use a Random Forest classifier with 500 trees, which is a good trade-off between good performance and computational resources. We will not go deep into tuning the classifier and keep the parameters fixed for all data sets. We will then use stratified 5-fold cross-validation to train and test models with and without TTA. . #collapse-show # classifier clf = RandomForestClassifier(n_estimators = 500, random_state = 1, n_jobs = 4) # settings folds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 23) . . The cell below implements the following experiment: . We loop through the datasets and perform cross-validation, training Random Forest on each fold combination. | Next, we predict labels of the validation cases and calculate the AUC of the model predictions. This is our benchmark. | We predict labels of the validation cases with the same model but now implement TTA to adjust the predictions. | By comparing the average AUC difference before and after TTA, we can judge whether TTA actually helps to boost the predictive performance. | #collapse-show # placeholders auc_change = [] # timer start = time.time() # modeling loop for data in datasets: ##### DATA PREPARATION # import data X = pd.read_csv(&#39;../data/&#39; + data) # convert target to integer X.loc[X.BAD == &#39;BAD&#39;, &#39;BAD&#39;] = 1 X.loc[X.BAD == &#39;GOOD&#39;, &#39;BAD&#39;] = 0 # extract X and y y = X[&#39;BAD&#39;] del X[&#39;BAD&#39;] # create dummies X = pd.get_dummies(X, prefix_sep = &#39;_dummy_&#39;) # data information print(&#39;-&#39;) print(&#39;Dataset:&#39;, data, X.shape) print(&#39;-&#39;) ##### CROSS-VALIDATION # create objects oof_preds_raw = np.zeros((len(X), y.nunique())) oof_preds_tta = np.zeros((len(X), y.nunique())) # modeling loop for fold_, (trn_, val_) in enumerate(folds.split(y, y)): # data partitioning trn_x, trn_y = X.iloc[trn_], y.iloc[trn_] val_x, val_y = X.iloc[val_], y.iloc[val_] # train the model clf.fit(trn_x, trn_y) # identify dummies dummies = list(X.filter(like = &#39;_dummy_&#39;).columns) # predictions oof_preds_raw[val_, :] = clf.predict_proba(val_x) oof_preds_tta[val_, :] = predict_proba_with_tta(data = val_x, model = clf, dummies = dummies, num_tta = 5, alpha = np.sqrt(len(trn_x)) / 3000, beta = np.sqrt(len(trn_x)) / 30000, seed = 1) # print performance print(&#39;- AUC before TTA = %.6f &#39; % roc_auc_score(y, oof_preds_raw[:,1])) print(&#39;- AUC with TTA = %.6f &#39; % roc_auc_score(y, oof_preds_tta[:,1])) print(&#39;-&#39;) print(&#39;&#39;) # save the AUC delta delta = roc_auc_score(y, oof_preds_tta[:,1]) - roc_auc_score(y, oof_preds_raw[:,1]) auc_change.append(delta) # display results print(&#39;-&#39;) print(&#39;Finished in %.1f minutes&#39; % ((time.time() - start) / 60)) print(&#39;-&#39;) print(&#39;TTA improves AUC in %.0f/%.0f cases&#39; % (np.sum(np.array(auc_change) &gt; 0), len(datasets))) print(&#39;Mean AUC change = %.6f&#39; % np.mean(auc_change)) print(&#39;-&#39;) . . - Dataset: thomas.csv (1225, 28) - - AUC before TTA = 0.612322 - AUC with TTA = 0.613617 - - Dataset: german.csv (1000, 61) - - AUC before TTA = 0.796233 - AUC with TTA = 0.796300 - - Dataset: hmeq.csv (5960, 20) - - AUC before TTA = 0.975995 - AUC with TTA = 0.976805 - - Dataset: bene2.csv (7190, 28) - - AUC before TTA = 0.801193 - AUC with TTA = 0.799387 - - Dataset: lendingclub.csv (43344, 114) - - AUC before TTA = 0.625029 - AUC with TTA = 0.628207 - - Dataset: bene1.csv (3123, 84) - - AUC before TTA = 0.788607 - AUC with TTA = 0.789447 - - Dataset: cashbus.csv (15000, 642) - - AUC before TTA = 0.629648 - AUC with TTA = 0.624874 - - Dataset: uk.csv (30000, 51) - - AUC before TTA = 0.712042 - AUC with TTA = 0.723359 - - Dataset: australian.csv (690, 42) - - AUC before TTA = 0.931787 - AUC with TTA = 0.931958 - - Dataset: pakdd.csv (50000, 373) - - AUC before TTA = 0.620081 - AUC with TTA = 0.623080 - - Dataset: gmsc.csv (150000, 68) - - AUC before TTA = 0.846187 - AUC with TTA = 0.855176 - - Dataset: paipaidai.csv (60000, 1934) - - AUC before TTA = 0.716398 - AUC with TTA = 0.721679 - - Finished in 206.1 minutes - TTA improves AUC in 10/12 cases Mean AUC change = 0.002364 - . Looks like TTA is working! Overall, TTA improves the AUC in 10 out of 12 data sets. The observed performance gains are rather small: on average, TTA improves AUC by 0.00236. The results are visualized in the barplot below: . #collapse-hide objects = list(range(len(datasets))) y_pos = np.arange(len(objects)) perf = np.sort(auc_change2) plt.figure(figsize = (6, 8)) plt.barh(y_pos, perf, align = &#39;center&#39;, color = &#39;blue&#39;, alpha = 0.66) plt.ylabel(&#39;Dataset&#39;) plt.yticks(y_pos, objects) plt.xlabel(&#39;AUC Gain&#39;) plt.title(&#39;&#39;) ax.plot([0, 0], [1, 12], &#39;k--&#39;) plt.tight_layout() . . . We should bear in mind that performance gains, although appearing rather small, come almost &quot;for free&quot;. We don&#39;t need to train a new model and only require a relatively small amount of extra resources to create synthetic copies of the loan applications. Sounds good! . It is possible that further fine-tuning of the TTA meta-parameters can uncover larger performance gains. Furthermore, a considerable variance of the average gains from TTA across the data sets indicates that TTA can be more helpful in specific settings. The important factors influencing the TTA performance may relate to both the data and the classifier used to produce predictions. More research is needed to identify and analyze such factors. . 5. Closing words . The purpose of this tutorial was to explore TTA applications for tabular data. We have discussed the corresponding challenges, developed a TTA wrapper function for scikit-learn and demonstrated that it could indeed be helpful on multiple credit scoring data sets. I hope you found this post interesting. . The project described in this blog post is a work in progress. I will update the post once the working paper on the usage of TTA for tabular data is available. Stay tuned and happy learning! .",
            "url": "https://kozodoi.me/blog/20210908/tta-tabular",
            "relUrl": "/blog/20210908/tta-tabular",
            "date": " • Sep 8, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Extracting Intermediate Layer Outputs in PyTorch",
            "content": "Last update: 23.10.2021. All opinions are my own. . 1. Overview . In deep learning tasks, we usually work with predictions outputted by the final layer of a neural network. In some cases, we might also be interested in the outputs of intermediate layers. Whether we want to extract data embeddings or inspect what is learned by earlier layers, it may not be straightforward how to extract the intermediate features from the network. . This blog post provides a quick tutorial on the extraction of intermediate activations from any layer of a deep learning model in PyTorch using the forward hook functionality. The important advantage of this method is its simplicity and ability to extract features without having to run the inference twice, only requiring a single forward pass through the model to save multiple outputs. . 2. Why do we need intermediate features? . Extracting intermediate activations (also called features) can be useful in many applications. In computer vision problems, outputs of intermediate CNN layers are frequently used to visualize the learning process and illustrate visual features distinguished by the model on different layers. Another popular use case is extracting intermediate outputs to create image or text embeddings, which can be used to detect duplicate items, included as input features in a classical ML model, visualize data clusters and much more. When working with Encoder-Decoder architectures, outputs of intermediate layers can also be used to compress the data into a smaller-sized vector containing the data represenatation. There are many further use cases in which intermediate activations can be useful. So, let&#39;s discuss how to get them! . 3. How to extract activations? . To extract activations from intermediate layers, we will need to register a so-called forward hook for the layers of interest in our neural network and perform inference to store the relevant outputs. . For the purpose of this tutorial, I will use image data from a Cassava Leaf Disease Classification Kaggle competition. In the next few cells, we will import relevant libraries and set up a Dataloader object. Feel free to skip them if you are familiar with standard PyTorch data loading practices and go directly to the feature extraction part. . Preparations . # collapse-hide ##### PACKAGES import numpy as np import pandas as pd import torch import torch.nn as nn from torch.utils.data import Dataset, DataLoader !pip install timm import timm import albumentations as A from albumentations.pytorch import ToTensorV2 import cv2 import os device = torch.device(&quot;cuda&quot;) . . # collapse-hide ##### DATASET class ImageData(Dataset): # init def __init__(self, data, directory, transform): self.data = data self.directory = directory self.transform = transform # length def __len__(self): return len(self.data) # get item def __getitem__(self, idx): # import image = cv2.imread( os.path.join(self.directory, self.data.iloc[idx][&quot;image_id&quot;]) ) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # augmentations image = self.transform(image=image)[&quot;image&quot;] return image . . We will use a standrd PyTorch dataloader to load the data in batches of 32 images. . # collapse-show ##### DATA LOADER # import data df = pd.read_csv(&quot;../input/cassava-leaf-disease-classification/train.csv&quot;) display(df.head()) # augmentations transforms = A.Compose([A.Resize(height=128, width=128), A.Normalize(), ToTensorV2()]) # dataset data_set = ImageData( data=df, directory=&quot;../input/cassava-leaf-disease-classification/train_images/&quot;, transform=transforms, ) # dataloader data_loader = DataLoader(data_set, batch_size=32, shuffle=False, num_workers=2) . . image_id label . 0 1000015157.jpg | 0 | . 1 1000201771.jpg | 3 | . 2 100042118.jpg | 1 | . 3 1000723321.jpg | 1 | . 4 1000812911.jpg | 3 | . Model . To extract anything from a neural net, we first need to set up this net, right? In the cell below, we define a simple resnet18 model with a two-node output layer. We use timm library to instantiate the model, but feature extraction will also work with any neural network written in PyTorch. . We also print out the architecture of our network. As you can see, there are many intermediate layers through which our image travels during a forward pass before turning into a two-number output. We should note the names of the layers because we will need to provide them to a feature extraction function. . ##### DEFINE MODEL model = timm.create_model(model_name=&quot;resnet18&quot;, pretrained=True) model.fc = nn.Linear(512, 2) model.to(device) . ResNet( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act1): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act1): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act2): ReLU(inplace=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act1): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act2): ReLU(inplace=True) ) ) ... (layer4): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act1): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act2): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act1): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (act2): ReLU(inplace=True) ) ) (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True) (fc): Linear(in_features=512, out_features=2, bias=True) ) . Feature extraction . The implementation of feature extraction requires two simple steps: . Registering a forward hook on a certain layer of the network. | Performing standard inference to extract features of that layer. | First, we need to define a helper function that will introduce a so-called hook. A hook is simply a command that is executed when a forward or backward call to a certain layer is performed. If you want to know more about hooks, you can check out this link. . In out setup, we are interested in a forward hook that simply copies the layer outputs, sends them to CPU and saves them to a dictionary object we call features. . The hook is defined in a cell below. The name argument in get_features() specifies the dictionary key under which we will store our intermediate activations. . ##### HELPER FUNCTION FOR FEATURE EXTRACTION def get_features(name): def hook(model, input, output): features[name] = output.detach() return hook . After the helper function is defined, we can register a hook using .register_forward_hook() method. The hook can be applied to any layer of the neural network. . Since we work with a CNN, extracting features from the last convolutional layer might be useful to get image embeddings. Therefore, we are registering a hook for the outputs of the (global_pool). To extract features from an earlier layer, we could also access them with, e.g., model.layer1[1].act2 and save it under a different name in the features dictionary. With this method, we can actually register multiple hooks (one for every layer of interest), but we will only keep one for the purpose of this example. . ##### REGISTER HOOK model.global_pool.register_forward_hook(get_features(&quot;feats&quot;)) . &lt;torch.utils.hooks.RemovableHandle at 0x7f2540254290&gt; . Now we are ready to extract features! The nice thing about hooks is that we can now perform inference as we usually would and get multiple outputs at the same time: . outputs of the final layer | outputs of every layer with a registered hook | . The feature extraction happens automatically during the forward pass whenever we run model(inputs). To store intermediate features and concatenate them over batches, we just need to include the following in our inference loop: . Create placeholder list FEATS = []. This list will store intermediate outputs from all batches. | Create placeholder dict features = {}. We will use this dictionary for storing intermediate outputs from each batch. | Iteratively extract batch features to features, send them to CPU and append to the list FEATS. | ##### FEATURE EXTRACTION LOOP # placeholders PREDS = [] FEATS = [] # placeholder for batch features features = {} # loop through batches for idx, inputs in enumerate(data_loader): # move to device inputs = inputs.to(device) # forward pass [with feature extraction] preds = model(inputs) # add feats and preds to lists PREDS.append(preds.detach().cpu().numpy()) FEATS.append(features[&quot;feats&quot;].cpu().numpy()) # early stop if idx == 9: break . This is it! Looking at the shapes of resulting arrays, you can see that the code worked well: we extracted both final layer outputs as PREDS and intermediate activations as FEATS. We can now save these features and work with them further. . ##### INSPECT FEATURES PREDS = np.concatenate(PREDS) FEATS = np.concatenate(FEATS) print(&quot;- preds shape:&quot;, PREDS.shape) print(&quot;- feats shape:&quot;, FEATS.shape) . - preds shape: (320, 2) - feats shape: (320, 512) . 4. Closing words . The purpose of this tutorial was to learn you how to extract intermediate outputs from the most interesting layers of your neural networks. With hooks, you can do all feature extraction in a single inference run and avoid complex modifications of your model. I hope you found this post helpful. . If you are interested, check out my other blog posts to see more tips on deep learning and PyTorch. Happy learning! .",
            "url": "https://kozodoi.me/blog/20210527/extracting-features",
            "relUrl": "/blog/20210527/extracting-features",
            "date": " • May 27, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Tracking ML Experiments with Neptune.ai",
            "content": "This post is also published on the Neptune.ai blog. All opinions are my own.* . 1. Introduction . Many ML projects, including Kaggle competitions, have a similar workflow. You start with a simple pipeline with a benchmark model. Next, you begin incorporating improvements: adding features, augmenting the data, tuning the model... On each iteration, you evaluate your solution and keep changes that improve the target metric. . . The figure illustrates the iterative improvement process in ML projects. . This workflow involves running a lot of experiments. As time goes by, it becomes difficult to keep track of the progress and positive changes. Instead of working on new ideas, you spend time thinking: . “have I already tried this thing?”, | “what was that hyperparameter value that worked so well last week?” | . You end up running the same stuff multiple times. If you are not tracking your experiments yet, I highly recommend you to start! In my previous Kaggle projects, I used to rely on spreadsheets for tracking. It worked very well in the beginning, but soon I realized that setting up and managing spreadsheets with experiment meta-data requires loads of additional work. I got tired of manually filling in model parameters and performance values after each experiment and really wanted to switch to an automated solution. . This is when I discovered Neptune.ai. This tool allowed me to save a lot of time and focus on modeling decisions, which helped me to earn three medals in Kaggle competitions. . In this post, I will share my story of switching from spreadsheets to Neptune for experiment tracking. I will describe a few disadvantages of spreadsheets, explain how Neptune helps to address them, and give a couple of tips on using Neptune for Kaggle. . 2. What is wrong with spreadsheets for experiment tracking? . Spreadsheets are great for many purposes. To track experiments, you can simply set up a spreadsheet with different columns containing the relevant parameters and performance of your pipeline. It is also easy to share this spreadsheet with teammates. . Sounds great, right? . Unfortunately, there are a few problems with this. . . The figure illustrates ML experiment tracking with spreadsheets. . Manual work . After doing it for a while, you will notice that maintaining a spreadsheet starts eating too much time. You need to manually fill in a row with meta-data for each new experiment and add a column for each new parameter. This will get out of control once your pipeline becomes more sophisticated. . It is also very easy to make a typo, which can lead to bad decisions. . When working on one deep learning competition, I incorrectly entered a learning rate in one of my experiments. Looking at the spreadsheet, I concluded that a high learning rate decreases the accuracy and went on working on other things. It was only a few days later when I realized that there was a typo and poor performance actually comes from a low learning rate. This cost me two days of work invested in the wrong direction based on a false conclusion. . No live tracking . With spreadsheets, you need to wait until an experiment is completed in order to record the performance. . Apart from being frustrated to do it manually every time, this also does not allow you to compare intermediate results across the experiments, which is helpful to see if a new run looks promising. . Of course, you can log in model performance after every epoch, but doing it manually for each experiment requires even more time and effort. I never had enough diligence to do it regularly and ended up spending some computing resources not optimally. . Attachment limitations . Another issue with spreadsheets is that they only support textual meta-data that can be entered in a cell. . What if you want to attach other meta-data like: . model weights, | source code, | plots with model predictions, | input data version? | . You need to manually store this stuff in your project folders outside of the spreadsheet. . In practice, it gets complicated to organize and sync experiment outputs between local machines, Google Colab, Kaggle Notebooks, and other environments your teammates might use. Having such meta-data attached to a tracking spreadsheet seems useful, but it is very difficult to do it. . 3. Switching from spreadsheets to Neptune . A few months ago, our team was working on a Cassava Leaf Disease competition and used Google spreadsheets for experiment tracking. One month into the challenge, our spreadsheet was already cluttered: . Some runs were missing performance because one of us forgot to log it in and did not have the results anymore. | PDFs with loss curves were scattered over Google Drive and Kaggle Notebooks. | Some parameters might have been entered incorrectly, but it was too time-consuming to restore and double-check older script versions. | . It was difficult to make good data-driven decisions based on our spreadsheet. . Even though there were only four weeks left, we decided to switch to Neptune. I was surprised to see how little effort it actually took us to set it up. In brief, there are three main steps: . sign up for a Neptune account and create a project, | install the neptune package in your environment, | include several lines in the pipeline to enable logging of relevant meta-data. | . You can read more about the exact steps to start using Neptune here. Of course, going through the documentation and getting familiar with the platform may take you a few hours. But remember that this is only a one-time investment. After learning the tool once, I was able to automate much of the tracking and rely on Neptune in the next Kaggle competitions with very little extra effort . 4. What is good about Neptune? . The figure illustrates ML experiment tracking with Neptune. . Less manual work . One of the key advantages of Neptune over spreadsheets is that it saves you a lot of manual work. With Neptune, you use the API within the pipeline to automatically upload and store meta-data while the code is running. . import neptune.new as neptune run = neptune.init(project = &#39;#&#39;, api_token = &#39;#&#39;) # your credentials # Track relevant parameters config = { &#39;batch_size&#39;: 64, &#39;learning_rate&#39;: 0.001, &#39;optimizer&#39;: &#39;Adam&#39; } run[&#39;parameters&#39;] = config # Track the training process by logging your training metrics for epoch in range(100): run[&#39;train/accuracy&#39;].log(epoch * 0.6) # Log the final results run[&#39;f1_score&#39;] = 0.66 . You don’t have to manually put it in the results table, and you also save yourself from making a typo. Since the meta-data is sent to Neptune directly from the code, you will get all numbers right no matter how many digits they have. . It may sound like a small thing, but the time saved from logging in each experiment accumulates very quickly and leads to tangible gains by the end of the project. This gives you an opportunity to not think too much about the actual tracking process and better focus on the modeling decisions. In a way, this is like hiring an assistant to take care of some boring (but very useful) logging tasks so that you can focus more on the creative work. . Live tracking . What I like a lot about Neptune is that it allows you to do live tracking. If you work with models like neural networks or gradient boosting that require a lot of iterations before convergence, you know it is quite useful to look at the loss dynamics early to detect issues and compare models. . Tracking intermediate results in a spreadsheet is too frustrating. Neptune API can log in performance after every epoch or even every batch so that you can start comparing the learning curves while your experiment is still running. . . This proves to be very helpful. As you might expect, many ML experiments have negative results (sorry, but this great idea you were working on for a few days actually decreases the accuracy). . This is completely fine because this is how ML works. . What is not fine is that you may need to wait a long time until getting that negative signal from your pipeline. Using Neptune dashboard to compare the intermediate plots with the first few performance values may be enough to realize that you need to stop the experiment and change something. . Attaching outputs . Another advantage of Neptune is the ability to attach pretty much anything to every experiment run. This really helps to keep important outputs such as model weights and predictions in one place and easily access them from your experiments table. . This is particularly helpful if you and your colleagues work in different environments and have to manually upload the outputs to sync the files. . I also like the ability to attach the source code to each run to make sure you have the notebook version that produced the corresponding result. This can be very useful in case you want to revert some changes that did not improve the performance and would like to go back to the previous best version. . 4. Tips to improve Kaggle performance with Neptune . When working on Kaggle competitions, there are a few tips I can give you to further improve your tracking experience. . Using Neptune in Kaggle Notebooks or Google Colab . First, Neptune is very helpful for working in Kaggle Notebooks or Google Colab that have session time limits when using GPU/TPU. I can not count how many times I lost all experiment outputs due to a notebook crash when training was taking just a few minutes more than the allowed 9-hour limit! . To avoid that, I would highly recommend setting up Neptune such that model weights and loss metrics are stored after each epoch. That way, you will always have a checkpoint uploaded to Neptune servers to resume your training even if your Kaggle notebook times out. You will also have an opportunity to compare your intermediate results before the session crash with other experiments to judge their potential. . Updating runs with the Kaggle leaderboard score . Second, an important metric to track in Kaggle projects is the leaderboard score. With Neptune, you can track your cross-validation score automatically but getting the leaderboard score inside the code is not possible since it requires you to submit predictions via the Kaggle website. . The most convenient way to add the leaderboard score of your experiment to the Neptune tracking table is to use the &quot;resume run&quot; functionality. It allows you to update any finished experiment with a new metric with a couple of lines of code. This feature is also helpful to resume tracking crashed sessions, which we discussed in the previous paragraph. . import neptune.new as neptune run = neptune.init(project = &#39;Your-Kaggle-Project&#39;, run = &#39;SUN-123&#39;) # Add a new metric run[“LB_score”] = 0.5 # Download snapshot of model weights model = run[&#39;train/model_weights&#39;].download() # Continue working . Downloading experiment meta-data . Finally, I know that many Kagglers like to perform complex analyses of their submissions, like estimating the correlation between CV and LB scores or plotting the best score dynamics with respect to time. . While it is not yet feasible to do such things on the website, Neptune allows you to download meta-data from all experiments directly into your notebook using a single API call. It makes it easy to take a deeper dive into the results or export the meta-data table and share it externally with people who use a different tracking tool or don’t rely on any experiment tracking. . import neptune.new as neptune my_project = neptune.get_project(&#39;Your-Workspace/Your-Kaggle-Project&#39;) # Get dashboard with runs contributed by &#39;sophia&#39; sophia_df = my_project.fetch_runs_table(owner = &#39;sophia&#39;).to_pandas() sophia_df.head() . 5. Final thoughts . In this post, I shared my story of switching from spreadsheets to Neptune for tracking ML experiments and emphasized some advantages of Neptune. I would like to stress once again that investing time in infrastructure tools - be it experiment tracking, code versioning, or anything else - is always a good decision and will likely pay off with the increased productivity. Tracking experiment meta-data with spreadsheets is much better than not doing any tracking. It will help you to better see your progress, understand what modifications improve your solution, and help make modeling decisions. Doing it with spreadsheets will also cost you some additional time and effort. Tools like Neptune take the experiment tracking to a next level, allowing you to automate the meta-data logging and focus on the modeling decisions. . I hope you find my story useful. Good luck with your future ML projects! .",
            "url": "https://kozodoi.me/blog/20210430/tracking-experiments",
            "relUrl": "/blog/20210430/tracking-experiments",
            "date": " • Apr 30, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Computing Mean & STD in Image Dataset",
            "content": "Last update: 16.10.2021. All opinions are my own. . 1. Overview . In computer vision, it is recommended to normalize image pixel values relative to the dataset mean and standard deviation. This helps to get consistent results when applying a model to new images and can also be useful for transfer learning. In practice, computing these statistics can be a little non-trivial since we usually can&#39;t load the whole dataset in memory and have to loop through it in batches. . This blog post provides a quick tutorial on computing dataset mean and std within RGB channels using a regular PyTorch dataloader. While computing mean is easy (we can simply average means over batches), standard deviation is a bit more tricky: averaging STDs across batches is not the same as the overall STD. Let&#39;s see how to do it properly! . 2. Preparations . To demonstrate how to compute image stats, we will use data from Cassava Leaf Disease Classification Kaggle competition with about 21,000 plant images. Feel free to scroll down to Section 3 to jump directly to calculations. . First, we will import the usual libraries and specify relevant parameters. No need to use GPU because there is no modeling involved. . # collapse-hide ####### PACKAGES import numpy as np import pandas as pd import torch import torchvision from torch.utils.data import Dataset, DataLoader import albumentations as A from albumentations.pytorch import ToTensorV2 import cv2 from tqdm import tqdm import matplotlib.pyplot as plt %matplotlib inline ####### PARAMS device = torch.device(&quot;cpu&quot;) num_workers = 4 image_size = 512 batch_size = 8 data_path = &quot;/kaggle/input/cassava-leaf-disease-classification/&quot; . . Now, let&#39;s import a dataframe with image paths and create a Dataset class that will read images and supply them to the dataloader. . # collapse-show df = pd.read_csv(data_path + &quot;train.csv&quot;) df.head() . . image_id label . 0 1000015157.jpg | 0 | . 1 1000201771.jpg | 3 | . 2 100042118.jpg | 1 | . 3 1000723321.jpg | 1 | . 4 1000812911.jpg | 3 | . # collapse-show class LeafData(Dataset): def __init__(self, data, directory, transform=None): self.data = data self.directory = directory self.transform = transform def __len__(self): return len(self.data) def __getitem__(self, idx): # import path = os.path.join(self.directory, self.data.iloc[idx][&quot;image_id&quot;]) image = cv2.imread(path, cv2.COLOR_BGR2RGB) # augmentations if self.transform is not None: image = self.transform(image=image)[&quot;image&quot;] return image . . We want to compute stats for raw images, so our data augmentation pipeline should be minimal and not include any heavy transformations we might use during training. Below, we use A.Normalize() with mean = 0 and std = 1 to scale pixel values from [0, 255] to [0, 1] and ToTensorV2() to convert numpy arrays into torch tensors. . # collapse-show augs = A.Compose( [ A.Resize(height=image_size, width=image_size), A.Normalize(mean=(0, 0, 0), std=(1, 1, 1)), ToTensorV2(), ] ) . . Let&#39;s check if our code works correctly. We define a DataLoader to load images in batches from LeafData and plot the first batch. . ####### EXAMINE SAMPLE BATCH # dataset image_dataset = LeafData(data=df, directory=data_path + &quot;train_images/&quot;, transform=augs) # data loader image_loader = DataLoader( image_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True, ) # display images for batch_idx, inputs in enumerate(image_loader): fig = plt.figure(figsize=(14, 7)) for i in range(8): ax = fig.add_subplot(2, 4, i + 1, xticks=[], yticks=[]) plt.imshow(inputs[i].numpy().transpose(1, 2, 0)) break . Looks like everithing is working correctly! Now we can use our image_loader to compute image stats. . 3. Computing image stats . The computation is done in three steps: . Define placeholders to store two batch-level stats: sum and squared sum of pixel values. The first will be used to compute means, and the latter will be needed for standard deviation calculations. | Loop through the batches and add up channel-specific sum and squared sum values. | Perform final calculations to obtain data-level mean and standard deviation. | The first two steps are done in the snippet below. Note that we set axis = [0, 2, 3] to compute mean values with respect to axis 1. The dimensions of inputs is [batch_size x 3 x image_size x image_size], so we need to make sure we aggregate values per each RGB channel separately. . ####### COMPUTE MEAN / STD # placeholders psum = torch.tensor([0.0, 0.0, 0.0]) psum_sq = torch.tensor([0.0, 0.0, 0.0]) # loop through images for inputs in tqdm(image_loader): psum += inputs.sum(axis=[0, 2, 3]) psum_sq += (inputs**2).sum(axis=[0, 2, 3]) . 100%|██████████| 2675/2675 [04:21&lt;00:00, 10.23it/s] . Finally, we make some further calculations: . mean: simply divide the sum of pixel values by the total count - number of pixels in the dataset computed as len(df) * image_size * image_size | standard deviation: use the following equation: total_std = sqrt(psum_sq / count - total_mean ** 2) | . Why we use such a weird formula for STD? Well, because this is how the variance equation can be simplified to make use of the sum of squares when other data is not available. If you are not sure about this, expand the cell below to see a calculation example or read this for some details. . . #collapse-hide # Consider three vectors: A = [1, 1] B = [2, 2] C = [1, 1, 2, 2] # Let&#39;s compute SDs in a classical way: 1. Mean(A) = 1; Mean(B) = 2; Mean(C) = 1.5 2. SD(A) = SD(B) = 0 # because there is no variation around the means 3. SD(C) = sqrt(1/4 * ((1 - 1.5)**2 + (1 - 1.5)**2 + (1 - 1.5)**2 + (1 - 1.5)**2)) = 1/2 # Note that SD(C) is clearly not equal to SD(A) + SD(B), which is zero. # Instead, we could compute SD(C) in three steps using the equation above: 1. psum = 1 + 1 + 2 + 2 = 6 2. psum_sq = (1**2 + 1**2 + 2**2 + 2**2) = 10 3. SD(C) = sqrt((psum_sq - 1/N * psum**2) / N) = sqrt((10 - 36 / 4) / 4) = sqrt(1/4) = 1/2 # We get the same result as in the classical way! . . ####### FINAL CALCULATIONS # pixel count count = len(df) * image_size * image_size # mean and std total_mean = psum / count total_var = (psum_sq / count) - (total_mean**2) total_std = torch.sqrt(total_var) # output print(&quot;mean: &quot; + str(total_mean)) print(&quot;std: &quot; + str(total_std)) . mean: tensor([0.4417, 0.5110, 0.3178]) std: tensor([0.2330, 0.2358, 0.2247]) . This is it! Now you can plug in the mean and std values to A.Normalize() in your data augmentation pipeline to make sure your dataset is normalized :) . 4. Closing words . I hope this tutorial was helpful for those looking for a quick guide on computing the image dataset stats. From my experience, normalizing images with respect to the data-level mean and std does not always help to improve the performance, but it is one of the things I always try first. Happy learning and stay tuned for the next posts! .",
            "url": "https://kozodoi.me/blog/20210308/compute-image-stats",
            "relUrl": "/blog/20210308/compute-image-stats",
            "date": " • Mar 8, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Gradient Accumulation in PyTorch",
            "content": "Last update: 15.10.2021. All opinions are my own. . 1. Overview . Deep learning models are getting bigger and bigger. It becomes difficult to fit such networks in the GPU memory. This is especially relevant in computer vision applications where we need to reserve some memory for high-resolution images as well. As a result, we are sometimes forced to use small batches during training, which may lead to a slower convergence and lower accuracy. . This blog post provides a quick tutorial on how to increase the effective batch size by using a trick called gradient accumulation. Simply speaking, gradient accumulation means that we will use a small batch size but save the gradients and update network weights once every couple of batches. Automated solutions for this exist in higher-level frameworks such as fast.ai or lightning, but those who love using PyTorch might find this tutorial useful. . 2. What is gradient accumulation . When training a neural network, we usually divide our data in mini-batches and go through them one by one. The network predicts batch labels, which are used to compute the loss with respect to the actual targets. Next, we perform backward pass to compute gradients and update model weights in the direction of those gradients. . Gradient accumulation modifies the last step of the training process. Instead of updating the network weights on every batch, we can save gradient values, proceed to the next batch and add up the new gradients. The weight update is then done only after several batches have been processed by the model. . Gradient accumulation helps to imitate a larger batch size. Imagine you want to use 32 images in one batch, but your hardware crashes once you go beyond 8. In that case, you can use batches of 8 images and update weights once every 4 batches. If you accumulate gradients from every batch in between, the results will be (almost) the same and you will be able to perform training on a less expensive machine! . 3. How to make it work . The implementation of gradient accumulation is rather straightforward. The standard training loop without accumulation usually looks like this: . # loop through batches for inputs, labels in data_loader: # extract inputs and labels inputs = inputs.to(device) labels = labels.to(device) # passes and weights update with torch.set_grad_enabled(True): # forward pass preds = model(inputs) loss = criterion(preds, labels) # backward pass loss.backward() # weights update optimizer.step() optimizer.zero_grad() . Now let&#39;s implement gradient accumulation! There are three things we need to do: . Specify the accum_iter parameter. This is just an integer value indicating once in how many batches we would like to update the network weights. | Condition the weight update on the index of the running batch. This requires using enumerate(data_loader) to store the batch index when looping through the data. | Divide the running loss by acum_iter. This normalizes the loss to reduce the contribution of each mini-batch we are actually processing. Depending on the way you compute the loss, you might not need this step: if you average loss within each batch, the division is already correct and there is no need for extra normalization. | # batch accumulation parameter accum_iter = 4 # loop through enumaretad batches for batch_idx, (inputs, labels) in enumerate(data_loader): # extract inputs and labels inputs = inputs.to(device) labels = labels.to(device) # passes and weights update with torch.set_grad_enabled(True): # forward pass preds = model(inputs) loss = criterion(preds, labels) # normalize loss to account for batch accumulation loss = loss / accum_iter # backward pass loss.backward() # weights update if ((batch_idx + 1) % accum_iter == 0) or (batch_idx + 1 == len(data_loader)): optimizer.step() optimizer.zero_grad() . It is really that simple! The gradients are computed when we call loss.backward() and are stored by PyTorch until we call optimizer.zero_grad(). Therefore, we just need to move the weight update performed in optimizer.step() and the gradient reset under the if condition that check the batch index. It is important to also update weights on the last batch when batch_idx + 1 == len(data_loader) - this makes sure that data from the last batches are not discarded and used for optimizing the network. . Please also note that some network architectures have batch-specific operations. For instance, batch normalization is performed on a batch level and therefore may yield slightly different results when using the same effective batch size with and without gradient accumulation. This means that you should not expect to see a 100% match between the results. . In my experience, the potential performance gains from increasing the number of cases used to update the network weights are largest when one is forced to use very small batches (e.g., 8 or 10). Therefore, I always recommend using gradient accumulation when working with large architectures that consume a lof of GPU memory. . 4. Closing words . This is it! I hope this brief tutorial helps you to finally fit that model on your machine and train it with the batch size it deserves. If you are interested, check out my other blog posts on tips on deep learning and PyTorch. Happy learning! .",
            "url": "https://kozodoi.me/blog/20210219/gradient-accumulation",
            "relUrl": "/blog/20210219/gradient-accumulation",
            "date": " • Feb 19, 2021"
        }
        
    
  
    
        ,"post13": {
            "title": "Training PyTorch Models on TPU",
            "content": "Last update: 19.10.2021. All opinions are my own. . 1. Overview . Deep learning heavily relies on Graphical Processing Units (GPUs) to enable fast training. Recently, Google introduced Tensor Processing Units (TPUs) to further advance the speed of computations used in neural networks. Using cloud TPUs is possible on Kaggle and Google Colab. While TPU chips have been optimized for TensorFlow, PyTorch users can also take advantage of the better compute. This requires using PyTorch/XLA and implementing certain changes in the modeling pipeline. . Moving a PyTorch pipeline to TPU includes the following steps: . installing relevant packages ans setting up TPU | adjusting syntax of some modeling steps such as initialization, optimizer and verbosity | distributing data loaders over multiple TPU cores | wrapping data processing, training and inference into a master function | . This post provides a tutorial on using PyTorch/XLA to build the TPU pipeline. The code is optimized for multi-core TPU training. Many of the ideas are adapted from here and here. We will focus on a computer vision application, but the framework can be used with other deep learning models as well. We will use data from RSNA STR Pulmonary Embolism Detection Kaggle competition on detecting pulmonary embolism on more than 1.7 million CT scans. . 2. Preparations and packages . When setting up a script, it is important to introduce two TPU-related parameters: batch size and number of workers. . Google recommends using 128 images per batch for the best performance on the current TPU v3 chips. The v3 chips have 8 cores. This implies that each of the 8 cores can receive a batch of 128 images at each training step, and the modeling can be performed simultaneously on the separate cores. The model weights are then updated based on the outcomes observed on each core. Therefore, the batch size of 128 actually implies 128 * 8 images in each iteration. . #collapse-show # partitioning num_folds = 5 use_fold = 0 # image params image_size = 128 # modeling batch_size = 128 # num_images = batch_size*num_tpu_workers batches_per_epoch = 1000 # num_images = batch_size*batches_per_epoch*num_tpu_workers num_epochs = 1 batch_verbose = 100 num_tpu_workers = 8 # learning rate eta = 0.0001 step = 1 gamma = 0.5 # paths data_path = &#39;/kaggle/input/rsna-str-pulmonary-embolism-detection/&#39; image_path = &#39;/kaggle/input/rsna-str-pe-detection-jpeg-256/train-jpegs/&#39; . . After specifying the parameters, we need to set up TPU by installing and importing torch_xla using the snippet below. There are two options: install the last stable XLA version (1.7 as of 30.03.2021) or the so-called &#39;nightly&#39; version that includes the latest updates but may be unstable. I recommend going for the stable version. . We also specify XLA_USE_BF16 variable (default tensor precision format) and XLA_TENSOR_ALLOCATOR_MAXSIZE variable (maximum tensor allocator size). When working in Google Colab we can also run assert os.environ[&#39;COLAB_TPU_ADDR&#39;] to check that Colab is correctly connected to a TPU instance. . Don&#39;t be discouraged if you see error messages during the installation of fastai, kornia and allennlp. The installation would still proceed to the required versions of torch and torchvision needed to work with TPUs. This may take a few minutes. . # XLA version xla_version = &#39;1.7&#39; # &#39;nightly&#39; or &#39;1.7&#39; # installation !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py !python pytorch-xla-env-setup.py --verion $xla_version # XLA imports import torch_xla import torch_xla.debug.metrics as met import torch_xla.distributed.data_parallel as dp import torch_xla.distributed.parallel_loader as pl import torch_xla.utils.utils as xu import torch_xla.core.xla_model as xm import torch_xla.distributed.xla_multiprocessing as xmp import torch_xla.test.test_utils as test_utils # configurations import os os.environ[&#39;XLA_USE_BF16&#39;] = &#39;1&#39; os.environ[&#39;XLA_TENSOR_ALLOCATOR_MAXSIZE&#39;] = &#39;1000000000&#39; . Running on TPU [&#39;10.0.0.2:8470&#39;] % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 5115 100 5115 0 0 9199 0 --:--:-- --:--:-- --:--:-- 9183 Updating... This may take around 2 minutes. Updating TPU runtime to pytorch-dev20200515 ... ... Setting up libopenblas-dev:amd64 (0.2.20+ds-4) ... update-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so to provide /usr/lib/x86_64-linux-gnu/libblas.so (libblas.so-x86_64-linux-gnu) in auto mode update-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so to provide /usr/lib/x86_64-linux-gnu/liblapack.so (liblapack.so-x86_64-linux-gnu) in auto mode Processing triggers for libc-bin (2.27-3ubuntu1) ... . Next, we import all other relevant libraries. . #collapse-hide import numpy as np import pandas as pd import torch import torchvision import torch.nn as nn import torch.optim as optim from torch.optim import lr_scheduler from torchvision import transforms, models, datasets from torch.utils.data import Dataset from PIL import Image, ImageFile ImageFile.LOAD_TRUNCATED_IMAGES = True import cv2 from sklearn.model_selection import GroupKFold import glob import random import time import sys import os import gc import matplotlib.pyplot as plt import seaborn as sns %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) . . 3. Data preparation . Mostly, the data processing pipeline does not need adjustments when training on TPU instead of GPU. It is only necessary to change the data loaders such that they distribute image batches over the TPU cores. This is covered in the next section that overviews the modeling stage, since the data samplers need to be wrapped into the modeling function. Feel free to skip this section if you already know how to process the image data. . It is important to note that given a more efficient training, data import usually becomes a computational bottleneck. Hence, it is crucial to optimize data reading/processing as much as possible. For the best read performance, I recommend transforming the data to .tfrec format. You can read more about using .tfrec with PyTorch here. . Below, we construct a standard Dataset class to read JPG images of the CT scans. Each image has ten binary labels indicating the presence of pulmonary embolism and its characteristics. . #collapse-hide label_names = [&#39;pe_present_on_image&#39;, &#39;negative_exam_for_pe&#39;, &#39;rv_lv_ratio_gte_1&#39;, &#39;rv_lv_ratio_lt_1&#39;, &#39;leftsided_pe&#39;, &#39;chronic_pe&#39;, &#39;rightsided_pe&#39;, &#39;acute_and_chronic_pe&#39;, &#39;central_pe&#39;, &#39;indeterminate&#39;] . . #collapse-show ### DATASET class PEData(Dataset): def __init__(self, data, directory, transform = None, load_jpg = False, labeled = False): self.data = data self.directory = directory self.transform = transform self.load_jpg = load_jpg self.labeled = labeled def __len__(self): return len(self.data) def __getitem__(self, idx): # import img_name = glob.glob(os.path.join(self.directory, &#39;/&#39;.join(self.data.iloc[idx][[&#39;StudyInstanceUID&#39;, &#39;SeriesInstanceUID&#39;]]) + &#39;/*&#39; + self.data.iloc[idx][&#39;SOPInstanceUID&#39;] + &#39;.jpg&#39;))[0] image = cv2.imread(img_name) # switch channels and normalize image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) image = image / 255.0 # convert image = torch.tensor(image, dtype = torch.float) image = image.permute(2, 0, 1) # augmentations image = self.transform(image) # output if self.labeled: labels = torch.tensor(self.data.iloc[idx][label_names].values.astype(&#39;int&#39;), dtype = torch.float) return image, labels else: return image ### AUGMENTATIONS train_trans = test_trans = transforms.Compose([transforms.ToPILImage(), transforms.Resize(image_size), transforms.ToTensor()]) . . We split data into training and validation folds such that images from the same patient - StudyInstanceUID - do not appear in both. We will only use a single fold for demonstration purposes. . #collapse-hide # partitioning train = pd.read_csv(data_path + &#39;train.csv&#39;) gkf = GroupKFold(n_splits = num_folds) train[&#39;fold&#39;] = -1 for fold, (_, val_idx) in enumerate(gkf.split(train, groups = train[&#39;StudyInstanceUID&#39;])): train.loc[val_idx, &#39;fold&#39;] = fold # load splits data_train = train.loc[train.fold != use_fold].reset_index(drop = True) data_valid = train.loc[train.fold == use_fold].reset_index(drop = True) # datasets train_dataset = PEData(data = data_train, directory = image_path, transform = train_trans, load_jpg = load_jpegs, labeled = True) valid_dataset = PEData(data = data_valid, directory = image_path, transform = test_trans, load_jpg = load_jpegs, labeled = True) . . Before proceeding to modeling, let&#39;s take a look at a sample batch of training images using our processing pipeline. . #collapse-show # sample loader sample_loader = torch.utils.data.DataLoader(valid_dataset, shuffle = False, batch_size = 8, num_workers = 1) # display images for batch_idx, (inputs, labels) in enumerate(sample_loader): fig = plt.figure(figsize = (14, 7)) for i in range(8): ax = fig.add_subplot(2, 4, i + 1, xticks = [], yticks = []) plt.imshow(inputs[i].numpy().transpose(1, 2, 0)) ax.set_title(labels.numpy()[:, i]) break . . 4. Model setup . The modeling stage needs to be modified because the modeling is performed simultaneously on multiple TPU cores. This requires changes to model initialization, optimizer and building a master function to distribute data loaders, training and inference over multi-core TPU chips. Let&#39;s dive in! . We start with the model. We use ResNet-34 with ten output nodes corresponding to each of the binary labels. After initializing the model, we need to wrap it into the MX object that can be sent to TPU. This is done by a simple command mx = xmp.MpModelWrapper(model). . # initialization def init_model(): model = models.resnet34(pretrained = True) model.fc = torch.nn.Linear(in_features = 512, out_features = len(label_names), bias = True) return model # model wrapper model = init_model() mx = xmp.MpModelWrapper(model) . Tracking the running loss when training on multiple TPU cores can a bit difficult since we need to aggregate batch losses between the TPU cores. The following helper class allows to externally store the loss values and update it based on the batch outputs from each worker. . class AverageMeter(object): &#39;&#39;&#39;Computes and stores the average and current value&#39;&#39;&#39; def __init__(self): self.reset() def reset(self): self.val = 0 self.avg = 0 self.sum = 0 self.count = 0 def update(self, val, n = 1): self.val = val self.sum += val * n self.count += n self.avg = self.sum / self.count . Next, we need to wrap modeling into a single master function that can be distributed over TPU cores. We will first define functions for training and inference and then introduce the wrapper function. . The training pass must have several steps: . the optimizer step is done with xm.optimizer_step(optimizer) | the printing statements need to be defined as xm.master_print() instead of print() in order to only print a statement once (otherwise, each TPU core will print it) | dataloader should be defined outside of the function and read as an argument to distribute it over the cores | running loss can be computed using the defined AverageMeter() object | . In addition, it is important to clear the TPU memory as often as possible to ensure that the modeling does not crash: . del [object] to delete objects once they are not needed | gc.collect() to collect garbage left in memory | . #collapse-show ### TRAINING def train_fn(epoch, para_loader, optimizer, criterion, scheduler, device): # initialize model.train() trn_loss_meter = AverageMeter() # training loop for batch_idx, (inputs, labels) in enumerate(para_loader): # extract inputs and labels inputs = inputs.to(device) labels = labels.to(device) optimizer.zero_grad() # forward and backward pass preds = model(inputs) loss = criterion(preds, labels) loss.backward() xm.optimizer_step(optimizer, barrier = True) # barrier is required on single-core training but can be dropped with multiple cores # compute loss trn_loss_meter.update(loss.detach().item(), inputs.size(0)) # feedback if (batch_idx &gt; 0) and (batch_idx % batch_verbose == 0): xm.master_print(&#39;-- batch {} | cur_loss = {:.6f}, avg_loss = {:.6f}&#39;.format( batch_idx, loss.item(), trn_loss_meter.avg)) # clear memory del inputs, labels, preds, loss gc.collect() # early stop if batch_idx &gt; batches_per_epoch: break # scheduler step scheduler.step() # clear memory del para_loader, batch_idx gc.collect() return trn_loss_meter.avg . . Similar to the training pass, inference function uses dataloader as an argument, updates loss using the AverageMeter() object and clears memory after each batch. . #collapse-show ### INFERENCE def valid_fn(epoch, para_loader, criterion, device): # initialize model.eval() val_loss_meter = AverageMeter() # validation loop for batch_idx, (inputs, labels) in enumerate(para_loader): # extract inputs and labels inputs = inputs.to(device) labels = labels.to(device) # compute preds with torch.no_grad(): preds = model(inputs) loss = criterion(preds, labels) # compute loss val_loss_meter.update(loss.detach().item(), inputs.size(0)) # feedback if (batch_idx &gt; 0) and (batch_idx % batch_verbose == 0): xm.master_print(&#39;-- batch {} | cur_loss = {:.6f}, avg_loss = {:.6f}&#39;.format( batch_idx, loss.item(), val_loss_meter.avg)) # clear memory del inputs, labels, preds, loss gc.collect() # clear memory del para_loader, batch_idx gc.collect() return val_loss_meter.avg . . The master modeling function also includes several TPU-based modifications. . First, we need to create a distributed data sampler that reads our Dataset object and distributes batches over TPU cores. This is done with torch.utils.data.distributed.DistributedSampler(), which allows data loaders from different cores to only take a portion of the whole dataset. Setting num_replicas to xm.xrt_world_size() checks the number of available TPU cores. After defining the sampler, we can set up a data loader that uses the sampler. . Second, the model is sent to TPU with the following code: . device = xm.xla_device() model = mx.to(device) . Third, we need to update learning rate since the modeling is done simultaneously on batches on different cores: scaled_eta = eta * xm.xrt_world_size(). . Finally, we continue keeping track of the memory and clearing it whenever possible, and use xm.master_print() for displaying intermediate results. We also set up the function to return lists with the training and validation loss values. . #collapse-show ### MASTER FUNCTION def _run(model): ### DATA PREP # data samplers train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset, num_replicas = xm.xrt_world_size(), rank = xm.get_ordinal(), shuffle = True) valid_sampler = torch.utils.data.distributed.DistributedSampler(valid_dataset, num_replicas = xm.xrt_world_size(), rank = xm.get_ordinal(), shuffle = False) # data loaders valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = batch_size, sampler = valid_sampler, num_workers = 0, pin_memory = True) train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, sampler = train_sampler, num_workers = 0, pin_memory = True) ### MODEL PREP # send to TPU device = xm.xla_device() model = mx.to(device) # scale LR scaled_eta = eta * xm.xrt_world_size() # optimizer and loss criterion = nn.BCEWithLogitsLoss() optimizer = optim.Adam(model.parameters(), lr = scaled_eta) scheduler = lr_scheduler.StepLR(optimizer, step_size = step, gamma = gamma) ### MODELING # placeholders trn_losses = [] val_losses = [] best_val_loss = 1 # modeling loop gc.collect() for epoch in range(num_epochs): # display info xm.master_print(&#39;-&#39;*55) xm.master_print(&#39;EPOCH {}/{}&#39;.format(epoch + 1, num_epochs)) xm.master_print(&#39;-&#39;*55) xm.master_print(&#39;- initialization | TPU cores = {}, lr = {:.6f}&#39;.format( xm.xrt_world_size(), scheduler.get_lr()[len(scheduler.get_lr()) - 1] / xm.xrt_world_size())) epoch_start = time.time() gc.collect() # update train_loader shuffling train_loader.sampler.set_epoch(epoch) # training pass train_start = time.time() xm.master_print(&#39;- training...&#39;) para_loader = pl.ParallelLoader(train_loader, [device]) trn_loss = train_fn(epoch = epoch + 1, para_loader = para_loader.per_device_loader(device), criterion = criterion, optimizer = optimizer, scheduler = scheduler, device = device) del para_loader gc.collect() # validation pass valid_start = time.time() xm.master_print(&#39;- validation...&#39;) para_loader = pl.ParallelLoader(valid_loader, [device]) val_loss = valid_fn(epoch = epoch + 1, para_loader = para_loader.per_device_loader(device), criterion = criterion, device = device) del para_loader gc.collect() # save weights if val_loss &lt; best_val_loss: xm.save(model.state_dict(), &#39;weights_{}.pt&#39;.format(model_name)) best_val_loss = val_loss # display info xm.master_print(&#39;- elapsed time | train = {:.2f} min, valid = {:.2f} min&#39;.format( (valid_start - train_start) / 60, (time.time() - valid_start) / 60)) xm.master_print(&#39;- average loss | train = {:.6f}, valid = {:.6f}&#39;.format( trn_loss, val_loss)) xm.master_print(&#39;-&#39;*55) xm.master_print(&#39;&#39;) # save losses trn_losses.append(trn_loss) val_losses.append(val_loss) del trn_loss, val_loss gc.collect() # print results xm.master_print(&#39;Best results: loss = {:.6f} (epoch {})&#39;.format(np.min(val_losses), np.argmin(val_losses) + 1)) return trn_losses, val_losses . . 5. Modeling . After all helper functions have been introduced, we can finally launch the training! To do that, we need to define the last wrapper function that runs the modeling on multiple TPU cores: _mp_fn(rank, flags). Within the wrapper function, we set default tensor type to make sure that new tensors are initialized as float torch tensors on TPU and then run the modeling. It is important to set nprocs to the number of available TPU cores. The FLAGS object can be used to pass further training arguments to the modeling function. . Running xmp.spawn() will launch our _mp_fn() on multiple TPU cores! Since it does not provide any output, it is useful to save losses or other objects you might be interested in after running the _run() function. . # wrapper function def _mp_fn(rank, flags): torch.set_default_tensor_type(&#39;torch.FloatTensor&#39;) trn_losses, val_losses = _run(model) np.save(&#39;trn_losses.npy&#39;, np.array(trn_losses)) np.save(&#39;val_losses.npy&#39;, np.array(val_losses)) # modeling gc.collect() FLAGS = {} xmp.spawn(_mp_fn, args = (FLAGS,), nprocs = num_tpu_workers, start_method = &#39;fork&#39;) . - EPOCH 1/1 - - initialization | TPU cores = 8, lr = 0.000010 - training... -- batch 100 | cur_loss = 0.343750, avg_loss = 0.367342 -- batch 200 | cur_loss = 0.285156, avg_loss = 0.333299 -- batch 300 | cur_loss = 0.241211, avg_loss = 0.311770 -- batch 400 | cur_loss = 0.194336, avg_loss = 0.292837 -- batch 500 | cur_loss = 0.179688, avg_loss = 0.275745 -- batch 600 | cur_loss = 0.180664, avg_loss = 0.260264 -- batch 700 | cur_loss = 0.166016, avg_loss = 0.246196 -- batch 800 | cur_loss = 0.139648, avg_loss = 0.232805 -- batch 900 | cur_loss = 0.085449, avg_loss = 0.220550 -- batch 1000 | cur_loss = 0.109375, avg_loss = 0.209381 - validation... -- batch 100 | cur_loss = 0.184570, avg_loss = 0.353043 - elapsed time | train = 125.82 min, valid = 26.05 min - average loss | train = 0.209277, valid = 0.366887 - Best results: loss = 0.366887 (epoch 1) . The training is working! Note that every 100 batches displayed in the snippet above actually refer to 100 * batch_size * num_tpu_workers images, since every core processes the same amount of different images simultaneously but printing is done just from one core. . 6. Closing words . This is the end of this blog post. Using a computer vision application, we demonstrated how to use PyTorch/XLA to take advantage of TPU when training deep learning models. We covered important changes that need to be implemented into the modeling pipeline to enable TPU-based training, including data processing, modeling and displaying results. I hope this post will help you to get started with TPUs! . If you are interested in further reading, make sure to check tutorial notebooks developed by PyTorch/XLA team available at their GitHub repo. .",
            "url": "https://kozodoi.me/blog/20201030/pytorch-xla-tpu",
            "relUrl": "/blog/20201030/pytorch-xla-tpu",
            "date": " • Oct 30, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Pre-Training with Surrogate Labels",
            "content": "Last update: 22.10.2021. All opinions are my own. . 1. Overview . In many real-world settings, the size of the labeled training sample is lower compared to the unlabeled test data. This blogpost demonstrates a technique that can improve the performance of neural networks in such settings by learning from both training and test data. This is done by pre-training a model on the complete data set using a surrogate label. The approach can help to reduce the impact of sampling bias by exposing the model to the test data and benefit from a larger sample size while learning. . We will focus on a computer vision application, but the idea can be used with deep learning models in other domains. We will use data from SIIM-ISIC Melanoma Classification Kaggle competition to distinguish malignant and benign lesions on medical images. The modeling is performed in tensorflow. A shorter and interactive version of this blogpost is also available as a Kaggle notebook. . 2. Intuition . How to make use of the test sample on the pre-training stage? The labels are only observed for the training data. Luckily, in many settings, there is a bunch of meta-data available for both labeled and unlabeled images. Consider the task of lung cancer detection. The CT scans of cancer patients may contain information on the patient&#39;s age and gender. In contrast with the label, which requires medical tests or experts&#39; diagnosis, meta-data is available at no additional cost. Another example is bird image classification, where the image meta-data such as time and location of the photo can serve the same purpose. In this blogpost, we will focus on malignant lesion classification, where patient meta-data is available for all images. . We can leverage meta-data in the following way: . Pre-train a supplementary model on the complete train + test data using one of the meta-features as a surrogate label. | Initialize from the pre-trained weights when training the main model. | The intuition behind this approach is that by learning to classify images according to one of meta variables, the model can learn some of the visual features that might be useful for the main task, which in our case is malignant lesion classification. For instance, lesion size and skin color can be helpful in determining both lesion location (surrogate label) and lesion type (actual label). Exposing the model to the test data also allows it to take a sneak peek at test images, which may help to learn patterns prevalent in the test distribution. . P.S. The notebook heavily relies on the great modeling pipeline developed by Chris Deotte for the SIIM-ISIC competition and reuses much of his original code. Kindly refer to his notebook for general questions on the pipeline where he provided comments and documentation. . 3. Initialization . #collapse-hide ### PACKAGES !pip install -q efficientnet &gt;&gt; /dev/null import pandas as pd, numpy as np from kaggle_datasets import KaggleDatasets import tensorflow as tf, re, math import tensorflow.keras.backend as K import efficientnet.tfkeras as efn from sklearn.model_selection import KFold from sklearn.metrics import roc_auc_score import matplotlib.pyplot as plt from scipy.stats import rankdata import PIL, cv2 . . Let&#39;s set up training parameters such as image size, number of folds and batch size. In addition to these parameters, we introduce USE_PRETRAIN_WEIGHTS variable to reflect whether we want to pre-train a supplementary model on full data before training the main melanoma classification model. . For demonstration purposes, we use EfficientNet B0, 128x128 image size and no TTA. Feel free to experiment with larger architectures and images sizes by editing this notebook. . #collapse-show # DEVICE DEVICE = &quot;TPU&quot; # USE DIFFERENT SEED FOR DIFFERENT STRATIFIED KFOLD SEED = 42 # NUMBER OF FOLDS. USE 3, 5, OR 15 FOLDS = 5 # WHICH IMAGE SIZES TO LOAD EACH FOLD IMG_SIZES = [128]*FOLDS # BATCH SIZE AND EPOCHS BATCH_SIZES = [32]*FOLDS EPOCHS = [10]*FOLDS # WHICH EFFICIENTNET TO USE EFF_NETS = [0]*FOLDS # WEIGHTS FOR FOLD MODELS WHEN PREDICTING TEST WGTS = [1/FOLDS]*FOLDS # PRETRAINED WEIGHTS USE_PRETRAIN_WEIGHTS = True . . Below, we connect to TPU or GPU for faster training. . #collapse-hide # CONNECT TO DEVICE if DEVICE == &quot;TPU&quot;: print(&quot;connecting to TPU...&quot;) try: tpu = tf.distribute.cluster_resolver.TPUClusterResolver() print(&#39;Running on TPU &#39;, tpu.master()) except ValueError: print(&quot;Could not connect to TPU&quot;) tpu = None if tpu: try: print(&quot;initializing TPU ...&quot;) tf.config.experimental_connect_to_cluster(tpu) tf.tpu.experimental.initialize_tpu_system(tpu) strategy = tf.distribute.experimental.TPUStrategy(tpu) print(&quot;TPU initialized&quot;) except _: print(&quot;failed to initialize TPU&quot;) else: DEVICE = &quot;GPU&quot; if DEVICE != &quot;TPU&quot;: print(&quot;Using default strategy for CPU and single GPU&quot;) strategy = tf.distribute.get_strategy() if DEVICE == &quot;GPU&quot;: print(&quot;Num GPUs Available: &quot;, len(tf.config.experimental.list_physical_devices(&#39;GPU&#39;))) AUTO = tf.data.experimental.AUTOTUNE REPLICAS = strategy.num_replicas_in_sync print(f&#39;REPLICAS: {REPLICAS}&#39;) . . connecting to TPU... Running on TPU grpc://10.0.0.2:8470 initializing TPU ... TPU initialized REPLICAS: 8 . 4. Image processing . First, we specify data paths. The data is stored as tfrecords to enable fast processing. You can read more on the data here. . #collapse-show # IMAGE PATHS GCS_PATH = [None]*FOLDS for i,k in enumerate(IMG_SIZES): GCS_PATH[i] = KaggleDatasets().get_gcs_path(&#39;melanoma-%ix%i&#39;%(k,k)) files_train = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + &#39;/train*.tfrec&#39;))) files_test = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[0] + &#39;/test*.tfrec&#39;))) . . The read_labeled_tfrecord() function provides two outputs: . Image tensor. | Either anatom_site_general_challenge or target as a label. The former is a one-hot-encoded categorical feature with six possible values indicating the lesion location. The latter is a binary target indicating whether the lesion is malignant. The selection of the label is controlled by the pretraining argument read from the get_dataset() function below. Setting pretraining = True implies using anatom_site_general_challenge as a surrogate label. | We also set up read_unlabeled_tfrecord() that returns image and image name. . #collapse-show def read_labeled_tfrecord(example, pretraining = False): if pretraining: tfrec_format = { &#39;image&#39; : tf.io.FixedLenFeature([], tf.string), &#39;image_name&#39; : tf.io.FixedLenFeature([], tf.string), &#39;anatom_site_general_challenge&#39;: tf.io.FixedLenFeature([], tf.int64), } else: tfrec_format = { &#39;image&#39; : tf.io.FixedLenFeature([], tf.string), &#39;image_name&#39; : tf.io.FixedLenFeature([], tf.string), &#39;target&#39; : tf.io.FixedLenFeature([], tf.int64) } example = tf.io.parse_single_example(example, tfrec_format) return example[&#39;image&#39;], tf.one_hot(example[&#39;anatom_site_general_challenge&#39;], 6) if pretraining else example[&#39;target&#39;] def read_unlabeled_tfrecord(example, return_image_name=True): tfrec_format = { &#39;image&#39; : tf.io.FixedLenFeature([], tf.string), &#39;image_name&#39; : tf.io.FixedLenFeature([], tf.string), } example = tf.io.parse_single_example(example, tfrec_format) return example[&#39;image&#39;], example[&#39;image_name&#39;] if return_image_name else 0 def prepare_image(img, dim = 256): img = tf.image.decode_jpeg(img, channels = 3) img = tf.cast(img, tf.float32) / 255.0 img = img * circle_mask img = tf.reshape(img, [dim,dim, 3]) return img def count_data_items(filenames): n = [int(re.compile(r&quot;-([0-9]*) .&quot;).search(filename).group(1)) for filename in filenames] return np.sum(n) . . The get_dataset() function is a wrapper function that loads and processes images given the arguments that control the import options. . #collapse-show def get_dataset(files, shuffle = False, repeat = False, labeled = True, pretraining = False, return_image_names = True, batch_size = 16, dim = 256): ds = tf.data.TFRecordDataset(files, num_parallel_reads = AUTO) ds = ds.cache() if repeat: ds = ds.repeat() if shuffle: ds = ds.shuffle(1024*2) #if too large causes OOM in GPU CPU opt = tf.data.Options() opt.experimental_deterministic = False ds = ds.with_options(opt) if labeled: ds = ds = ds.map(lambda example: read_labeled_tfrecord(example, pretraining), num_parallel_calls=AUTO) else: ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_names), num_parallel_calls = AUTO) ds = ds.map(lambda img, imgname_or_label: ( prepare_image(img, dim = dim), imgname_or_label), num_parallel_calls = AUTO) ds = ds.batch(batch_size * REPLICAS) ds = ds.prefetch(AUTO) return ds . . We also use a circular crop (a.k.a. microscope augmentation) to improve image consistency. The snippet below creates a circular mask, which is applied in the prepare_image() function. . #collapse-show # CIRCLE CROP PREPARATIONS circle_img = np.zeros((IMG_SIZES[0], IMG_SIZES[0]), np.uint8) circle_img = cv2.circle(circle_img, (int(IMG_SIZES[0]/2), int(IMG_SIZES[0]/2)), int(IMG_SIZES[0]/2), 1, thickness = -1) circle_img = np.repeat(circle_img[:, :, np.newaxis], 3, axis = 2) circle_mask = tf.cast(circle_img, tf.float32) . . Let&#39;s have a quick look at a batch of our images: . #collapse-hide # LOAD DATA AND APPLY AUGMENTATIONS def show_dataset(thumb_size, cols, rows, ds): mosaic = PIL.Image.new(mode=&#39;RGB&#39;, size=(thumb_size*cols + (cols-1), thumb_size*rows + (rows-1))) for idx, data in enumerate(iter(ds)): img, target_or_imgid = data ix = idx % cols iy = idx // cols img = np.clip(img.numpy() * 255, 0, 255).astype(np.uint8) img = PIL.Image.fromarray(img) img = img.resize((thumb_size, thumb_size), resample = PIL.Image.BILINEAR) mosaic.paste(img, (ix*thumb_size + ix, iy*thumb_size + iy)) nn = target_or_imgid.numpy().decode(&quot;utf-8&quot;) display(mosaic) return nn files_train = tf.io.gfile.glob(GCS_PATH[0] + &#39;/train*.tfrec&#39;) ds = tf.data.TFRecordDataset(files_train, num_parallel_reads = AUTO).shuffle(1024) ds = ds.take(10).cache() ds = ds.map(read_unlabeled_tfrecord, num_parallel_calls = AUTO) ds = ds.map(lambda img, target: (prepare_image(img, dim = IMG_SIZES[0]), target), num_parallel_calls = AUTO) ds = ds.take(12*5) ds = ds.prefetch(AUTO) # DISPLAY IMAGES name = show_dataset(128, 5, 2, ds) . . i# 5. Modeling . Pre-trained model with surrogate label . The build_model() function incorporates three important features that depend on the training regime: . When building a model for pre-training, we use CategoricalCrossentropy as a loss because anatom_site_general_challenge is a categorical variable. When building a model that classifies lesions as benign/malignant, we use BinaryCrossentropy as a loss. | When training a final binary classification model, we load the pre-trained weights using base.load_weights(&#39;base_weights.h5&#39;) if use_pretrain_weights == True. | We use a dense layer with six output nodes and softmax activation when doing pre-training and a dense layer with a single output node and sigmoid activation when training a final model. | #collapse-show EFNS = [efn.EfficientNetB0, efn.EfficientNetB1, efn.EfficientNetB2, efn.EfficientNetB3, efn.EfficientNetB4, efn.EfficientNetB5, efn.EfficientNetB6, efn.EfficientNetB7] def build_model(dim = 256, ef = 0, pretraining = False, use_pretrain_weights = False): # base inp = tf.keras.layers.Input(shape = (dim,dim,3)) base = EFNS[ef](input_shape = (dim,dim,3), weights = &#39;imagenet&#39;, include_top = False) # base weights if use_pretrain_weights: base.load_weights(&#39;base_weights.h5&#39;) x = base(inp) x = tf.keras.layers.GlobalAveragePooling2D()(x) if pretraining: x = tf.keras.layers.Dense(6, activation = &#39;softmax&#39;)(x) model = tf.keras.Model(inputs = inp, outputs = x) opt = tf.keras.optimizers.Adam(learning_rate = 0.001) loss = tf.keras.losses.CategoricalCrossentropy() model.compile(optimizer = opt, loss = loss) else: x = tf.keras.layers.Dense(1, activation = &#39;sigmoid&#39;)(x) model = tf.keras.Model(inputs = inp, outputs = x) opt = tf.keras.optimizers.Adam(learning_rate = 0.001) loss = tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.01) model.compile(optimizer = opt, loss = loss, metrics = [&#39;AUC&#39;]) return model . . #collapse-hide ### LEARNING RATE SCHEDULE def get_lr_callback(batch_size=8): lr_start = 0.000005 lr_max = 0.00000125 * REPLICAS * batch_size lr_min = 0.000001 lr_ramp_ep = 5 lr_sus_ep = 0 lr_decay = 0.8 def lrfn(epoch): if epoch &lt; lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start elif epoch &lt; lr_ramp_ep + lr_sus_ep: lr = lr_max else: lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min return lr lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False) return lr_callback . . The pre-trained model is trained on both training and test data. Here, we use the original training data merged with the complete test set as a training sample. We fix the number of training epochs to EPOCHS and do not perform early stopping. You can also experiment with setting up a small validation sample from both training and test data to perform early stopping. . #collapse-show ### PRE-TRAINED MODEL if USE_PRETRAIN_WEIGHTS: # USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit VERBOSE = 2 # DISPLAY INFO if DEVICE == &#39;TPU&#39;: if tpu: tf.tpu.experimental.initialize_tpu_system(tpu) # CREATE TRAIN AND VALIDATION SUBSETS files_train = tf.io.gfile.glob(GCS_PATH[0] + &#39;/train*.tfrec&#39;) print(&#39;#### Using 2020 train data&#39;) files_train += tf.io.gfile.glob(GCS_PATH[0] + &#39;/test*.tfrec&#39;) print(&#39;#### Using 2020 test data&#39;) np.random.shuffle(files_train) # BUILD MODEL K.clear_session() tf.random.set_seed(SEED) with strategy.scope(): model = build_model(dim = IMG_SIZES[0], ef = EFF_NETS[0], pretraining = True) # SAVE BEST MODEL EACH FOLD sv = tf.keras.callbacks.ModelCheckpoint( &#39;weights.h5&#39;, monitor=&#39;loss&#39;, verbose=0, save_best_only=True, save_weights_only=True, mode=&#39;min&#39;, save_freq=&#39;epoch&#39;) # TRAIN print(&#39;Training...&#39;) history = model.fit( get_dataset(files_train, dim = IMG_SIZES[0], batch_size = BATCH_SIZES[0], shuffle = True, repeat = True, pretraining = True), epochs = EPOCHS[0], callbacks = [sv, get_lr_callback(BATCH_SIZES[0])], steps_per_epoch = count_data_items(files_train)/BATCH_SIZES[0]//REPLICAS, verbose = VERBOSE) else: print(&#39;#### NOT using a pre-trained model&#39;) . . #### Using 2020 train data #### Using 2020 test data Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b0_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5 16809984/16804768 [==============================] - 0s 0us/step Training... Epoch 1/10 170/170 - 10s - loss: 1.7556 - lr: 5.0000e-06 Epoch 2/10 170/170 - 10s - loss: 1.1257 - lr: 6.8000e-05 Epoch 3/10 170/170 - 11s - loss: 0.8906 - lr: 1.3100e-04 Epoch 4/10 170/170 - 10s - loss: 0.8118 - lr: 1.9400e-04 Epoch 5/10 170/170 - 9s - loss: 0.8222 - lr: 2.5700e-04 Epoch 6/10 170/170 - 9s - loss: 0.8626 - lr: 3.2000e-04 Epoch 7/10 170/170 - 9s - loss: 0.8402 - lr: 2.5620e-04 Epoch 8/10 170/170 - 9s - loss: 0.8257 - lr: 2.0516e-04 Epoch 9/10 170/170 - 10s - loss: 0.8091 - lr: 1.6433e-04 Epoch 10/10 170/170 - 10s - loss: 0.7865 - lr: 1.3166e-04 . The pre-training is complete! Now, we need to resave weights of our pre-trained model to make it easier to load them in the future. We are not really interested in the classification head, so we only export the weights of the convolutional part of the network. We can index these layers using model.layers[1]. . #collapse-show # LOAD WEIGHTS AND CHECK MODEL if USE_PRETRAIN_WEIGHTS: model.load_weights(&#39;weights.h5&#39;) model.summary() # EXPORT BASE WEIGHTS if USE_PRETRAIN_WEIGHTS: model.layers[1].save_weights(&#39;base_weights.h5&#39;) . . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 128, 128, 3)] 0 _________________________________________________________________ efficientnet-b0 (Model) (None, 4, 4, 1280) 4049564 _________________________________________________________________ global_average_pooling2d (Gl (None, 1280) 0 _________________________________________________________________ dense (Dense) (None, 6) 7686 ================================================================= Total params: 4,057,250 Trainable params: 4,015,234 Non-trainable params: 42,016 _________________________________________________________________ . Main classification model . Now we can train a final classification model using a cross-validation framework on the training data! . We need to take care of a couple of changes: . Make sure that we don&#39;t use test data in the training folds. | Set use_pretrain_weights = True and pretraining = False in the build_model() function to initialize from the pre-trained weights in the beginning of each fold. | #collapse-show # USE VERBOSE=0 for silent, VERBOSE=1 for interactive, VERBOSE=2 for commit VERBOSE = 0 skf = KFold(n_splits = FOLDS, shuffle = True, random_state = SEED) oof_pred = []; oof_tar = []; oof_val = []; oof_names = []; oof_folds = [] preds = np.zeros((count_data_items(files_test),1)) for fold,(idxT,idxV) in enumerate(skf.split(np.arange(15))): # DISPLAY FOLD INFO if DEVICE == &#39;TPU&#39;: if tpu: tf.tpu.experimental.initialize_tpu_system(tpu) print(&#39;#&#39;*25); print(&#39;#### FOLD&#39;,fold+1) # CREATE TRAIN AND VALIDATION SUBSETS files_train = tf.io.gfile.glob([GCS_PATH[fold] + &#39;/train%.2i*.tfrec&#39;%x for x in idxT]) np.random.shuffle(files_train); print(&#39;#&#39;*25) files_valid = tf.io.gfile.glob([GCS_PATH[fold] + &#39;/train%.2i*.tfrec&#39;%x for x in idxV]) files_test = np.sort(np.array(tf.io.gfile.glob(GCS_PATH[fold] + &#39;/test*.tfrec&#39;))) # BUILD MODEL K.clear_session() tf.random.set_seed(SEED) with strategy.scope(): model = build_model(dim = IMG_SIZES[fold], ef = EFF_NETS[fold], use_pretrain_weights = USE_PRETRAIN_WEIGHTS, pretraining = False) # SAVE BEST MODEL EACH FOLD sv = tf.keras.callbacks.ModelCheckpoint( &#39;fold-%i.h5&#39;%fold, monitor=&#39;val_auc&#39;, verbose=0, save_best_only=True, save_weights_only=True, mode=&#39;max&#39;, save_freq=&#39;epoch&#39;) # TRAIN print(&#39;Training...&#39;) history = model.fit( get_dataset(files_train, shuffle = True, repeat = True, dim = IMG_SIZES[fold], batch_size = BATCH_SIZES[fold]), epochs = EPOCHS[fold], callbacks = [sv,get_lr_callback(BATCH_SIZES[fold])], steps_per_epoch = count_data_items(files_train)/BATCH_SIZES[fold]//REPLICAS, validation_data = get_dataset(files_valid, shuffle = False, repeat = False, dim = IMG_SIZES[fold]), verbose = VERBOSE ) model.load_weights(&#39;fold-%i.h5&#39;%fold) # PREDICT OOF print(&#39;Predicting OOF...&#39;) ds_valid = get_dataset(files_valid,labeled=False,return_image_names=False,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4) ct_valid = count_data_items(files_valid); STEPS = ct_valid/BATCH_SIZES[fold]/4/REPLICAS pred = model.predict(ds_valid,steps=STEPS,verbose=VERBOSE)[:ct_valid,] oof_pred.append(pred) # GET OOF TARGETS AND NAMES ds_valid = get_dataset(files_valid,dim=IMG_SIZES[fold],labeled=True, return_image_names=True) oof_tar.append(np.array([target.numpy() for img, target in iter(ds_valid.unbatch())]) ) oof_folds.append(np.ones_like(oof_tar[-1],dtype=&#39;int8&#39;)*fold ) ds = get_dataset(files_valid,dim=IMG_SIZES[fold],labeled=False,return_image_names=True) oof_names.append(np.array([img_name.numpy().decode(&quot;utf-8&quot;) for img, img_name in iter(ds.unbatch())])) # PREDICT TEST print(&#39;Predicting Test...&#39;) ds_test = get_dataset(files_test,labeled=False,return_image_names=False,shuffle=False,dim=IMG_SIZES[fold],batch_size=BATCH_SIZES[fold]*4) ct_test = count_data_items(files_test); STEPS = ct_test/BATCH_SIZES[fold]/4/REPLICAS pred = model.predict(ds_test,steps=STEPS,verbose=VERBOSE)[:ct_test,] preds[:,0] += (pred * WGTS[fold]).reshape(-1) . . ######################### #### FOLD 1 ######################### Training... Predicting OOF... Predicting Test... ######################### #### FOLD 2 ######################### Training... Predicting OOF... Predicting Test... ######################### #### FOLD 3 ######################### Training... Predicting OOF... Predicting Test... ######################### #### FOLD 4 ######################### Training... Predicting OOF... Predicting Test... ######################### #### FOLD 5 ######################### Training... Predicting OOF... Predicting Test... . #collapse-show # COMPUTE OOF AUC oof = np.concatenate(oof_pred); true = np.concatenate(oof_tar); names = np.concatenate(oof_names); folds = np.concatenate(oof_folds) auc = roc_auc_score(true,oof) print(&#39;Overall OOF AUC = %.4f&#39;%auc) . . Overall OOF AUC = 0.8414 . How does the OOF AUC compare to a model without the pre-training stage? To check this, we can simply set USE_PRETRAIN_WEIGHTS = False in the beginning of the notebook. This is done in thus version of the Kaggle notebook, yielding a model with a lower OOF AUC (0.8329 compared to 0.8414 with pre-training). . Compared to a model initialized from the Imagenet weights, pre-training on a surrogate label brings a CV improvement. The AUC gain also translates into the performance gain on the competition leaderboard (increase from 0.8582 to 0.8809). Great news! . 6. Closing words . This is the end of this blogpost. Using a computer vision application, we demonstrated how to use meta-data to construct a surrogate label and pre-train a CNN on both training and test data to improve performance. . The pre-trained model can be further optimized to increase performance gains. Using a validation subset on the pre-training stage can help to tune the number of epochs and other learning parameters. Another idea could be to construct a surrogate label with more unique values (e.g., combination of anatom_site_general_challenge and sex) to make the pre-training task more challenging and motivate the model to learn better. On the other hand, further optimizing the main classification model may reduce the benefit of pre-training. .",
            "url": "https://kozodoi.me/blog/20200830/pre-training",
            "relUrl": "/blog/20200830/pre-training",
            "date": " • Aug 30, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Profit-Driven Demand Forecasting",
            "content": "Last update: 21.03.2021. All opinions are my own. . 1. Overview . Demand forecasting is an important task that helps to optimize inventory planning. Optimized stocks reduce retailer&#39;s costs and increase customer satisfaction due to faster delivery time. . The 2020 edition of the Data Mining Cup was devoted to profit-driven demand prediction for a set of items using past purchase data. Together with Elizaveta Zinovyeva, we represented the Humboldt University of Berlin and finished in the top-15 of the leaderboard. . This blog post provides a detailed walkthrough covering the crucial steps of our solution: . data preparation and feature engineering | aggregation of transactional data into the daily format | implementation of custom profit loss functions | two-stage demand forecasting with LightGBM | hyper-parameter tuning with hyperopt | . Feel free to jump directly to the sections interesting to you! The code with our solution is available on Github. . 2. Data preparation . Data overview . The competition data includes three files: . items.csv: item-specific characteristics such as brand, manufacturer, etc | orders.csv: purchase transactions over the 6-month period | infos.csv: prices and promotions in the unlabeled test set | . Let&#39;s have a look at the data: . # packages import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # data import infos = pd.read_csv(&#39;../data/raw/infos.csv&#39;, sep = &#39;|&#39;) items = pd.read_csv(&#39;../data/raw/items.csv&#39;, sep = &#39;|&#39;) orders = pd.read_csv(&#39;../data/raw/orders.csv&#39;, sep = &#39;|&#39;) print(infos.shape) print(items.shape) print(orders.shape) . (10463, 3) (10463, 8) (2181955, 5) . infos.head(3) . itemID simulationPrice promotion . 0 | 1 | 3.43 | NaN | . 1 | 2 | 9.15 | NaN | . 2 | 3 | 14.04 | NaN | . items.head(3) . itemID brand manufacturer customerRating category1 category2 category3 recommendedRetailPrice . 0 | 1 | 0 | 1 | 4.38 | 1 | 1 | 1 | 8.84 | . 1 | 2 | 0 | 2 | 3.00 | 1 | 2 | 1 | 16.92 | . 2 | 3 | 0 | 3 | 5.00 | 1 | 3 | 1 | 15.89 | . orders.head(3) . time transactID itemID order salesPrice . 0 | 2018-01-01 00:01:56 | 2278968 | 450 | 1 | 17.42 | . 1 | 2018-01-01 00:01:56 | 2278968 | 83 | 1 | 5.19 | . 2 | 2018-01-01 00:07:11 | 2255797 | 7851 | 2 | 20.47 | . For each of the 10,463 items, we need to predict the total number of orders in the 14-day period following the last day in orders. . Preprocessing . Let&#39;s prepare the data! First, we merge item-level data in items and infos: . print(infos.shape) print(items.shape) items = pd.merge(infos, items, on = &#39;itemID&#39;, how = &#39;left&#39;) print(items.shape) del infos . (10463, 3) (10463, 8) (10463, 10) . Next, we check and convert feature types to the appropriate format: . #collapse-hide print(&#39;-&#39; * 50) print(items.dtypes) print(&#39;-&#39; * 50) print(orders.dtypes) print(&#39;-&#39; * 50) # items for var in [&#39;itemID&#39;, &#39;brand&#39;, &#39;manufacturer&#39;, &#39;category1&#39;, &#39;category2&#39;, &#39;category3&#39;]: items[var] = items[var].astype(&#39;str&#39;).astype(&#39;object&#39;) # orders for var in [&#39;transactID&#39;, &#39;itemID&#39;]: orders[var] = orders[var].astype(&#39;str&#39;).astype(&#39;object&#39;) # dates orders[&#39;time&#39;] = pd.to_datetime(orders[&#39;time&#39;].astype(&#39;str&#39;), infer_datetime_format = True) . . -- itemID int64 simulationPrice float64 promotion object brand int64 manufacturer int64 customerRating float64 category1 int64 category2 int64 category3 int64 recommendedRetailPrice float64 dtype: object -- time object transactID int64 itemID int64 order int64 salesPrice float64 dtype: object -- . Finally, we unfold the promotion feature containing a sequence of coma-separated dates. We use split_nested_features() from dptools to split a string column into separate features. . dptools is a package developed by me to simplify common data preprocessing and feature engineering tasks. Below, you will see more examples on using dptools for other applications. You can read more about the package here. . #collapse-show # import packages !pip install dptools from dptools import * # split promotion feature items = split_nested_features(items, split_vars = &#39;promotion&#39;, sep = &#39;,&#39;) print(items.head(3)) # convert dates promotion_vars = items.filter(like = &#39;promotion_&#39;).columns for var in promotion_vars: items[var] = pd.to_datetime(items[var], infer_datetime_format = True) . . Added 3 split-based features. . itemID simulationPrice brand manufacturer customerRating category1 category2 category3 recommendedRetailPrice promotion_0 promotion_1 promotion_2 . 0 | 1 | 3.43 | NaN | 1 | 4.38 | 1 | 1 | 1 | 8.84 | NaN | NaN | NaN | . 1 | 2 | 9.15 | NaN | 2 | 3.00 | 1 | 2 | 1 | 16.92 | NaN | NaN | NaN | . 2 | 3 | 14.04 | NaN | 3 | 5.00 | 1 | 3 | 1 | 15.89 | NaN | NaN | NaN | . We now export the data as csv. I use save_csv_version() to automatically add a version number to the file name to prevent overwriting the data after making changes. . save_csv_version(&#39;../data/prepared/orders.csv&#39;, orders, index = False, compression = &#39;gzip&#39;) save_csv_version(&#39;../data/prepared/items.csv&#39;, items, index = False, compression = &#39;gzip&#39;) . Saved as ../data/prepared/orders_v2.csv Saved as ../data/prepared/items_v2.csv . 3. Aggregation and feature engineering . Data aggregation . Let&#39;s work with orders, which provides a list of transactions with timestamps. . We need to aggregate this data for future modeling. Since the task is a 14-day demand forecasting, a simple way would be to aggregate transactions on a two-week basis. However, this could lead to losing some more granular information. We aggregate transactions by day: . #collapse-show orders[&#39;day_of_year&#39;] = orders[&#39;time&#39;].dt.dayofyear orders_price = orders.groupby([&#39;itemID&#39;, &#39;day_of_year&#39;])[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index() orders = orders.groupby([&#39;itemID&#39;, &#39;day_of_year&#39;])[&#39;order&#39;].agg(&#39;sum&#39;).reset_index() orders.head(3) . . itemID day_of_year order . 0 | 1 | 23 | 1 | . 1 | 1 | 25 | 1 | . 2 | 1 | 29 | 307 | . Adding missing item-day combinations . The aggregated data only contains entries for day-item pairs for which there is at least one transaction. This results in missing information: . most items are only sold on a few days; no data on days with no orders is recorded | there are a few items that are never sold and therefore do not appear in orders | . To account for the missing data, we add entries with order = 0 for missing day-item combinations. This increases the number of observations from 100,771 to 1,883,340 and provides useful information about zero sales. . #collapse-show # add items that were never sold before missing_itemIDs = set(items[&#39;itemID&#39;].unique()) - set(orders[&#39;itemID&#39;].unique()) missing_rows = pd.DataFrame({&#39;itemID&#39;: list(missing_itemIDs), &#39;day_of_year&#39;: np.ones(len(missing_itemIDs)).astype(&#39;int&#39;), &#39;order&#39;: np.zeros(len(missing_itemIDs)).astype(&#39;int&#39;)}) orders = pd.concat([orders, missing_rows], axis = 0) print(orders.shape) # add zeros for days with no transactions agg_orders = orders.groupby([&#39;itemID&#39;, &#39;day_of_year&#39;]).order.unique().unstack(&#39;day_of_year&#39;).stack(&#39;day_of_year&#39;, dropna = False) agg_orders = agg_orders.reset_index() agg_orders.columns = [&#39;itemID&#39;, &#39;day_of_year&#39;, &#39;order&#39;] agg_orders[&#39;order&#39;].fillna(0, inplace = True) agg_orders[&#39;order&#39;] = agg_orders[&#39;order&#39;].astype(int) print(agg_orders.shape) . . (100771, 3) (1883340, 3) . Labeling promotions . The data documentation says that promotions in the training data are not explicitly marked. . We need to manually mark promotion days. Ignoring it complicates forecasting because the number of orders in some days explodes without an apparent reason. In such cases, the underlying reason is likely a promotion, which should be reflected in a corresponding feature. . We need to be very careful about marking promotions. Labeling too many days as promotions based on the number of orders risks introducing data leakage since the number of orders is unknown at the prediction time. Below, I use find_peaks() to isolate peaks in the order time series and encode them as promotions: . #collapse-show # computations agg_orders[&#39;promotion&#39;] = 0 for itemID in tqdm(agg_orders[&#39;itemID&#39;].unique()): promo = np.zeros(len(agg_orders[agg_orders[&#39;itemID&#39;] == itemID])) avg = agg_orders[(agg_orders[&#39;itemID&#39;] == itemID)][&#39;order&#39;].median() std = agg_orders[(agg_orders[&#39;itemID&#39;] == itemID)][&#39;order&#39;].std() peaks, _ = find_peaks(np.append(agg_orders[agg_orders[&#39;itemID&#39;] == itemID][&#39;order&#39;].values, avg), # append avg to enable marking last point as promo prominence = max(5, std), # peak difference with neighbor points; max(5,std) to exclude cases when std is too small height = avg + 2*std) # minimal height of a peak promo[peaks] = 1 agg_orders.loc[agg_orders[&#39;itemID&#39;] == itemID, &#39;promotion&#39;] = promo # compare promotion number promo_in_train = (agg_orders[&#39;promotion&#39;].sum() / agg_orders[&#39;day_of_year&#39;].max()) / len(items) promo_in_test = (3*len(items) - items.promotion_0.isnull().sum() - items.promotion_2.isnull().sum() - items.promotion_1.isnull().sum()) / 14 / len(items) print(&#39;Daily p(promotion) per item in train: {}&#39;.format(np.round(promo_in_train, 4))) print(&#39;Daily p(promotion) per item in test: {}&#39;.format(np.round(promo_in_test , 4))) . . Daily p(promotion) per item in train: 0.0079 Daily p(promotion) per item in test: 0.0141 . Our method identifies 14,911 promotions. Compared to the test set where promotions are explicitly reported, this amounts to about half as many promos per item and day. . Let&#39;s look visualize promotions for some items: . #collapse-hide # compute promo count promo_count = agg_orders.groupby(&#39;itemID&#39;)[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index() promo_count = promo_count.sort_values(&#39;promotion&#39;).reset_index(drop = True) # plot some items item_plots = [0, 2000, 4000, 6000, 8000, 9000, 10000, 10100, 10200, 10300, 10400, 10462] fig = plt.figure(figsize = (16, 12)) for i in range(len(item_plots)): plt.subplot(3, 4, i + 1) df = agg_orders[agg_orders.itemID == promo_count[&#39;itemID&#39;][item_plots[i]]] plt.scatter(df[&#39;day_of_year&#39;], df[&#39;order&#39;], c = df[&#39;promotion&#39;]) plt.ylabel(&#39;Total Orders&#39;) plt.xlabel(&#39;Day&#39;) . . . The yellow marker indicates promotions. Our method identifies some outliers as promos but misses a few points that are less prominent. At the same time, we can not be sure that these cases are necessarily promotions: the large number of orders on these days could be observed due to other reasons. We will stick to this solution but note that it might require further improvement. . Feature engineering . Now that the data is aggregated, we construct transaction-based features and the targets. For each day, we compute target as the total number of orders in the following 14 days. The days preceding the considered day are used to extract features. We extract slices of the past [1, 7, ..., 35] days and compute features based on data from that slice. . For each item, we compute the following features: . total count of orders and ordered items | total count of promotions | mean item price | recency of the last order | . The number of orders and promotions is also aggregated on a manufacturer and category level. . In addition, we use tsfresh package to automatically extract features based on the order dynamics in the last 35 days. tsfresh computes hundreds of features describing the time series. We only keep features with no missing values. . #collapse-show # packages from tsfresh import extract_features # parameters days_input = [1, 7, 14, 21, 28, 35] days_target = 14 # preparations day_first = np.max(days_input) day_last = agg_orders[&#39;day_of_year&#39;].max() - days_target + 1 orders = None # merge manufacturer and category agg_orders = agg_orders.merge(items[[&#39;itemID&#39;, &#39;manufacturer&#39;]], how = &#39;left&#39;) agg_orders = agg_orders.merge(items[[&#39;itemID&#39;, &#39;category&#39;]], how = &#39;left&#39;) # computations for day_of_year in tqdm(list(range(149, day_last)) + [agg_orders[&#39;day_of_year&#39;].max()]): ### VALIDAION: TARGET, PROMOTIONS, PRICES # day intervals target_day_min = day_of_year + 1 target_day_max = day_of_year + days_target # compute target and promo: labeled data if day_of_year &lt; agg_orders[&#39;day_of_year&#39;].max(): # target and future promo tmp_df = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;itemID&#39;)[&#39;order&#39;, &#39;promotion&#39;].agg(&#39;sum&#39;).reset_index() tmp_df.columns = [&#39;itemID&#39;, &#39;target&#39;, &#39;promo_in_test&#39;] # future price tmp_df[&#39;mean_price_test&#39;] = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;itemID&#39;)[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index()[&#39;salesPrice&#39;] # merge manufacturer and category tmp_df = tmp_df.merge(items[[&#39;itemID&#39;, &#39;manufacturer&#39;, &#39;category&#39;]], how = &#39;left&#39;, on = &#39;itemID&#39;) # future price per manufacturer tmp_df_manufacturer = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;manufacturer&#39;)[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;mean_price_test_manufacturer&#39;] tmp_df = tmp_df.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # future price per category tmp_df_category = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;category&#39;)[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;mean_price_test_category&#39;] tmp_df = tmp_df.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) # future promo per manufacturer tmp_df_manufacturer = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;manufacturer&#39;)[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;promo_in_test_manufacturer&#39;] tmp_df = tmp_df.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # future promo per category tmp_df_category = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= target_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= target_day_max) ].groupby(&#39;category&#39;)[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;promo_in_test_category&#39;] tmp_df = tmp_df.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) # compute target and promo: unlabeled data else: # placeholders tmp_df = pd.DataFrame({&#39;itemID&#39;: items.itemID, &#39;target&#39;: np.nan, &#39;promo_in_test&#39;: np.nan, &#39;mean_price_test&#39;: items.simulationPrice, &#39;manufacturer&#39;: items.manufacturer, &#39;category&#39;: items.category, &#39;promo_in_test_manufacturer&#39;: np.nan, &#39;promo_in_test_category&#39;: np.nan}) ### TRAINING: LAG-BASED FEATURES # compute features for day_input in days_input: # day intervals input_day_min = day_of_year - day_input + 1 input_day_max = day_of_year # frequency, promo and price tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max) ].groupby(&#39;itemID&#39;) tmp_df[&#39;order_sum_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(&#39;sum&#39;).reset_index()[&#39;order&#39;] tmp_df[&#39;order_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(lambda x: len(x[x &gt; 0])).reset_index()[&#39;order&#39;] tmp_df[&#39;promo_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index()[&#39;promotion&#39;] tmp_df[&#39;mean_price_last_&#39; + str(day_input)] = tmp_df_input[&#39;salesPrice&#39;].agg(&#39;mean&#39;).reset_index()[&#39;salesPrice&#39;] # frequency, promo per manufacturer tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max) ].groupby(&#39;manufacturer&#39;) tmp_df_manufacturer = tmp_df_input[&#39;order&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;order_manufacturer_sum_last_&#39; + str(day_input)] tmp_df_manufacturer[&#39;order_manufacturer_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(lambda x: len(x[x &gt; 0])).reset_index()[&#39;order&#39;] tmp_df_manufacturer[&#39;promo_manufacturer_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index()[&#39;promotion&#39;] tmp_df = tmp_df.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # frequency, promo per category tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max) ].groupby(&#39;category&#39;) tmp_df_category = tmp_df_input[&#39;order&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;order_category_sum_last_&#39; + str(day_input)] tmp_df_category[&#39;order_category_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(lambda x: len(x[x &gt; 0])).reset_index()[&#39;order&#39;] tmp_df_category[&#39;promo_category_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;promotion&#39;].agg(&#39;sum&#39;).reset_index()[&#39;promotion&#39;] tmp_df = tmp_df.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) # frequency, promo per all items tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max)] tmp_df[&#39;order_all_sum_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(&#39;sum&#39;) tmp_df[&#39;order_all_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;order&#39;].agg(lambda x: len(x[x &gt; 0])) tmp_df[&#39;promo_all_count_last_&#39; + str(day_input)] = tmp_df_input[&#39;promotion&#39;].agg(&#39;sum&#39;) # recency if day_input == max(days_input): tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max) &amp; (agg_orders[&#39;order&#39;] &gt; 0) ].groupby(&#39;itemID&#39;) tmp_df[&#39;days_since_last_order&#39;] = (day_of_year - tmp_df_input[&#39;day_of_year&#39;].agg(&#39;max&#39;)).reindex(tmp_df.itemID).reset_index()[&#39;day_of_year&#39;] tmp_df[&#39;days_since_last_order&#39;].fillna(day_input, inplace = True) # tsfresh features if day_input == max(days_input): tmp_df_input = agg_orders[(agg_orders[&#39;day_of_year&#39;] &gt;= input_day_min) &amp; (agg_orders[&#39;day_of_year&#39;] &lt;= input_day_max)] tmp_df_input = tmp_df_input[[&#39;day_of_year&#39;, &#39;itemID&#39;, &#39;order&#39;]] extracted_features = extract_features(tmp_df_input, column_id = &#39;itemID&#39;, column_sort = &#39;day_of_year&#39;) extracted_features[&#39;itemID&#39;] = extracted_features.index tmp_df = tmp_df.merge(extracted_features, how = &#39;left&#39;, on = &#39;itemID&#39;) ### FINAL PREPARATIONS # day of year tmp_df.insert(1, column = &#39;day_of_year&#39;, value = day_of_year) # merge data orders = pd.concat([orders, tmp_df], axis = 0) # drop manufacturer and category del orders[&#39;manufacturer&#39;] del orders[&#39;category&#39;] ##### REMOVE MISSINGS good_nas = [&#39;target&#39;, &#39;mean_price_test_category&#39;, &#39;mean_price_test_manufacturer&#39;, &#39;promo_in_test&#39;, &#39;promo_in_test_category&#39;, &#39;promo_in_test_manufacturer&#39;] nonas = list(orders.columns[orders.isnull().sum() == 0]) + good_nas orders = orders[nonas] print(orders.shape) ##### COMPUTE MEAN PRICE RATIOS print(orders.shape) price_vars = [&#39;mean_price_last_1&#39;, &#39;mean_price_last_7&#39;, &#39;mean_price_last_14&#39;, &#39;mean_price_last_21&#39;, &#39;mean_price_last_28&#39;, &#39;mean_price_last_35&#39;] for var in price_vars: orders[&#39;ratio_&#39; + str(var)] = orders[&#39;mean_price_test&#39;] / orders[var] orders[&#39;ratio_manufacturer_&#39; + str(var)] = orders[&#39;mean_price_test_manufacturer&#39;] / orders[var] orders[&#39;ratio_category_&#39; + str(var)] = orders[&#39;mean_price_test_category&#39;] / orders[var] print(orders.shape) . . (1391579, 458) (1391579, 470) . The feature extraction takes about ten hours and outputs a data set with 470 features. Great job! . Now, let&#39;s create features in the items data set: . ratio of the actual and recommended price | item category index constructed of three subcategories | customer rating relative to the average rating of the items of the same manufacturer or category | . #collapse-show # price ratio items[&#39;recommended_simulation_price_ratio&#39;] = items[&#39;simulationPrice&#39;] / items[&#39;recommendedRetailPrice&#39;] # detailed item category items[&#39;category&#39;] = items[&#39;category1&#39;].astype(str) + items[&#39;category2&#39;].astype(str) + items[&#39;category3&#39;].astype(str) items[&#39;category&#39;] = items[&#39;category&#39;].astype(int) # customer rating ratio per manufacturer rating_manufacturer = items.groupby(&#39;manufacturer&#39;)[&#39;customerRating&#39;].agg(&#39;mean&#39;).reset_index() rating_manufacturer.columns = [&#39;manufacturer&#39;, &#39;mean_customerRating_manufacturer&#39;] items = items.merge(rating_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) items[&#39;customerRating_manufacturer_ratio&#39;] = items[&#39;customerRating&#39;] / items[&#39;mean_customerRating_manufacturer&#39;] del items[&#39;mean_customerRating_manufacturer&#39;] # customer rating ratio per category rating_category = items.groupby(&#39;category&#39;)[&#39;customerRating&#39;].agg(&#39;mean&#39;).reset_index() rating_category.columns = [&#39;category&#39;, &#39;mean_customerRating_category&#39;] items = items.merge(rating_category, how = &#39;left&#39;, on = &#39;category&#39;) items[&#39;customerRating_category_ratio&#39;] = items[&#39;customerRating&#39;] / items[&#39;mean_customerRating_category&#39;] del items[&#39;mean_customerRating_category&#39;] . . We can now merge orders and items. We also partition the data into the labeled training set and the unlabeled test set, compute some missing features for the test set and export the data as csv. . #collapse-hide ##### DATA PARTITIONING # merge data df = pd.merge(orders, items, on = &#39;itemID&#39;, how = &#39;left&#39;) # partition intro train and test df_train = df[df[&#39;day_of_year&#39;] &lt; df[&#39;day_of_year&#39;].max()] df_test = df[df[&#39;day_of_year&#39;] == df[&#39;day_of_year&#39;].max()] ##### COMPUTE FEATURES FOR TEST DATA # add promotion info to test promo_vars = df_test.filter(like = &#39;promotion_&#39;).columns df_test[&#39;promo_in_test&#39;] = 3 - df_test[promo_vars].isnull().sum(axis = 1) df_test[&#39;promo_in_test&#39;].describe() del df_test[&#39;promo_in_test_manufacturer&#39;], df_test[&#39;promo_in_test_category&#39;] # future promo per manufacturer tmp_df_manufacturer = df_test.groupby(&#39;manufacturer&#39;)[&#39;promo_in_test&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;promo_in_test_manufacturer&#39;] df_test = df_test.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # future promo per category tmp_df_category = df_test.groupby(&#39;category&#39;)[&#39;promo_in_test&#39;].agg(&#39;sum&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;promo_in_test_category&#39;] df_test = df_test.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) del df_test[&#39;mean_price_test_manufacturer&#39;], df_test[&#39;mean_price_test_category&#39;] # future price per manufacturer tmp_df_manufacturer = df_test.groupby(&#39;manufacturer&#39;)[&#39;mean_price_test&#39;].agg(&#39;mean&#39;).reset_index() tmp_df_manufacturer.columns = [&#39;manufacturer&#39;, &#39;mean_price_test_manufacturer&#39;] df_test = df_test.merge(tmp_df_manufacturer, how = &#39;left&#39;, on = &#39;manufacturer&#39;) # future price per category tmp_df_category = df_test.groupby(&#39;category&#39;)[&#39;mean_price_test&#39;].agg(&#39;mean&#39;).reset_index() tmp_df_category.columns = [&#39;category&#39;, &#39;mean_price_test_category&#39;] df_test = df_test.merge(tmp_df_category, how = &#39;left&#39;, on = &#39;category&#39;) # mean price ratios for var in price_vars: df_test[&#39;ratio_&#39; + str(var)] = df_test[&#39;mean_price_test&#39;] / df_test[var] df_test[&#39;ratio_manufacturer_&#39; + str(var)] = df_test[&#39;mean_price_test_manufacturer&#39;] / df_test[var] df_test[&#39;ratio_category_&#39; + str(var)] = df_test[&#39;mean_price_test_category&#39;] / df_test[var] ##### DROP FEATURES # drop promotion dates df_test.drop(promo_vars, axis = 1, inplace = True) df_train.drop(promo_vars, axis = 1, inplace = True) # drop mean prices price_vars = price_vars + [&#39;mean_price_test_manufacturer&#39;, &#39;mean_price_test_category&#39;] df_test.drop(price_vars, axis = 1, inplace = True) df_train.drop(price_vars, axis = 1, inplace = True) # export data save_csv_version(&#39;../data/prepared/df.csv&#39;, df_train, index = False, compression = &#39;gzip&#39;) save_csv_version(&#39;../data/prepared/df_test.csv&#39;, df_test, index = False, compression = &#39;gzip&#39;, min_version = 3) print(df_train.shape) print(df_test.shape) . . Saved as ../data/prepared/df_v14.csv Saved as ../data/prepared/df_test_v14.csv (1381116, 476) (10463, 476) . 4. Modeling . Custom loss functions . Machine learning encompasses a wide range of statically-inspired performance metrics such as MSE, MAE and others. In practice, machine learning models are used by a company that has specific goals. Usually, these goals can not be expressed in terms of such simple metrics. Therefore, it is important to come up with an evaluation metric consistent with the company&#39;s objectives to ensure that we judge performance on a criterion that actually matters. . In the DMC 2020 task, we are given a profit function of the retailer. The function accounts for asymmetric error costs: underpredicting demand results in lost revenue because the retailer can not sell a product that is not ready to ship, whereas overpredicting demand incurs a fee for storing the excessive amount of product. . Below, we derive profit according to the task description: . . Let&#39;s implement the profit function in Python: . def profit(y_true, y_pred, price): &#39;&#39;&#39; Computes retailer&#39;s profit. Arguments: - y_true (numpy array): ground truth demand values - y_pred (numpy array): predicted demand values - price (numpy array): item prices Returns: - profit value &#39;&#39;&#39; # remove negative and round y_pred = np.where(y_pred &gt; 0, y_pred, 0) y_pred = np.round(y_pred).astype(&#39;int&#39;) # sold units units_sold = np.minimum(y_true, y_pred) # overstocked units units_overstock = y_pred - y_true units_overstock[units_overstock &lt; 0] = 0 # profit revenue = units_sold * price fee = units_overstock * price * 0.6 profit = revenue - fee profit = profit.sum() return profit . The function above is great for evaluating quality of our predictions. But can we directly optimize it during modeling? . LightGBM supports custom loss functions on both training and validation stages. To use a custom loss during training, one needs to supply a function with its first and second-order derivatives. . Ideally, we would like to define the loss as a difference between the potential profit (given our demand prediction) and the oracle profit (when demand prediction is correct). However, such a loss is not differentiable. We can not compute derivatives to plug it as a training loss. Instead, we need to come up with a slightly different function that approximates profit and satisfies the loss conditions. . We define the loss as a squared difference between the oracle profit and profit based on predicted demand. In this setting, we can compute loss derivatives with respect to the prediction (Gradient and Hessian): . . The snippet below implements the training and validation losses for LightGBM. You can notice that we do not include the squared prices in the loss functions. The reason is that with sklearn API, it is difficult to include external variables like prices in the loss, so we will include the price later. . # collpase-show ##### TRAINING LOSS def asymmetric_mse(y_true, y_pred): &#39;&#39;&#39; Asymmetric MSE objective for training LightGBM regressor. Arguments: - y_true (numpy array): ground truth target values - y_pred (numpy array): estimated target values Returns: - gradient - hessian &#39;&#39;&#39; residual = (y_true - y_pred).astype(&#39;float&#39;) grad = np.where(residual &gt; 0, -2*residual, -0.72*residual) hess = np.where(residual &gt; 0, 2.0, 0.72) return grad, hess ##### VALIDATION LOSS def asymmetric_mse_eval(y_true, y_pred): &#39;&#39;&#39; Asymmetric MSE evaluation metric for evaluating LightGBM regressor. Arguments: - y_true (numpy array): ground truth target values - y_pred (numpy array): estimated target values Returns: - metric name - metric value - whether the metric is maximized &#39;&#39;&#39; residual = (y_true - y_pred).astype(&#39;float&#39;) loss = np.where(residual &gt; 0, 2*residual**2, 0.72*residual**2) return &#39;asymmetric_mse_eval&#39;, np.mean(loss), False . How to deal with the prices? . One option is to account for them within the fit() method. LightGBM supports weighing observations using the arguments sample_weight and eval_sample_weight. You will see how we supply price vectors in the modeling code in the next section. Note that including prices as weights instead of plugging them into the loss leads to losing some information, since Gradients and Hessians are computed without the price multiplication. Still, this approach provides a pretty close approximation of the original profit function. If you are interested in including prices in the loss, you can check lightGBM API that allows more flexibility. . The only missing piece is the relationship between the penalty size and the prediction error. By taking a square root of the profit differences instead of the absolute value, we penalize larger errors more than the smaller ones. However, our profit changes linearly with the error size. This is how we can can address it: . transform target using a non-linear transformation (e.g. square root) | train a model that optimizes the MSE loss on the transformed target | apply the inverse transformation to the model predictions | . Target transformation smooths out the square effect in MSE. We still penalize large errors more, but the large errors on a transformed scale are also smaller compared to the original scale. This helps to balance the two effects and approximate a linear relationship between the error size and the loss penalty. . Modeling pipeline . Good, let&#39;s start building models! First, we extract the target and flag ID features not used for prediction. . #collapse-hide # extract target y = df_train[&#39;target&#39;] X = df_train.drop(&#39;target&#39;, axis = 1) del df_train print(X.shape, y.shape) # format test data X_test = df_test.drop(&#39;target&#39;, axis = 1) del df_test print(X_test.shape) # relevant features drop_feats = [&#39;itemID&#39;, &#39;day_of_year&#39;] features = [var for var in X.columns if var not in drop_feats] . . (1381116, 475) (1381116,) (10463, 475) . The modeling pipeline uses multiple tricks discovered during the model refinement process. We toggle these tricks using logical variables that define the following training options: . target_transform = True: transforms target to reduce penalty for large errors. Motivation for this is provided in the previous section. | train_on_positive = False: trains only on cases with positive sales (i.e., at least one of the order lags is greater than zero) and predicts null demand for items with no sales. This substantially reduces the training time but also leads to a drop in the performance. | two_stage = True: trains a two-stage model: (i) binary classifier predicting whether the future sales will be zero; (ii) regression model predicting the volume of sales. Predictions of the regression model are only stored for cases where the classifier predicts positive sales. | tuned_params = True: imports optimized LightGBM hyper-parameter values. The next section describes the tuning procedure. | . #collapse-hide ##### TRAINING OPTIONS # target transformation target_transform = True # train on positive sales only train_on_positive = False # two-stage model two_stage = True # use tuned meta-params tuned_params = True ##### CLASSIFIER PARAMETERS # rounds and options cores = 4 stop_rounds = 100 verbose = 100 seed = 23 # LGB parameters lgb_params = { &#39;boosting_type&#39;: &#39;goss&#39;, &#39;objective&#39;: asymmetric_mse, &#39;metrics&#39;: asymmetric_mse_eval, &#39;n_estimators&#39;: 1000, &#39;learning_rate&#39;: 0.1, &#39;bagging_fraction&#39;: 0.8, &#39;feature_fraction&#39;: 0.8, &#39;lambda_l1&#39;: 0.1, &#39;lambda_l2&#39;: 0.1, &#39;silent&#39;: True, &#39;verbosity&#39;: -1, &#39;nthread&#39; : cores, &#39;random_state&#39;: seed, } # load optimal parameters if tuned_params: par_file = open(&#39;../lgb_meta_params_100.pkl&#39;, &#39;rb&#39;) lgb_params = pickle.load(par_file) lgb_params[&#39;nthread&#39;] = cores lgb_params[&#39;random_state&#39;] = seed # second-stage LGB if two_stage: lgb_classifier_params = lgb_params.copy() lgb_classifier_params[&#39;objective&#39;] = &#39;binary&#39; lgb_classifier_params[&#39;metrics&#39;] = &#39;logloss&#39; . . We also define the partitioning parameters. We use a sliding window approach with 7 folds, where each subsequent fold is shifted by one day into the past. . #collapse-hide num_folds = 7 # no. CV folds test_days = 14 # no. days in the test set . . Let&#39;s explain the partitioning using the first fold as an example. Each fold is divided into training and validation subsets. The first 35 days are cut off and only used to compute lag-based features for the days starting from 36. Days 36 - 145 are used for training. For each of these days, we have features based on the previous 35 days and targets based on the next 14 days. Days 159 - 173 are used for validation. Days 146 - 158 between training and validation subsets are skipped to avoid data leakage: the target for these days would use information from the validation period. . . We can now set up a modeling loop with the following steps for each of the folds: . extract data from the fold and partition it into training and validation sets | train LightGBM on the training set and perform early stopping on the validation set | save predictions for the validation set (denoted as OOF) and predictions for the test set | save feature importance and performance on the validation fold | . #collapse-show # placeholders importances = pd.DataFrame() preds_oof = np.zeros((num_folds, items.shape[0])) reals_oof = np.zeros((num_folds, items.shape[0])) prices_oof = np.zeros((num_folds, items.shape[0])) preds_test = np.zeros(items.shape[0]) oof_rmse = [] oof_profit = [] oracle_profit = [] clfs = [] train_idx = [] valid_idx = [] # objects train_days = X[&#39;day_of_year&#39;].max() - test_days + 1 - num_folds - X[&#39;day_of_year&#39;].min() # no. days in the train set time_start = time.time() # modeling loop for fold in range(num_folds): ##### PARTITIONING # dates if fold == 0: v_end = X[&#39;day_of_year&#39;].max() else: v_end = v_end - 1 v_start = v_end t_end = v_start - (test_days + 1) t_start = t_end - (train_days - 1) # extract index train_idx.append(list(X[(X.day_of_year &gt;= t_start) &amp; (X.day_of_year &lt;= t_end)].index)) valid_idx.append(list(X[(X.day_of_year &gt;= v_start) &amp; (X.day_of_year &lt;= v_end)].index)) # extract samples X_train, y_train = X.iloc[train_idx[fold]][features], y.iloc[train_idx[fold]] X_valid, y_valid = X.iloc[valid_idx[fold]][features], y.iloc[valid_idx[fold]] X_test = X_test[features] # keep positive cases if train_on_positive: y_train = y_train.loc[(X_train[&#39;order_sum_last_28&#39;] &gt; 0) | (X_train[&#39;promo_in_test&#39;] &gt; 0)] X_train = X_train.loc[(X_train[&#39;order_sum_last_28&#39;] &gt; 0) | (X_train[&#39;promo_in_test&#39;] &gt; 0)] # information print(&#39;-&#39; * 65) print(&#39;- train period days: {} -- {} (n = {})&#39;.format(t_start, t_end, len(train_idx[fold]))) print(&#39;- valid period days: {} -- {} (n = {})&#39;.format(v_start, v_end, len(valid_idx[fold]))) print(&#39;-&#39; * 65) ##### MODELING # target transformation if target_transform: y_train = np.sqrt(y_train) y_valid = np.sqrt(y_valid) # first stage model if two_stage: y_train_binary, y_valid_binary = y_train.copy(), y_valid.copy() y_train_binary[y_train_binary &gt; 0] = 1 y_valid_binary[y_valid_binary &gt; 0] = 1 clf_classifier = lgb.LGBMClassifier(**lgb_classifier_params) clf_classifier = clf_classifier.fit(X_train, y_train_binary, eval_set = [(X_train, y_train_binary), (X_valid, y_valid_binary)], eval_metric = &#39;logloss&#39;, early_stopping_rounds = stop_rounds, verbose = verbose) preds_oof_fold_binary = clf_classifier.predict(X_valid) preds_test_fold_binary = clf_classifier.predict(X_test) # training clf = lgb.LGBMRegressor(**lgb_params) clf = clf.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_valid, y_valid)], eval_metric = asymmetric_mse_eval, sample_weight = X_train[&#39;simulationPrice&#39;].values, eval_sample_weight = [X_train[&#39;simulationPrice&#39;].values, X_valid[&#39;simulationPrice&#39;].values], early_stopping_rounds = stop_rounds, verbose = verbose) clfs.append(clf) # inference if target_transform: preds_oof_fold = postprocess_preds(clf.predict(X_valid)**2) reals_oof_fold = y_valid**2 preds_test_fold = postprocess_preds(clf.predict(X_test)**2) / num_folds else: preds_oof_fold = postprocess_preds(clf.predict(X_valid)) reals_oof_fold = y_valid preds_test_fold = postprocess_preds(clf.predict(X_test)) / num_folds # impute zeros if train_on_positive: preds_oof_fold[(X_valid[&#39;order_sum_last_28&#39;] == 0) &amp; (X_valid[&#39;promo_in_test&#39;] == 0)] = 0 preds_test_fold[(X_test[&#39;order_sum_last_28&#39;] == 0) &amp; (X_test[&#39;promo_in_test&#39;] == 0)] = 0 # multiply with first stage predictions if two_stage: preds_oof_fold = preds_oof_fold * np.round(preds_oof_fold_binary) preds_test_fold = preds_test_fold * np.round(preds_test_fold_binary) # write predictions preds_oof[fold, :] = preds_oof_fold reals_oof[fold, :] = reals_oof_fold preds_test += preds_test_fold # save prices prices_oof[fold, :] = X.iloc[valid_idx[fold]][&#39;simulationPrice&#39;].values ##### EVALUATION # evaluation oof_rmse.append(np.sqrt(mean_squared_error(reals_oof[fold, :], preds_oof[fold, :]))) oof_profit.append(profit(reals_oof[fold, :], preds_oof[fold, :], price = X.iloc[valid_idx[fold]][&#39;simulationPrice&#39;].values)) oracle_profit.append(profit(reals_oof[fold, :], reals_oof[fold, :], price = X.iloc[valid_idx[fold]][&#39;simulationPrice&#39;].values)) # feature importance fold_importance_df = pd.DataFrame() fold_importance_df[&#39;Feature&#39;] = features fold_importance_df[&#39;Importance&#39;] = clf.feature_importances_ fold_importance_df[&#39;Fold&#39;] = fold + 1 importances = pd.concat([importances, fold_importance_df], axis = 0) # information print(&#39;-&#39; * 65) print(&#39;FOLD {:d}/{:d}: RMSE = {:.2f}, PROFIT = {:.0f}&#39;.format(fold + 1, num_folds, oof_rmse[fold], oof_profit[fold])) print(&#39;-&#39; * 65) print(&#39;&#39;) # print performance print(&#39;&#39;) print(&#39;-&#39; * 65) print(&#39;- AVERAGE RMSE: {:.2f}&#39;.format(np.mean(oof_rmse))) print(&#39;- AVERAGE PROFIT: {:.0f} ({:.2f}%)&#39;.format(np.mean(oof_profit), 100 * np.mean(oof_profit) / np.mean(oracle_profit))) print(&#39;- RUNNING TIME: {:.2f} minutes&#39;.format((time.time() - time_start) / 60)) print(&#39;-&#39; * 65) . . -- - train period days: 41 -- 151 (n = 1161393) - valid period days: 166 -- 166 (n = 10463) -- Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [532] training&#39;s binary_logloss: 0.238417 valid_1&#39;s binary_logloss: 0.347182 Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [788] training&#39;s rmse: 0.611924 training&#39;s asymmetric_mse_eval: 2.82734 valid_1&#39;s rmse: 0.98004 valid_1&#39;s asymmetric_mse_eval: 5.83945 -- FOLD 1/7: RMSE = 74.46, PROFIT = 4146664 -- ... -- - train period days: 35 -- 145 (n = 1161393) - valid period days: 160 -- 160 (n = 10463) -- Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [711] training&#39;s binary_logloss: 0.22193 valid_1&#39;s binary_logloss: 0.338637 Training until validation scores don&#39;t improve for 100 rounds Early stopping, best iteration is: [782] training&#39;s rmse: 0.600011 training&#39;s asymmetric_mse_eval: 2.71385 valid_1&#39;s rmse: 0.987073 valid_1&#39;s asymmetric_mse_eval: 5.67223 -- FOLD 7/7: RMSE = 74.46, PROFIT = 3958098 -- -- - AVERAGE RMSE: 69.66 - AVERAGE PROFIT: 3997598 (54.19%) - RUNNING TIME: 88.53 minutes -- . Looks good! The modeling pipeline took us about 1.5 hours to run. . Forecasting demand with our models results in 3,997,598 daily profit, which is about 54% of the maximum possible profit. Let&#39;s visualize the results: . # collapse-hide fig = plt.figure(figsize = (20, 7)) # residual plot plt.subplot(1, 2, 1) plt.scatter(reals_oof.reshape(-1), preds_oof.reshape(-1)) axis_lim = np.max([reals_oof.max(), preds_oof.max()]) plt.ylim(top = 1.02*axis_lim) plt.xlim(right = 1.02*axis_lim) plt.plot((0, axis_lim), (0, axis_lim), &#39;r--&#39;) plt.title(&#39;Residual Plot&#39;) plt.ylabel(&#39;Predicted demand&#39;) plt.xlabel(&#39;Actual demand&#39;) # feature importance plt.subplot(1, 2, 2) top_feats = 50 cols = importances[[&#39;Feature&#39;, &#39;Importance&#39;]].groupby(&#39;Feature&#39;).mean().sort_values(by = &#39;Importance&#39;, ascending = False)[0:top_feats].index importance = importances.loc[importances.Feature.isin(cols)] sns.barplot(x = &#39;Importance&#39;, y = &#39;Feature&#39;, data = importance.sort_values(by = &#39;Importance&#39;, ascending = False), ci = 0) plt.title(&#39;Feature Importance&#39;) plt.tight_layout() . . . The scatterplot shows that there is a space for further improvement: many predictions are far from the 45-degree line where predicted and real orders are equal. The important features mostly contain price information followed by features that count the previous orders. . We can now use predictions stored in preds_test to create a submission. Mission accomplished! . 5. Hyper-parameter tuning . One way to improve our solution is to optimize the LightGBM hyper-parameters. . We tune hyper-parameters using the hyperopt package, which performs optimization using Tree of Parzen Estimators (TPE) as a search algorithm. You don&#39;t really need to know how TPE works. As a user, you are only required to supply a parameter grid indicating the range of possible values. Compared to standard tuning methods like grid search or random search, TPE explores the search space more efficiently, allowing you to find a suitable solution faster. If you want to read more, see the package documentation here. . So, let&#39;s specify hyper-parameter ranges! We create a dictionary using the following options: . hp.choice(&#39;name&#39;, list_of_values): sets a hyper-parameter to one of the values from a list. This is suitable for hyper-parameters that can have multiple distinct values like boosting_type | hp.uniform(&#39;name&#39;, min, max): sets a hyper-parameter to a float between min and max. This works well with hyper-parameters such as learning_rate | hp.quniform(&#39;name&#39;, min, max, step): sets a hyper-parameter to a value between min and max with a step size of step. This is useful for integer parameters like max_depth | . #collapse-show # training params lgb_reg_params = { &#39;boosting_type&#39;: hp.choice(&#39;boosting_type&#39;, [&#39;gbdt&#39;, &#39;goss&#39;]), &#39;objective&#39;: &#39;rmse&#39;, &#39;metrics&#39;: &#39;rmse&#39;, &#39;n_estimators&#39;: 10000, &#39;learning_rate&#39;: hp.uniform(&#39;learning_rate&#39;, 0.0001, 0.3), &#39;max_depth&#39;: hp.quniform(&#39;max_depth&#39;, 1, 16, 1), &#39;num_leaves&#39;: hp.quniform(&#39;num_leaves&#39;, 10, 64, 1), &#39;bagging_fraction&#39;: hp.uniform(&#39;bagging_fraction&#39;, 0.3, 1), &#39;feature_fraction&#39;: hp.uniform(&#39;feature_fraction&#39;, 0.3, 1), &#39;lambda_l1&#39;: hp.uniform(&#39;lambda_l1&#39;, 0, 1), &#39;lambda_l2&#39;: hp.uniform(&#39;lambda_l2&#39;, 0, 1), &#39;silent&#39;: True, &#39;verbosity&#39;: -1, &#39;nthread&#39; : 4, &#39;random_state&#39;: 77, } # evaluation params lgb_fit_params = { &#39;eval_metric&#39;: &#39;rmse&#39;, &#39;early_stopping_rounds&#39;: 100, &#39;verbose&#39;: False, } # combine params lgb_space = dict() lgb_space[&#39;reg_params&#39;] = lgb_reg_params lgb_space[&#39;fit_params&#39;] = lgb_fit_params . . Next, we create HPOpt object that performs tuning. We can avoid this in a simple tuning task, but defining an object gives us more control of the optimization process, which is useful with a custom loss. We define three object methods: . process: runs optimization. By default, HPO uses fmin() to minimize the specified loss | lgb_reg: initializes LightGBM model | train_reg: trains LightGBM and computes the loss. Since we aim to maximize profit, we simply define loss as negative profit | . # collapse-show class HPOpt(object): # INIT def __init__(self, x_train, x_test, y_train, y_test): self.x_train = x_train self.x_test = x_test self.y_train = y_train self.y_test = y_test # optimization process def process(self, fn_name, space, trials, algo, max_evals): fn = getattr(self, fn_name) try: result = fmin(fn = fn, space = space, algo = algo, max_evals = max_evals, trials = trials) except Exception as e: return {&#39;status&#39;: STATUS_FAIL, &#39;exception&#39;: str(e)} return result, trials # LGBM initialization def lgb_reg(self, para): para[&#39;reg_params&#39;][&#39;max_depth&#39;] = int(para[&#39;reg_params&#39;][&#39;max_depth&#39;]) para[&#39;reg_params&#39;][&#39;num_leaves&#39;] = int(para[&#39;reg_params&#39;][&#39;num_leaves&#39;]) reg = lgb.LGBMRegressor(**para[&#39;reg_params&#39;]) return self.train_reg(reg, para) # training and inference def train_reg(self, reg, para): # fit LGBM reg.fit(self.x_train, self.y_train, eval_set = [(self.x_train, self.y_train), (self.x_test, self.y_test)], sample_weight = self.x_train[&#39;simulationPrice&#39;].values, eval_sample_weight = [self.x_train[&#39;simulationPrice&#39;].values, self.x_test[&#39;simulationPrice&#39;].values], **para[&#39;fit_params&#39;]) # inference if target_transform: preds = postprocess_preds(reg.predict(self.x_test)**2) reals = self.y_test**2 else: preds = postprocess_preds(reg.predict(self.x_test)) reals = self.y_test # compute loss [negative profit] loss = np.round(-profit(reals, preds, price = self.x_test[&#39;simulationPrice&#39;].values)) return {&#39;loss&#39;: loss, &#39;status&#39;: STATUS_OK} . . To prevent overfitting, we perform tuning on a different subset of data compared to the models trained in the previous section by going one day further in the past. . # collapse-hide # validation dates v_end = 158 # 1 day before last validation fold in code_03_modeling v_start = v_end # same as v_start # training dates t_start = 28 # first day in the data t_end = v_start - 15 # validation day - two weeks # extract index train_idx = list(X[(X.day_of_year &gt;= t_start) &amp; (X.day_of_year &lt;= t_end)].index) valid_idx = list(X[(X.day_of_year &gt;= v_start) &amp; (X.day_of_year &lt;= v_end)].index) # extract samples X_train, y_train = X.iloc[train_idx][features], y.iloc[train_idx] X_valid, y_valid = X.iloc[valid_idx][features], y.iloc[valid_idx] # target transformation if target_transform: y_train = np.sqrt(y_train) y_valid = np.sqrt(y_valid) # information print(&#39;-&#39; * 65) print(&#39;- train period days: {} -- {} (n = {})&#39;.format(t_start, t_end, len(train_idx))) print(&#39;- valid period days: {} -- {} (n = {})&#39;.format(v_start, v_end, len(valid_idx))) print(&#39;-&#39; * 65) . . -- - train period days: 28 -- 143 (n = 1213708) - valid period days: 158 -- 158 (n = 10463) -- . Now, we just need to instantiate the HPOpt object and launch the tuning trials! The optimization will run automatically, and we would only need to extract the optimized values: . # collapse-show # instantiate objects hpo_obj = HPOpt(X_train, X_valid, y_train, y_valid) trials = Trials() # perform tuning lgb_opt_params = hpo_obj.process(fn_name = &#39;lgb_reg&#39;, space = lgb_space, trials = trials, algo = tpe.suggest, max_evals = tuning_trials) # merge best params to fixed params params = list(lgb_opt_params[0].keys()) for par_id in range(len(params)): lgb_reg_params[params[par_id]] = lgb_opt_params[0][params[par_id]] # postprocess lgb_reg_params[&#39;boosting_type&#39;] = boost_types[lgb_reg_params[&#39;boosting_type&#39;]] lgb_reg_params[&#39;max_depth&#39;] = int(lgb_reg_params[&#39;max_depth&#39;]) lgb_reg_params[&#39;num_leaves&#39;] = int(lgb_reg_params[&#39;num_leaves&#39;]) # print best params print(&#39;Best meta-parameters:&#39;) lgb_reg_params . . Best meta-parameters: . {&#39;boosting_type&#39;: &#39;goss&#39;, &#39;objective&#39;: &#39;rmse&#39;, &#39;metrics&#39;: &#39;rmse&#39;, &#39;n_estimators&#39;: 10000, &#39;learning_rate&#39;: 0.004012, &#39;max_depth&#39;: 10, &#39;num_leaves&#39;: 64, &#39;bagging_fraction&#39;: 0.934688, &#39;feature_fraction&#39;: 0.668076, &#39;lambda_l1&#39;: 0.280133, &#39;lambda_l2&#39;: 0.589682, &#39;silent&#39;: True, &#39;verbosity&#39;: -1, &#39;nthread&#39;: 4, &#39;random_state&#39;: 77} . Done! Now we can save the optimized values and import them when setting up the model. . # collapse-hide par_file = open(&#39;../lgb_meta_params.pkl&#39;, &#39;wb&#39;) pickle.dump(lgb_reg_params, par_file) par_file.close() . . 6. Closing words . This blog post has finally come to an end. Thank you for reading! . We looked at important stages of our solution and covered steps such as data aggregation, feature engineering, custom loss functions, target transformation and hyper-parameter tuning. . Our final solution was a simple ensemble of multiple LightGBM models with different features and training options discussed in this post. If you are interested in the ensembling part, you can find the codes in my Github. . Please feel free to use the comment window below to ask questions and stay tuned for the next editions of Data Mining Cup! .",
            "url": "https://kozodoi.me/blog/20200727/demand-forecasting",
            "relUrl": "/blog/20200727/demand-forecasting",
            "date": " • Jul 27, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Detecting Blindness with Deep Learning",
            "content": "Last update: 09.06.2022. All opinions are my own. . 1. Overview . Can deep learning help to detect blindness? . This blog post describes a project that develops a convolutional neural network (CNN) for predicting the severity of the diabetic retinopathy based on the patient&#39;s retina photos. The project was completed within the scope of the Udacity ML Engineer nano-degree program and the Kaggle competition hosted by the Asia Pacific Tele-Ophthalmology Society (APTOS). ^ The blog post provides a project walkthrough covering the following steps: . data exploration and image preprocessing to normalize images from different clinics | using transfer learning to pre-train CNN on a larger data set | employing techniques such as learning rate scheduler, test-time augmentation and others | . The modeling is performed in PyTorch. All notebooks and a PDF report are available on Github. . 2. Motivation . Diabetic retinopathy (DR) is one of the leading causes of vision loss. The World Health Organization reports that more than 300 million people worldwide have diabetes (Wong et al. 2016). In 2019, the global prevalence of DR among individuals with diabetes was at more than 25% (Thomas et al. 2019). The prevalence has been rising rapidly in developing countries. . Early detection and treatment are crucial steps towards preventing DR. The screening procedure requires a trained clinical expert to examine the fundus photographs of the patient&#39;s retina. This creates delays in diagnosis and treatment. This is especially relevant for developing countries, which often lack qualified medical staff to perform the diagnosis. Automated detection of DR can speed up the efficiency and coverage of the screening programs. . . Image source: https://www.eyeops.com/contents/our-services/eye-diseases/diabetic-retinopathy . 3. Data preparation . Data preparation is a very important step that is frequently underestimated. The quality of the input data has a strong impact on the resulting performance of the developed machine learning models. Therefore, it is crucial to take some time to look at the data and think about possible issues that should be addressed before moving on to the modeling stage. Let&#39;s do that! . Data exploration . The data set is available for the download at the competition&#39;s website. The data includes 3,662 labeled retina images of clinical patients and a test set with 1,928 images with unknown labels. . The images are labeled by a clinical expert. The integer labels indicate the severity of DR on a scale from 0 to 4, where 0 indicates no disease and 5 is the proliferative stage of DR. . Let&#39;s start by importing the data and looking at the class distribution. . #collapse-hide ##### PACKAGES import numpy as np import pandas as pd import torch import torchvision from torchvision import transforms, datasets from torch.utils.data import Dataset from PIL import Image, ImageFile ImageFile.LOAD_TRUNCATED_IMAGES = True import cv2 from tqdm import tqdm_notebook as tqdm import random import time import sys import os import math import matplotlib.pyplot as plt import seaborn as sns pd.set_option(&#39;display.max_columns&#39;, None) %matplotlib inline import warnings warnings.filterwarnings(&#39;ignore&#39;) ##### CLASS DISTRIBUTION # import data train = pd.read_csv(&#39;../input/aptos2019-blindness-detection/train.csv&#39;) test = pd.read_csv(&#39;../input/aptos2019-blindness-detection/sample_submission.csv&#39;) # plot fig = plt.figure(figsize = (15, 5)) plt.hist(train[&#39;diagnosis&#39;]) plt.title(&#39;Class Distribution&#39;) plt.ylabel(&#39;Number of examples&#39;) plt.xlabel(&#39;Diagnosis&#39;) . . The data is imbalanced: 49% images are from healthy patients. The remaining 51% are different stages of DR. The least common class is 3 (severe stage) with only 5% of the total examples. . The data is collected from multiple clinics using a variety of camera models, which creates discrepancies in the image resolution, aspect ratio and other parameters. This is demonstrated in the snippet below, where we plot histograms of image width, height and aspect ratio. . #collapse-hide # placeholder image_stats = [] # import loop for index, observation in tqdm(train.iterrows(), total = len(train)): # import image img = cv2.imread(&#39;../input/aptos2019-blindness-detection/train_images/{}.png&#39;.format(observation[&#39;id_code&#39;])) # compute stats height, width, channels = img.shape ratio = width / height # save image_stats.append(np.array((observation[&#39;diagnosis&#39;], height, width, channels, ratio))) # construct DF image_stats = pd.DataFrame(image_stats) image_stats.columns = [&#39;diagnosis&#39;, &#39;height&#39;, &#39;width&#39;, &#39;channels&#39;, &#39;ratio&#39;] # create plot fig = plt.figure(figsize = (15, 5)) # width plt.subplot(1, 3, 1) plt.hist(image_stats[&#39;width&#39;]) plt.title(&#39;(a) Image Width&#39;) plt.ylabel(&#39;Number of examples&#39;) plt.xlabel(&#39;Width&#39;) # height plt.subplot(1, 3, 2) plt.hist(image_stats[&#39;height&#39;]) plt.title(&#39;(b) Image Height&#39;) plt.ylabel(&#39;Number of examples&#39;) plt.xlabel(&#39;Height&#39;) # ratio plt.subplot(1, 3, 3) plt.hist(image_stats[&#39;ratio&#39;]) plt.title(&#39;(c) Aspect Ratio&#39;) plt.ylabel(&#39;Number of examples&#39;) plt.xlabel(&#39;Ratio&#39;) . . Now, let&#39;s look into the actual eyes! The code below creates the EyeData dataset class to import images. We also create a DataLoader object to load sample images and visualize the first batch. . #collapse-hide ##### DATASET # image preprocessing def prepare_image(path, image_size = 256): # import image = cv2.imread(path) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # resize image = cv2.resize(image, (int(image_size), int(image_size))) # convert to tensor image = torch.tensor(image) image = image.permute(2, 1, 0) return image # dataset class EyeData(Dataset): # initialize def __init__(self, data, directory, transform = None): self.data = data self.directory = directory self.transform = transform # length def __len__(self): return len(self.data) # get items def __getitem__(self, idx): img_name = os.path.join(self.directory, self.data.loc[idx, &#39;id_code&#39;] + &#39;.png&#39;) image = prepare_image(img_name) image = self.transform(image) label = torch.tensor(self.data.loc[idx, &#39;diagnosis&#39;]) return {&#39;image&#39;: image, &#39;label&#39;: label} ##### EXAMINE SAMPLE BATCH # transformations sample_trans = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), ]) # dataset sample = EyeData(data = train, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = sample_trans) # data loader sample_loader = torch.utils.data.DataLoader(dataset = sample, batch_size = 10, shuffle = False, num_workers = 4) # display images for batch_i, data in enumerate(sample_loader): # extract data inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1, 1) # create plot fig = plt.figure(figsize = (15, 7)) for i in range(len(labels)): ax = fig.add_subplot(2, len(labels)/2, i + 1, xticks = [], yticks = []) plt.imshow(inputs[i].numpy().transpose(1, 2, 0)) ax.set_title(labels.numpy()[i]) break . . . The illustration further emphasizes differences in the aspect ratio and lighting conditions. . The severity of DR is diagnosed by the presence of visual cues such as abnormal blood vessels, hard exudates and so-called cotton wool spots. You can read more about the diagnosing process here. Comparing the sample images, we can see the presence of exudates and cotton wool spots on some of the retina images of sick patients. . Image preprocessing . To simplify the classification task for our model, we need to ensure that retina images look similar. . First, using cameras with different aspect ratios results in some images having large black areas around the eye. The black areas do not contain information relevant for prediction and can be cropped. However, the size of black areas varies from one image to another. To address this, we develop a cropping function that converts the image to grayscale and marks black areas based on the pixel intensity. Next, we find a mask of the image by selecting rows and columns in which all pixels exceed the intensity threshold. This helps to remove vertical or horizontal rectangles filled with black similar to the ones observed in the upper-right image. After removing the black stripes, we resize the images to the same height and width. . Another issue is the eye shape. Depending on the image parameters, some eyes have a circular form, whereas others look like ovals. Since the size and shape of cues located in the retina determine the disease severity, it is crucial to standardize the eye shape as well. To do so, we develop another function that makes a circular crop around the center of the image. . Finally, we correct for the lightning and brightness discrepancies by smoothing the images using a Gaussian filter. . The snippet below provides the updated prepare_image() function that incorporates the discussed preprocessing steps. . #collapse-show ### image preprocessing function def prepare_image(path, sigmaX = 10, do_random_crop = False): # import image image = cv2.imread(path) image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # perform smart crops image = crop_black(image, tol = 7) if do_random_crop == True: image = random_crop(image, size = (0.9, 1)) # resize and color image = cv2.resize(image, (int(image_size), int(image_size))) image = cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0, 0), sigmaX), -4, 128) # circular crop image = circle_crop(image, sigmaX = sigmaX) # convert to tensor image = torch.tensor(image) image = image.permute(2, 1, 0) return image ### automatic crop of black areas def crop_black(img, tol = 7): if img.ndim == 2: mask = img &gt; tol return img[np.ix_(mask.any(1),mask.any(0))] elif img.ndim == 3: gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) mask = gray_img &gt; tol check_shape = img[:,:,0][np.ix_(mask.any(1),mask.any(0))].shape[0] if (check_shape == 0): return img else: img1 = img[:,:,0][np.ix_(mask.any(1),mask.any(0))] img2 = img[:,:,1][np.ix_(mask.any(1),mask.any(0))] img3 = img[:,:,2][np.ix_(mask.any(1),mask.any(0))] img = np.stack([img1, img2, img3], axis = -1) return img ### circular crop around center def circle_crop(img, sigmaX = 10): height, width, depth = img.shape largest_side = np.max((height, width)) img = cv2.resize(img, (largest_side, largest_side)) height, width, depth = img.shape x = int(width / 2) y = int(height / 2) r = np.amin((x,y)) circle_img = np.zeros((height, width), np.uint8) cv2.circle(circle_img, (x,y), int(r), 1, thickness = -1) img = cv2.bitwise_and(img, img, mask = circle_img) return img ### random crop def random_crop(img, size = (0.9, 1)): height, width, depth = img.shape cut = 1 - random.uniform(size[0], size[1]) i = random.randint(0, int(cut * height)) j = random.randint(0, int(cut * width)) h = i + int((1 - cut) * height) w = j + int((1 - cut) * width) img = img[i:h, j:w, :] return img . . Next, we define a new EyeData class that uses the new processing functions and visualize a batch of sample images after corrections. . #collapse-show ##### DATASET # dataset class class EyeData(Dataset): # initialize def __init__(self, data, directory, transform = None): self.data = data self.directory = directory self.transform = transform # length def __len__(self): return len(self.data) # get items def __getitem__(self, idx): img_name = os.path.join(self.directory, self.data.loc[idx, &#39;id_code&#39;] + &#39;.png&#39;) image = prepare_image(img_name) image = self.transform(image) label = torch.tensor(self.data.loc[idx, &#39;diagnosis&#39;]) return {&#39;image&#39;: image, &#39;label&#39;: label} ##### EXAMINE SAMPLE BATCH image_size = 256 # transformations sample_trans = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), ]) # dataset sample = EyeData(data = train, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = sample_trans) # data loader sample_loader = torch.utils.data.DataLoader(dataset = sample, batch_size = 10, shuffle = False, num_workers = 4) # display images for batch_i, data in enumerate(sample_loader): # extract data inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1, 1) # create plot fig = plt.figure(figsize = (15, 7)) for i in range(len(labels)): ax = fig.add_subplot(2, len(labels)/2, i + 1, xticks = [], yticks = []) plt.imshow(inputs[i].numpy().transpose(1, 2, 0)) ax.set_title(labels.numpy()[i]) break . . . This looks much better! Comparing the retina images to the ones before the preprocessing, we can see that the apparent discrepancies between the photos are now fixed. The eyes now have a similar circular shape, and the color scheme is more consistent. This should help the model to detect the signs of the DR. . Check out this notebook by Nakhon Ratchasima for more ideas on the image preprocessing for retina photos. The functions in this project are largely inspired by his work during the competition. . 4. Modeling . CNNs achieve state-of-the-art performance in computer vision tasks. Recent medical research also shows a high potential of CNNs in DR classification (Gulshan et al. 2016). . In this project, we employ a CNN model with the EfficientNet architecture. EfficientNet is one of the recent state-of-the-art image classification models (Tan et al. 2019). It encompasses 8 architecture variants (B0 to B7) that differ in the model complexity and default image size. . The architecture of EfficientNet B0 is visualized below. We test multiple EfficientNet architectures and use the one that demonstrates the best performance. . . The modeling pipeline consists of three stages: . Pre-training. The data set has a limited number of images (N = 3,662). We pre-train the CNN model on a larger data set from the previous Kaggle competition. | Fine-tuning. We fine-tune the model on the target data set. We use cross-validation and make modeling decisions based on the performance of the out-of-fold predictions. | Inference. We aggregate predictions of the models trained on different combinations of training folds and use test-time augmentation to further improve the performance. | . Pre-training . Due to small sample size, we can not train a complex neural architecture from scratch. This is where transfer learning comes in handy. The idea of transfer learning is to pre-train a model on a different data (source domain) and fine-tune it on a relevant data set (target domain). . A good candidate for the source domain is the ImageNet database. Most published CNN models are trained on that data. However, ImageNet images are substantially different from the retina images we want to classify. Although initializing CNN with ImageNet weights might help the network to transfer the knowledge of basic image patterns such as shapes and edges, we still need to learn a lot from the target domain. . It turns out that APTOS had hosted another Kaggle competition on the DR classification in 2015. The data set of the 2015 competition features 35,126 retina images labeled by a clinician using the same scale as the target data set. The data is available for the download here. . This enables us to use following pipeline: . initialize weights from a CNN trained on ImageNet | train the CNN on the 2015 data set | fine-tune the CNN on the 2019 data set | . Let&#39;s start modeling! First, we enable GPU support and fix random seeds. The function seed_everything() sets seed for multiple packages, including numpy and pytorch, to ensure reproducibility. . #collapse-show # GPU check train_on_gpu = torch.cuda.is_available() if not train_on_gpu: print(&#39;CUDA is not available. Training on CPU...&#39;) device = torch.device(&#39;cpu&#39;) else: print(&#39;CUDA is available. Training on GPU...&#39;) device = torch.device(&#39;cuda:0&#39;) # set seed def seed_everything(seed = 23): os.environ[&#39;PYTHONHASHSEED&#39;] = str(seed) random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True torch.backends.cudnn.benchmark = False seed = 23 seed_everything(seed) . . CUDA is available. Training on GPU... . Let&#39;s take a quick look at the class distribution in the 2015 data. . #collapse-show # import data train = pd.read_csv(&#39;../input/diabetic-retinopathy-resized/trainLabels.csv&#39;) train.columns = [&#39;id_code&#39;, &#39;diagnosis&#39;] test = pd.read_csv(&#39;../input/aptos2019-blindness-detection/train.csv&#39;) # check shape print(train.shape, test.shape) print(&#39;-&#39; * 15) print(train[&#39;diagnosis&#39;].value_counts(normalize = True)) print(&#39;-&#39; * 15) print(test[&#39;diagnosis&#39;].value_counts(normalize = True)) . . (35126, 2) (3662, 2) 0 0.734783 2 0.150658 1 0.069550 3 0.024853 4 0.020156 Name: diagnosis, dtype: float64 0 0.492900 2 0.272802 1 0.101038 4 0.080557 3 0.052703 Name: diagnosis, dtype: float64 . The imbalance in the source data is stronger than in the target data: 73% of images represent healthy patients, whereas the most severe stage of the DR is only found in 2% of the images. To address the imbalance, we will use the target data set as a validation sample during training. . We create two Dataset objects to enable different augmentations on training and inference stages: EyeTrainData and EyeTestData. The former includes a random crop that is skipped for the test data. . #collapse-hide # dataset class: train class EyeTrainData(Dataset): # initialize def __init__(self, data, directory, transform = None): self.data = data self.directory = directory self.transform = transform # length def __len__(self): return len(self.data) # get items def __getitem__(self, idx): img_name = os.path.join(self.directory, self.data.loc[idx, &#39;id_code&#39;] + &#39;.jpeg&#39;) image = prepare_image(img_name, do_random_crop = True) image = self.transform(image) label = torch.tensor(self.data.loc[idx, &#39;diagnosis&#39;]) return {&#39;image&#39;: image, &#39;label&#39;: label} # dataset class: test class EyeTestData(Dataset): # initialize def __init__(self, data, directory, transform = None): self.data = data self.directory = directory self.transform = transform # length def __len__(self): return len(self.data) # get items def __getitem__(self, idx): img_name = os.path.join(self.directory, self.data.loc[idx, &#39;id_code&#39;] + &#39;.png&#39;) image = prepare_image(img_name, do_random_crop = False) image = self.transform(image) label = torch.tensor(self.data.loc[idx, &#39;diagnosis&#39;]) return {&#39;image&#39;: image, &#39;label&#39;: label} . . We use a batch size of 20 and set the image size of 256. The choice of these parameters is a trade-off between performance and resource capacity. Feel free to try larger image and batch sizes if you have resources. . We use the following data augmentations during training: . random horizontal flip | random vertical flip | random rotation in the range [-360 degrees, 360 degrees] | . #collapse-show # parameters batch_size = 20 image_size = 256 # train transformations train_trans = transforms.Compose([transforms.ToPILImage(), transforms.RandomRotation((-360, 360)), transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.ToTensor() ]) # validation transformations valid_trans = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor(), ]) # create datasets train_dataset = EyeTrainData(data = train, directory = &#39;../input/diabetic-retinopathy-resized/resized_train/resized_train&#39;, transform = train_trans) valid_dataset = EyeTestData(data = test, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = valid_trans) # create data loaders train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 4) valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = batch_size, shuffle = False, num_workers = 4) . . Next, let&#39;s instantiate the EfficentNet model. We use B4 architecture and initialize pre-trained ImageNet weights by downloading the model parameters in the PyTorch format. The convolutional part of the network responsible for feature extraction outputs a tensor with 1792 features. To adapt the CNN to our task, we replace the last fully-connected classification layer with a (1792, 5) fully-connected layer. . The CNN is instantiated with init_model(). The argument train ensures that we load ImageNet weights on the training stage and turn off gradient computation on the inference stage. . #collapse-show # model name model_name = &#39;enet_b4&#39; # initialization function def init_model(train = True): ### training mode if train == True: # load pre-trained model model = EfficientNet.from_pretrained(&#39;efficientnet-b4&#39;, num_classes = 5) ### inference mode if train == False: # load pre-trained model model = EfficientNet.from_name(&#39;efficientnet-b4&#39;) model._fc = nn.Linear(model._fc.in_features, 5) # freeze layers for param in model.parameters(): param.requires_grad = False return model # check architecture model = init_model() print(model) . . Since we are dealing with a multiple classification problem, we use cross-entropy as a loss function. We use nn.CrossEntropyLoss() which combines logsoftmax and negative log-likelihood loss and applies them to the output of the last network layer. . The Kaggle competition uses Cohen&#39;s kappa for evaluation. Kappa measures the agreement between the actual and predicted labels. Since Kappa is non-differentiable, we can not use it as a loss function. At the same time, we can use Kappa to evaluate the performance and early stop the training epochs. . We use Adam optimizer with a starting learning rate of 0.001. During training, we use a learning rate scheduler, which multiplies the learning rate by 0.5 after every 5 epochs. This helps to make smaller changes to the network weights when we are getting closer to the optimum. . #collapse-show # loss function criterion = nn.CrossEntropyLoss() # epochs max_epochs = 15 early_stop = 5 # learning rates eta = 1e-3 # scheduler step = 5 gamma = 0.5 # optimizer optimizer = optim.Adam(model.parameters(), lr = eta) scheduler = lr_scheduler.StepLR(optimizer, step_size = step, gamma = gamma) # initialize model and send to GPU model = init_model() model = model.to(device) . . After each training epoch, we validate the model on the target data. We extract class scores from the last fully-connected layer and predict the image class corresponding to the highest score. We train the network for 15 epochs, tracking the validation loss and Cohen&#39;s kappa. If the kappa does not increase for 5 consecutive epochs, we terminate the training process and save model weights for the epoch associated with the highest validation kappa. . #collapse-show # placeholders oof_preds = np.zeros((len(test), 5)) val_kappas = [] val_losses = [] trn_losses = [] bad_epochs = 0 # timer cv_start = time.time() # training and validation loop for epoch in range(max_epochs): ##### PREPARATION # timer epoch_start = time.time() # reset losses trn_loss = 0.0 val_loss = 0.0 # placeholders fold_preds = np.zeros((len(test), 5)) ##### TRAINING # switch regime model.train() # loop through batches for batch_i, data in enumerate(train_loader): # extract inputs and labels inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1) inputs = inputs.to(device, dtype = torch.float) labels = labels.to(device, dtype = torch.long) optimizer.zero_grad() # forward and backward pass with torch.set_grad_enabled(True): preds = model(inputs) loss = criterion(preds, labels) loss.backward() optimizer.step() # compute loss trn_loss += loss.item() * inputs.size(0) ##### INFERENCE # switch regime model.eval() # loop through batches for batch_i, data in enumerate(valid_loader): # extract inputs and labels inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1) inputs = inputs.to(device, dtype = torch.float) labels = labels.to(device, dtype = torch.long) # compute predictions with torch.set_grad_enabled(False): preds = model(inputs).detach() fold_preds[batch_i * batch_size:(batch_i + 1) * batch_size, :] = preds.cpu().numpy() # compute loss loss = criterion(preds, labels) val_loss += loss.item() * inputs.size(0) # save predictions oof_preds = fold_preds # scheduler step scheduler.step() ##### EVALUATION # evaluate performance fold_preds_round = fold_preds.argmax(axis = 1) val_kappa = metrics.cohen_kappa_score(test[&#39;diagnosis&#39;], fold_preds_round.astype(&#39;int&#39;), weights = &#39;quadratic&#39;) # save perfoirmance values val_kappas.append(val_kappa) val_losses.append(val_loss / len(test)) trn_losses.append(trn_loss / len(train)) ##### EARLY STOPPING # display info print(&#39;- epoch {}/{} | lr = {} | trn_loss = {:.4f} | val_loss = {:.4f} | val_kappa = {:.4f} | {:.2f} min&#39;.format( epoch + 1, max_epochs, scheduler.get_lr()[len(scheduler.get_lr()) - 1], trn_loss / len(train), val_loss / len(test), val_kappa, (time.time() - epoch_start) / 60)) # check if there is any improvement if epoch &gt; 0: if val_kappas[epoch] &lt; val_kappas[epoch - bad_epochs - 1]: bad_epochs += 1 else: bad_epochs = 0 # save model weights if improvement if bad_epochs == 0: oof_preds_best = oof_preds.copy() torch.save(model.state_dict(), &#39;../models/model_{}.bin&#39;.format(model_name)) # break if early stop if bad_epochs == early_stop: print(&#39;Early stopping. Best results: loss = {:.4f}, kappa = {:.4f} (epoch {})&#39;.format( np.min(val_losses), val_kappas[np.argmin(val_losses)], np.argmin(val_losses) + 1)) print(&#39;&#39;) break # break if max epochs if epoch == (max_epochs - 1): print(&#39;Did not met early stopping. Best results: loss = {:.4f}, kappa = {:.4f} (epoch {})&#39;.format( np.min(val_losses), val_kappas[np.argmin(val_losses)], np.argmin(val_losses) + 1)) print(&#39;&#39;) break # load best predictions oof_preds = oof_preds_best # print performance print(&#39;&#39;) print(&#39;Finished in {:.2f} minutes&#39;.format((time.time() - cv_start) / 60)) . . Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.7140 | val_loss = 1.3364 | val_kappa = 0.7268 | 30.03 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.6447 | val_loss = 1.0670 | val_kappa = 0.8442 | 27.19 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.6203 | val_loss = 0.7667 | val_kappa = 0.7992 | 27.21 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.6020 | val_loss = 0.7472 | val_kappa = 0.8245 | 27.68 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5899 | val_loss = 0.7720 | val_kappa = 0.8541 | 29.42 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5542 | val_loss = 0.9255 | val_kappa = 0.8682 | 29.33 min - epoch 7/15 | lr = 0.0005 | trn_loss = 0.5424 | val_loss = 0.8917 | val_kappa = 0.8763 | 29.91 min - epoch 8/15 | lr = 0.0005 | trn_loss = 0.5359 | val_loss = 0.9555 | val_kappa = 0.8661 | 30.70 min - epoch 9/15 | lr = 0.0005 | trn_loss = 0.5252 | val_loss = 0.8642 | val_kappa = 0.8778 | 28.76 min - epoch 10/15 | lr = 0.000125 | trn_loss = 0.5184 | val_loss = 1.1568 | val_kappa = 0.8403 | 31.14 min - epoch 11/15 | lr = 0.00025 | trn_loss = 0.4974 | val_loss = 0.9464 | val_kappa = 0.8784 | 28.00 min - epoch 12/15 | lr = 0.00025 | trn_loss = 0.4874 | val_loss = 0.9043 | val_kappa = 0.8820 | 27.50 min - epoch 13/15 | lr = 0.00025 | trn_loss = 0.4820 | val_loss = 0.7924 | val_kappa = 0.8775 | 27.36 min - epoch 14/15 | lr = 0.00025 | trn_loss = 0.4758 | val_loss = 0.9300 | val_kappa = 0.8761 | 27.33 min - epoch 15/15 | lr = 6.25e-05 | trn_loss = 0.4693 | val_loss = 0.9109 | val_kappa = 0.8803 | 27.51 min Did not met early stopping. Best results: loss = 0.7472, kappa = 0.8245 (epoch 4) Finished in 429.16 minutes . Training on the Kaggle GPU-enabled machine took us about 7 hours! Let&#39;s visualize the training and validation loss dynamics. . #collapse-show # plot size fig = plt.figure(figsize = (15, 5)) # plot loss dynamics plt.subplot(1, 2, 1) plt.plot(trn_losses, &#39;red&#39;, label = &#39;Training&#39;) plt.plot(val_losses, &#39;green&#39;, label = &#39;Validation&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Loss&#39;) plt.legend() # plot kappa dynamics plt.subplot(1, 2, 2) plt.plot(val_kappas, &#39;blue&#39;, label = &#39;Kappa&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Kappa&#39;) plt.legend() . . . The cross-entropy loss on the validation set reaches minimum already after 3 epochs. At the same time, kappa continues to increase up to the 15th epoch. Since we use kappa to evaluate the quality of our solution, we save weights after 15 epochs. . We also construct a confusion matrix of the trained model. The numbers in the cells are percentages. According to the results, the model does a poor job in distinguishing the mild and moderate stages of DR: 86% of images with mild DR are classified as moderate. The best performance is observed for healthy patients. Overall, we see that the model tends to confuse nearby severity stages but rarely misclassifies the proliferate and mild stages. . . Fine-tuning . Fine-tuning on the target data is performed within 4-fold cross-validation. To ensure that we have enough examples of each class, we perform cross-validation with stratification. . On each iteration, we instantiate the EfficientNet B4 model with the same architecture as in the previous section. Next, we load the saved weights from the model pre-trained on the source data. We freeze weights on all network layers except for the last fully-connected layer. The weights in this layer are fine-tuned. As on the pre-training stage, we use Adam optimizer and implement a learning rate scheduler. We also track performance on the validation folds and stop training if kappa does not increase for 5 consecutive epochs. . The process is repeated for each of the 4 folds, and the best model weights are saved for each combination of the training folds. . The init_model() is updated to load the weights saved on the pre-training stage and freeze the first layers of the network in the training regime. . #collapse-show # model name model_name = &#39;enet_b4&#39; # initialization function def init_model(train = True, trn_layers = 2): ### training mode if train == True: # load pre-trained model model = EfficientNet.from_pretrained(&#39;efficientnet-b4&#39;, num_classes = 5) model.load_state_dict(torch.load(&#39;../models/model_{}.bin&#39;.format(model_name, 1))) # freeze first layers for child in list(model.children())[:-trn_layers]: for param in child.parameters(): param.requires_grad = False ### inference mode if train == False: # load pre-trained model model = EfficientNet.from_pretrained(&#39;efficientnet-b4&#39;, num_classes = 5) model.load_state_dict(torch.load(&#39;../models/model_{}.bin&#39;.format(model_name, 1))) # freeze all layers for param in model.parameters(): param.requires_grad = False return model # check architecture model = init_model() . . The training loop is now wrapped into a cross-validation loop. . #collapse-hide ##### VALIDATION SETTINGS # no. folds num_folds = 4 # creating splits skf = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = seed) splits = list(skf.split(train[&#39;id_code&#39;], train[&#39;diagnosis&#39;])) # placeholders oof_preds = np.zeros((len(train), 1)) # timer cv_start = time.time() ##### PARAMETERS # loss function criterion = nn.CrossEntropyLoss() # epochs max_epochs = 15 early_stop = 5 # learning rates eta = 1e-3 # scheduler step = 5 gamma = 0.5 ##### CROSS-VALIDATION LOOP for fold in tqdm(range(num_folds)): ####### DATA PREPARATION # display information print(&#39;-&#39; * 30) print(&#39;FOLD {}/{}&#39;.format(fold + 1, num_folds)) print(&#39;-&#39; * 30) # load splits data_train = train.iloc[splits[fold][0]].reset_index(drop = True) data_valid = train.iloc[splits[fold][1]].reset_index(drop = True) # create datasets train_dataset = EyeTrainData(data = data_train, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = train_trans) valid_dataset = EyeTrainData(data = data_valid, directory = &#39;../input/aptos2019-blindness-detection/train_images&#39;, transform = valid_trans) # create data loaders train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 4) valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size = batch_size, shuffle = False, num_workers = 4) ####### MODEL PREPARATION # placeholders val_kappas = [] val_losses = [] trn_losses = [] bad_epochs = 0 # load best OOF predictions if fold &gt; 0: oof_preds = oof_preds_best.copy() # initialize and send to GPU model = init_model(train = True) model = model.to(device) # optimizer optimizer = optim.Adam(model._fc.parameters(), lr = eta) scheduler = lr_scheduler.StepLR(optimizer, step_size = step, gamma = gamma) ####### TRAINING AND VALIDATION LOOP for epoch in range(max_epochs): ##### PREPARATION # timer epoch_start = time.time() # reset losses trn_loss = 0.0 val_loss = 0.0 # placeholders fold_preds = np.zeros((len(data_valid), 1)) ##### TRAINING # switch regime model.train() # loop through batches for batch_i, data in enumerate(train_loader): # extract inputs and labels inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1) inputs = inputs.to(device, dtype = torch.float) labels = labels.to(device, dtype = torch.long) optimizer.zero_grad() # forward and backward pass with torch.set_grad_enabled(True): preds = model(inputs) loss = criterion(preds, labels) loss.backward() optimizer.step() # compute loss trn_loss += loss.item() * inputs.size(0) ##### INFERENCE # initialize model.eval() # loop through batches for batch_i, data in enumerate(valid_loader): # extract inputs and labels inputs = data[&#39;image&#39;] labels = data[&#39;label&#39;].view(-1) inputs = inputs.to(device, dtype = torch.float) labels = labels.to(device, dtype = torch.long) # compute predictions with torch.set_grad_enabled(False): preds = model(inputs).detach() _, class_preds = preds.topk(1) fold_preds[batch_i * batch_size:(batch_i + 1) * batch_size, :] = class_preds.cpu().numpy() # compute loss loss = criterion(preds, labels) val_loss += loss.item() * inputs.size(0) # save predictions oof_preds[splits[fold][1]] = fold_preds # scheduler step scheduler.step() ##### EVALUATION # evaluate performance fold_preds_round = fold_preds val_kappa = metrics.cohen_kappa_score(data_valid[&#39;diagnosis&#39;], fold_preds_round.astype(&#39;int&#39;), weights = &#39;quadratic&#39;) # save perfoirmance values val_kappas.append(val_kappa) val_losses.append(val_loss / len(data_valid)) trn_losses.append(trn_loss / len(data_train)) ##### EARLY STOPPING # display info print(&#39;- epoch {}/{} | lr = {} | trn_loss = {:.4f} | val_loss = {:.4f} | val_kappa = {:.4f} | {:.2f} min&#39;.format( epoch + 1, max_epochs, scheduler.get_lr()[len(scheduler.get_lr()) - 1], trn_loss / len(data_train), val_loss / len(data_valid), val_kappa, (time.time() - epoch_start) / 60)) # check if there is any improvement if epoch &gt; 0: if val_kappas[epoch] &lt; val_kappas[epoch - bad_epochs - 1]: bad_epochs += 1 else: bad_epochs = 0 # save model weights if improvement if bad_epochs == 0: oof_preds_best = oof_preds.copy() torch.save(model.state_dict(), &#39;../models/model_{}_fold{}.bin&#39;.format(model_name, fold + 1)) # break if early stop if bad_epochs == early_stop: print(&#39;Early stopping. Best results: loss = {:.4f}, kappa = {:.4f} (epoch {})&#39;.format( np.min(val_losses), val_kappas[np.argmin(val_losses)], np.argmin(val_losses) + 1)) print(&#39;&#39;) break # break if max epochs if epoch == (max_epochs - 1): print(&#39;Did not meet early stopping. Best results: loss = {:.4f}, kappa = {:.4f} (epoch {})&#39;.format( np.min(val_losses), val_kappas[np.argmin(val_losses)], np.argmin(val_losses) + 1)) print(&#39;&#39;) break # load best predictions oof_preds = oof_preds_best # print performance print(&#39;&#39;) print(&#39;Finished in {:.2f} minutes&#39;.format((time.time() - cv_start) / 60)) . . FOLD 1/4 Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.6279 | val_loss = 0.5368 | val_kappa = 0.8725 | 7.30 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.5699 | val_loss = 0.5402 | val_kappa = 0.8662 | 7.25 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.5572 | val_loss = 0.5380 | val_kappa = 0.8631 | 7.31 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.5482 | val_loss = 0.5357 | val_kappa = 0.8590 | 7.29 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5658 | val_loss = 0.5357 | val_kappa = 0.8613 | 7.25 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5537 | val_loss = 0.5346 | val_kappa = 0.8604 | 7.28 min Early stopping. Best results: loss = 0.5346, kappa = 0.8604 (epoch 6) FOLD 2/4 Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.6535 | val_loss = 0.5295 | val_kappa = 0.8767 | 7.24 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.5691 | val_loss = 0.5158 | val_kappa = 0.8717 | 7.20 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.5645 | val_loss = 0.5136 | val_kappa = 0.8732 | 7.23 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.5592 | val_loss = 0.5151 | val_kappa = 0.8705 | 7.26 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5530 | val_loss = 0.5213 | val_kappa = 0.8686 | 7.27 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5499 | val_loss = 0.5143 | val_kappa = 0.8733 | 7.21 min Early stopping. Best results: loss = 0.5136, kappa = 0.8732 (epoch 3) FOLD 3/4 Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.6503 | val_loss = 0.5286 | val_kappa = 0.8937 | 7.16 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.5916 | val_loss = 0.5166 | val_kappa = 0.8895 | 7.20 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.5702 | val_loss = 0.5115 | val_kappa = 0.8834 | 7.34 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.5606 | val_loss = 0.5133 | val_kappa = 0.8829 | 7.44 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5640 | val_loss = 0.5081 | val_kappa = 0.8880 | 7.28 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5515 | val_loss = 0.5109 | val_kappa = 0.8871 | 7.20 min Early stopping. Best results: loss = 0.5081, kappa = 0.8880 (epoch 5) FOLD 4/4 Loaded pretrained weights for efficientnet-b4 - epoch 1/15 | lr = 0.001 | trn_loss = 0.6379 | val_loss = 0.5563 | val_kappa = 0.8595 | 7.18 min - epoch 2/15 | lr = 0.001 | trn_loss = 0.5718 | val_loss = 0.5423 | val_kappa = 0.8610 | 7.20 min - epoch 3/15 | lr = 0.001 | trn_loss = 0.5590 | val_loss = 0.5433 | val_kappa = 0.8587 | 7.19 min - epoch 4/15 | lr = 0.001 | trn_loss = 0.5554 | val_loss = 0.5433 | val_kappa = 0.8579 | 7.17 min - epoch 5/15 | lr = 0.00025 | trn_loss = 0.5476 | val_loss = 0.5393 | val_kappa = 0.8608 | 7.18 min - epoch 6/15 | lr = 0.0005 | trn_loss = 0.5509 | val_loss = 0.5331 | val_kappa = 0.8610 | 7.32 min - epoch 7/15 | lr = 0.0005 | trn_loss = 0.5532 | val_loss = 0.5309 | val_kappa = 0.8567 | 7.28 min Early stopping. Best results: loss = 0.5309, kappa = 0.8567 (epoch 7) Finished in 181.29 minutes . The model converges rather quickly. The best validation performance is obtained after 3 to 7 training epochs depending on a fold. . Let&#39;s look at the confusion matrix. The matrix illustrates the advantages of the fine-tuned model over the pre-trained CNN and indicates a better performance in classifying mild stages of the DR. However, we also observe that the model classifies too many examples as moderate (class = 2). . #collapse-hide # construct confusion matrx oof_preds_round = oof_preds.copy() cm = confusion_matrix(train[&#39;diagnosis&#39;], oof_preds_round) cm = cm.astype(&#39;float&#39;) / cm.sum(axis = 1)[:, np.newaxis] annot = np.around(cm, 2) # plot matrix fig, ax = plt.subplots(figsize = (8, 6)) sns.heatmap(cm, cmap = &#39;Blues&#39;, annot = annot, lw = 0.5) ax.set_xlabel(&#39;Prediction&#39;) ax.set_ylabel(&#39;Ground Truth&#39;) ax.set_aspect(&#39;equal&#39;) . . . Inference . Let&#39;s now produce some predictions for the test set! . We aggregate predictions from the models trained during the cross-validation loop. To do so, we extract class scores from the last fully-connected layer and define class predictions as the classes with the maximal score. Next, we average predictions of the 4 networks trained on different combinations of the training folds. . #collapse-hide ##### TRANSFORMATIONS # parameters batch_size = 25 image_size = 256 # test transformations test_trans = transforms.Compose([transforms.ToPILImage(), transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.ToTensor()]) ##### DATA LOADER # create dataset test_dataset = EyeTestData(data = test, directory = &#39;../input/aptos2019-blindness-detection/test_images&#39;, transform = test_trans) # create data loader test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = False, num_workers = 4) ##### MODEL ARCHITECTURE # model name model_name = &#39;enet_b4&#39; # initialization function def init_model(train = True, trn_layers = 2): ### training mode if train == True: # load pre-trained model model = EfficientNet.from_pretrained(&#39;efficientnet-b4&#39;, num_classes = 5) # freeze first layers for child in list(model.children())[:-trn_layers]: for param in child.parameters(): param.requires_grad = False ### inference mode if train == False: # load pre-trained model model = EfficientNet.from_name(&#39;efficientnet-b4&#39;) model._fc = nn.Linear(model._fc.in_features, 5) # freeze all layers for param in model.parameters(): param.requires_grad = False ### return model return model # check architecture model = init_model(train = False) . . We also use test-time augmentations by creating 4 versions of the test images with random augmentations (horizontal and vertical flips) and average predictions over the image variants. The final prediction is an average of 4 models times 4 image variants. . #collapse-show # validation settings num_folds = 4 tta_times = 4 # placeholders test_preds = np.zeros((len(test), num_folds)) cv_start = time.time() # prediction loop for fold in tqdm(range(num_folds)): # load model and sent to GPU model = init_model(train = False) model.load_state_dict(torch.load(&#39;../models/model_{}_fold{}.bin&#39;.format(model_name, fold + 1))) model = model.to(device) model.eval() # placeholder fold_preds = np.zeros((len(test), 1)) # loop through batches for _ in range(tta_times): for batch_i, data in enumerate(test_loader): inputs = data[&#39;image&#39;] inputs = inputs.to(device, dtype = torch.float) preds = model(inputs).detach() _, class_preds = preds.topk(1) fold_preds[batch_i * batch_size:(batch_i + 1) * batch_size, :] += class_preds.cpu().numpy() fold_preds = fold_preds / tta_times # aggregate predictions test_preds[:, fold] = fold_preds.reshape(-1) # print performance test_preds_df = pd.DataFrame(test_preds.copy()) print(&#39;Finished in {:.2f} minutes&#39;.format((time.time() - cv_start) / 60)) . . Let&#39;s have a look at the distribution of predictions: . #collapse-hide # show predictions print(&#39;-&#39; * 45) print(&#39;PREDICTIONS&#39;) print(&#39;-&#39; * 45) print(test_preds_df.head()) # show correlation print(&#39;-&#39; * 45) print(&#39;CORRELATION MATRIX&#39;) print(&#39;-&#39; * 45) print(np.round(test_preds_df.corr(), 4)) print(&#39;Mean correlation = &#39; + str(np.round(np.mean(np.mean(test_preds_df.corr())), 4))) # show stats print(&#39;-&#39; * 45) print(&#39;SUMMARY STATS&#39;) print(&#39;-&#39; * 45) print(test_preds_df.describe()) # show prediction distribution print(&#39;-&#39; * 45) print(&#39;ROUNDED PREDICTIONS&#39;) print(&#39;-&#39; * 45) for f in range(num_folds): print(np.round(test_preds_df[f]).astype(&#39;int&#39;).value_counts(normalize = True)) print(&#39;-&#39; * 45) # plot densities test_preds_df.plot.kde() . . PREDICTIONS 0 1 2 3 0 2.0 2.0 2.0 1.5 1 2.0 2.5 2.0 2.0 2 2.0 2.0 2.0 2.0 3 2.0 3.0 2.0 2.0 4 2.0 2.0 2.0 2.0 CORRELATION MATRIX 0 1 2 3 0 1.0000 0.9624 0.9573 0.9534 1 0.9624 1.0000 0.9686 0.9490 2 0.9573 0.9686 1.0000 0.9478 3 0.9534 0.9490 0.9478 1.0000 Mean correlation = 0.9673 SUMMARY STATS 0 1 2 3 count 1928.0000 1928.0000 1928.0000 1928.0000 mean 1.6867 1.7103 1.7152 1.6501 std 0.9727 0.9730 0.9704 0.9606 min 0.0000 0.0000 0.0000 0.0000 25% 1.5000 2.0000 2.0000 1.3750 50% 2.0000 2.0000 2.0000 2.0000 75% 2.0000 2.0000 2.0000 2.0000 max 4.0000 4.0000 4.0000 4.0000 ROUNDED PREDICTIONS 2 0.6654 0 0.2002 1 0.0471 3 0.0440 4 0.0430 Name: 0, dtype: float64 2 0.6701 0 0.1950 4 0.0461 3 0.0451 1 0.0435 Name: 1, dtype: float64 2 0.6789 0 0.1950 4 0.0487 1 0.0389 3 0.0383 Name: 2, dtype: float64 2 0.6856 0 0.2079 1 0.0420 4 0.0409 3 0.0233 Name: 3, dtype: float64 . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8e403032d0&gt; . The model classifies a lot of images as moderate DR. To reduce the number of such examples, we can change thresholds used to round the averaged predictions into classes. We use the following vector of thresholds: [0.5, 1.75, 2.25, 3.5]. The final prediction is set to zero if the average value is below 0.5; set to one if the average value lies in [0.5, 1.75), etc. This reduces the share of images classified as moderate DR. . #collapse-show # aggregate predictions test_preds = test_preds_df.mean(axis = 1).values # set cutoffs coef = [0.5, 1.75, 2.25, 3.5] # rounding for i, pred in enumerate(test_preds): if pred &lt; coef[0]: test_preds[i] = 0 elif pred &gt;= coef[0] and pred &lt; coef[1]: test_preds[i] = 1 elif pred &gt;= coef[1] and pred &lt; coef[2]: test_preds[i] = 2 elif pred &gt;= coef[2] and pred &lt; coef[3]: test_preds[i] = 3 else: test_preds[i] = 4 . . We are done! We can now export test_preds as csv and submit it to the competition. . 5. Closing words . This blog post provides a complete walkthrough on the project on detecting blindness in the retina images using CNNs. We use image preprocessing to reduce discrepancies across images taken in different clinics, apply transfer learning to leverage knowledge from larger data sets and implement different techniques to improve performance. . If you are still reading this post, you might be wondering about ways to further improve the solution. There are multiple options. First, employing a larger network architecture and increasing the number of training epochs on the pre-training stage has a high potential for a better performance. At the same time, this would require more computing power, which might not be optimal considering the possible use of the automated retina image classification in practice. . Second, image preprocessing can be further improved. During the refinement process, the largest performance gains were attributed to different preprocessing steps. This is a more efficient way to further improve the performance. . Finally, the best solutions of other competitors rely on ensembles of CNNs using different image sizes and/or architectures. Incorporating multiple heterogeneous models and blending their predictions could also improve the proposed solution. Ensembling predictions of models similar to the one discussed in this post is what helped me to place in the top 9% of the leaderboard. .",
            "url": "https://kozodoi.me/blog/20200711/blindness-detection",
            "relUrl": "/blog/20200711/blindness-detection",
            "date": " • Jul 11, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Algorithmic Fairness in R",
            "content": "Last update: 18.10.2021. All opinions are my own. . 1. Overview . How to measure fairness of a machine learning model? . To date, a number of algorithmic fairness metrics have been proposed. Demographic parity, proportional parity and equalized odds are among the most commonly used metrics to evaluate fairness across sensitive groups in binary classification problems. Multiple other metrics have been proposed based on performance measures extracted from the confusion matrix (e.g., false positive rate parity, false negative rate parity). . Together with Tibor V. Varga, we developed fairness package for R. The package provides tools to calculate fairness metrics across different sensitive groups. It also provides opportunities to visualize and compare other prediction metrics between the groups. . This blog post provides a tutorial on using the fairness package on a COMPAS data set. The package is published on CRAN and GitHub. . The package implements the following fairness metrics: . Demographic parity (also known as independence) | Proportional parity | Equalized odds (also known as separation) | Predictive rate parity | False positive rate parity | False negative rate parity | Accuracy parity | Negative predictive value parity | Specificity parity | ROC AUC parity | MCC parity | . 2. Installation . The latest stable version published on CRAN is fairness 1.2.2 (as of 14.04.2021). You can install this from CRAN by running: . install.packages(&#39;fairness&#39;) library(fairness) . Installing package into ‘/usr/local/lib/R/site-library’ (as ‘lib’ is unspecified) . You may also install the development version from Github to get the latest features: . devtools::install_github(&#39;kozodoi/fairness&#39;) library(fairness) . 3. Data description . The package includes two exemplary data sets to study fairness: compas and germancredit. . compas . This tutorial uses a simplified version of the landmark COMPAS data set containing the criminal history of defendants from Broward County. You can read more about the data here. To load the data, all you need to do is: . data(&#39;compas&#39;) head(compas) . Two_yr_RecidivismNumber_of_PriorsAge_Above_FourtyFiveAge_Below_TwentyFiveFemaleMisdemeanorethnicityprobabilitypredicted . 1no | -0.68435 | no | no | Male | yes | Other | 0.31515 | 0 | . 2yes | 2.26688 | no | no | Male | no | Caucasian | 0.88546 | 1 | . 3no | -0.68435 | no | no | Female | yes | Caucasian | 0.25526 | 0 | . 4no | -0.68435 | no | no | Male | no | African_American | 0.41739 | 0 | . 5no | -0.68435 | no | no | Male | yes | Hispanic | 0.32009 | 0 | . The data set contains nine variables. The outcome variable is Two_yr_Recidivism, a binary indicator showing whether an individual committed a crime within the two-year period. The data also includes features on prior criminal record (Number_of_Priors, Misdemeanor), features describing age (Age_Above_FourtyFive, Age_Below_TwentyFive), sex and ethnicity (Female, ethnicity). . For illustrative purposes, we have already trained a classifier that uses all features to predict Two_yr_Recidivism and concatenated the predicted probabilities (probability) and predicted classes (predicted) to the data frame. Feel free to use these columns with predictions to test different fairness metrics before evaluating a custom model. . germancredit . The second included data set is a credit scoring data set labeled as germancredit. The data includes 20 features describing the loan applicants and a binary outcome variable BAD indicating whether the applicant defaulted on a loan. Similarly to COMPAS, germancredit also includes two columns with model predictions named probability and predicted. The data can be loaded with: . data(&#39;germancredit&#39;) . 4. Train a classifier . For the purpose of this tutorial, we will train two classifiers using different sets of features: . model that uses all features as input | model that uses all features except for ethnicity | . We partition the COMPAS data into training and validation subsets and use logistic regression as a base classifier. . #collapse-show # extract data compas &lt;- fairness::compas df &lt;- compas[, !(colnames(compas) %in% c(&#39;probability&#39;, &#39;predicted&#39;))] # partitioning params set.seed(77) val_percent &lt;- 0.3 val_idx &lt;- sample(1:nrow(df))[1:round(nrow(df) * val_percent)] # partition the data df_train &lt;- df[-val_idx, ] df_valid &lt;- df[ val_idx, ] # check dim print(nrow(df_train)) print(nrow(df_valid)) . . [1] 4320 [1] 1852 . #collapse-show # fit logit models model1 &lt;- glm(Two_yr_Recidivism ~ ., data = df_train, family = binomial(link = &#39;logit&#39;)) model2 &lt;- glm(Two_yr_Recidivism ~ . -ethnicity, data = df_train, family = binomial(link = &#39;logit&#39;)) . . Let&#39;s append model predictions to the validation set. Later, we will evaluate fairness of the two models based on these predictions. . #collapse-show # produce predictions df_valid$prob_1 &lt;- predict(model1, df_valid, type = &#39;response&#39;) df_valid$prob_2 &lt;- predict(model2, df_valid, type = &#39;response&#39;) head(df_valid) . . Two_yr_RecidivismNumber_of_PriorsAge_Above_FourtyFiveAge_Below_TwentyFiveFemaleMisdemeanorethnicityprob_1prob_2 . 1no | -0.68435 | no | no | Male | no | African_American | 0.36787 | 0.34815 | . 2no | 2.05607 | no | no | Male | no | Hispanic | 0.80241 | 0.83477 | . 3yes | -0.47355 | no | yes | Male | no | African_American | 0.58958 | 0.57305 | . 4no | -0.68435 | yes | no | Male | no | African_American | 0.23956 | 0.22189 | . 5yes | 0.58045 | no | no | Male | no | Caucasian | 0.59155 | 0.60107 | . 5. Intro to algorithimc fairness . An outlook on the confusion matrix . Most fairness metrics are calculated based on a confusion matrix produced by a classification model. The confusion matrix is comprised of four classes: . True positives (TP): the true class is positive and the prediction is positive (correct classification) | False positives (FP): the true class is negative and the prediction is positive (incorrect classification) | True negatives (TN): the true class is negative and the prediction is negative (correct classification) | False negatives (FN): the true class is positive and the prediction is negative (incorrect classification) | . Fairness metrics are calculated by comparing one or more of these measures across sensitive subgroups (e.g., male and female). For a detailed overview of measures coming from the confusion matrix and precise definitions, click here or here. . Fairness metrics functions . The package implements 11 fairness metrics. Many of these are mutually exclusive: results for a given classification problem often cannot be fair in terms of all metrics. Depending on a context, it is important to select an appropriate metric to evaluate fairness. . Below, we describe functions used to compute the implemented metrics. Every function has a similar set of arguments: . data: data.frame containing the input data and model predictions | group: column name indicating the sensitive group (factor variable) | base: base level of the sensitive group for fairness metrics calculation | outcome: column name indicating the binary outcome variable | outcome_base: base level of the outcome variable (i.e., negative class) for fairness metrics calculation | . We also need to supply model predictions. Depending on the metric, we need to provide either probabilistic predictions as probs or class predictions as preds. The model predictions can be appended to the original data.frame or provided as a vector. In this tutorial, we will use probabilistic predictions with all functions. When working with probabilistic predictions, some metrics require a cutoff value to convert probabilities into class predictions supplied as cutoff. . The package also supports a continuous group variable (e.g., age). If group is continuous, a user need to supply group_breaks argument to specify breaks in the variable values. More details are provided in the functions documentation. . Before looking at different metrics, we will create a binary numeric version of the outcome variable that we will supply as outcome in fairness metrics functions. We can also work with an original factor outcome Two_yr_Recidivism but in this case we should make sure that predictions and outcome have the same factor levels. . df_valid$Two_yr_Recidivism_01 &lt;- ifelse(df_valid$Two_yr_Recidivism == &#39;yes&#39;, 1, 0) . 6. Computing fairness metrics . Predictive rate parity . Let&#39;s demonstrate the fairness pipeline using predictive rate parity as an example. Predictive rate parity is achieved if the precisions (or positive predictive values) in the subgroups are close to each other. The precision stands for the number of the true positives divided by the total number of examples predicted positive within a group. . Formula: TP / (TP + FP) . Let&#39;s compute predictive rate parity for the first model that uses all features: . res1 &lt;- pred_rate_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;Caucasian&#39;) res1$Metric . CaucasianAfrican_AmericanAsianHispanicNative_AmericanOther . Precision 0.585034 | 0.702381 | 0.5000000 | 0.5909091 | 1.000000 | 0.900000 | . Predictive Rate Parity 1.000000 | 1.200581 | 0.8546512 | 1.0100423 | 1.709302 | 1.538372 | . Group size622 | 962 | 160 | 1440 | 4 | 104 | . The first row shows the raw precision values for all ethnicities. The second row displays the relative precisions compared to Caucasian defendants. . In a perfect world, all predictive rate parities should be equal to one, which would mean that precision in every group is the same as in the base group. In practice, values are going to be different. The parity above one indicates that precision in this group is relatively higher, whereas a lower parity implies a lower precision. Observing a large variance in parities should hint us that the model is not performing equally well for different groups. . If the other ethnic group is set as a base group (e.g. Hispanic), the raw precision values do not change, only the relative metrics: . res1h &lt;- pred_rate_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;Hispanic&#39;) res1h$Metric . HispanicCaucasianAfrican_AmericanAsianNative_AmericanOther . Precision 0.5909091 | 0.5850340 | 0.702381 | 0.5000000 | 1.000000 | 0.900000 | . Predictive Rate Parity 1.0000000 | 0.9900576 | 1.188645 | 0.8461538 | 1.692308 | 1.523077 | . Group size1440 | 6220 | 962 | 160 | 4 | 104 | . Overall, results suggest that the model precision varies between 0.5 and 1. The lowest precision is observed for Asian defendants. This implies that there are more cases where the model mistakingly predicts that a person will commit a crime among Asians than among, e.g., Native_American defendants. . A standard output of every fairness metric function includes a barchart that visualizes the relative metrics for all subgroups: . res1h$Metric_plot . Some fairness metrics do not require probabilistic predictions and can work with class predictions. When predicted probabilities are supplied, the output includes a density plot displaying the distributions of probabilities in all subgroups: . res1h$Probability_plot . Let&#39;s now compare the results to the second model that does not use ethnicity as a feature: . # model 2 res2 &lt;- pred_rate_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_2&#39;, cutoff = 0.5, base = &#39;Caucasian&#39;) res2$Metric . CaucasianAfrican_AmericanAsianHispanicNative_AmericanOther . Precision 0.5933333 | 0.703854 | 0.4000000 | 0.5142857 | 1.000000 | 0.625000 | . Predictive Rate Parity 1.0000000 | 1.186271 | 0.6741573 | 0.8667737 | 1.685393 | 1.053371 | . Group size6220 | 962 | 160 | 1440 | 4 | 104 | . We can see two things. . First, excluding ethnicity from the features slightly increases precision for some defendants (Caucasian and African_American) but results in a lower precision for some other groups (Asian and Hispanic). This illustrates that improving a model for one group may cost a fall in the predictive performance for the general population. Depending on the context, it is a task of a decision-maker to decide what is best. . Second, excluding ethnicity does not align the predictive rate parities substantially closer to one. This illustrates another important research finding: removing a sensitive variable does not guarantee that a model stops discriminating. Ethnicity correlates with other features and is still implicitly included in the input data. In order to make the classifier more fair, one would need to consider more sophisticated techniques than simply dropping the sensitive attribute. . In the rest of this tutorial, we will go through the functions that cover the remaining implemented fairness metrics, illustrating the corresponding equations and outputs. You can find more details on each of the fairness metric functions in the package documentation. Please don&#39;t hesitate to use the built-in helper to see further details and examples on the implemented metrics: . ?fairness::pred_rate_parity . Demographic parity . Demographic parity is one of the most popular fairness indicators in the literature. Demographic parity is achieved if the absolute number of positive predictions in the subgroups are close to each other. This measure does not take true class into consideration and only depends on the model predictions. In some literature, demographic parity is also referred to as statistical parity or independence. . Formula: (TP + FP) . res_dem &lt;- dem_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;Caucasian&#39;) res_dem$Metric . CaucasianAfrican_AmericanAsianHispanicNative_AmericanOther . Positively classified147 | 504 | 2.00000000 | 22.0000000 | 1.000000000 | 10.00000000 | . Demographic Parity 1 | 3.428571 | 0.01360544 | 0.1496599 | 0.006802721 | 0.06802721 | . Group size622 | 962 | 1600 | 1440 | 4000 | 10400 | . res_dem$Metric_plot . Of course, comparing the absolute number of positive predictions will show a high disparity when the number of cases within each group is different, which artificially boosts the disparity. This is true in our case: . table(df_valid$ethnicity) . Caucasian African_American Asian Hispanic 622 962 16 144 Native_American Other 4 104 . To address this, we can use proportional parity. . Proportional parity . Proportional parity is very similar to demographic parity but modifies it to address the issue discussed above. Proportional parity is achieved if the proportion of positive predictions in the subgroups are close to each other. Similar to the demographic parity, this measure also does not depend on the true labels. . Formula: (TP + FP) / (TP + FP + TN + FN) . res_prop &lt;- prop_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;Caucasian&#39;) res_prop$Metric . CaucasianAfrican_AmericanAsianHispanicNative_AmericanOther . Proportion 0.2363344 | 0.5239085 | 0.1250000 | 0.1527778 | 0.250000 | 0.09615385 | . Proportional Parity 1.0000000 | 2.2168102 | 0.5289116 | 0.6464475 | 1.057823 | 0.40685505 | . Group size6220 | 962 | 160 | 1440 | 4 | 10400 | . res_prop$Metric_plot . The proportional parity still shows that African-American defendants are treated unfairly by our model. At the same time, the disparity is lower compared to the one observed with the demographic parity. . All the remaining fairness metrics account for both model predictions and the true labels. . Equalized odds . Equalized odds, also known as separation, are achieved if the sensitivities in the subgroups are close to each other. The group-specific sensitivities indicate the number of the true positives divided by the total number of positives in that group. . Formula: TP / (TP + FN) . res_eq &lt;- equal_odds(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;African_American&#39;) res_eq$Metric . African_AmericanCaucasianAsianHispanicNative_AmericanOther . Sensitivity0.6996047 | 0.3659574 | 0.2500000 | 0.2600000 | 0.5000000 | 0.2142857 | . Equalized odds1.0000000 | 0.5230917 | 0.3573446 | 0.3716384 | 0.7146893 | 0.3062954 | . Group size962 | 6220 | 16 | 144 | 4 | 1040 | . Accuracy parity . Accuracy parity is achieved if the accuracies (all accurately classified examples divided by the total number of examples) in the subgroups are close to each other. . Formula: (TP + TN) / (TP + FP + TN + FN) . res_acc &lt;- acc_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;African_American&#39;) res_acc$Metric . African_AmericanCaucasianAsianHispanicNative_AmericanOther . Accuracy0.6860707 | 0.6623794 | 0.750000 | 0.6805556 | 0.750000 | 0.6730769 | . Accuracy Parity1.0000000 | 0.9654682 | 1.093182 | 0.9919613 | 1.093182 | 0.9810606 | . Group size962 | 6220 | 16 | 144 | 4 | 1040 | . False negative rate parity . False negative rate parity is achieved if the false negative rates (the ratio between the number of false negatives and the total number of positives) in the subgroups are close to each other. . Formula: FN / (TP + FN) . res_fnr &lt;- fnr_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;African_American&#39;) res_fnr$Metric . African_AmericanCaucasianAsianHispanicNative_AmericanOther . FNR 0.3003953 | 0.6340426 | 0.750000 | 0.740000 | 0.500000 | 0.7857143 | . FNR Parity 1.0000000 | 2.1106943 | 2.496711 | 2.463421 | 1.664474 | 2.6156015 | . Group size962 | 6220 | 16 | 144 | 4 | 1040 | . False positive rate parity . False positive rate parity is achieved if the false positive rates (the ratio between the number of false positives and the total number of negatives) in the subgroups are close to each other. . Formula: FP / (TN + FP) . res_fpr &lt;- fpr_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;African_American&#39;) res_fpr$Metric . African_AmericanCaucasianAsianHispanicNative_AmericanOther . FPR 0.3289474 | 0.1576227 | 0.08333333 | 0.09574468 | 0 | 0.01612903 | . FPR Parity 1.0000000 | 0.4791731 | 0.25333333 | 0.29106383 | 0 | 0.04903226 | . Group size962 | 6220 | 1600 | 14400 | 4 | 10400 | . Negative predictive value parity . Negative predictive value parity is achieved if the negative predictive values in the subgroups are close to each other. The negative predictive value is computed as a ratio between the number of true negatives and the total number of predicted negatives. This function can be considered the ‘inverse’ of the predictive rate parity. . Formula: TN / (TN + FN) . res_npv &lt;- npv_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;African_American&#39;) res_npv$Metric . African_AmericanCaucasianAsianHispanicNative_AmericanOther . NPV 0.6681223 | 0.6863158 | 0.7857143 | 0.6967213 | 0.6666667 | 0.6489362 | . NPV Parity 1.0000000 | 1.0272308 | 1.1760037 | 1.0428051 | 0.9978214 | 0.9712835 | . Group size962 | 6220 | 160 | 1440 | 40 | 1040 | . Specificity parity . Specificity parity is achieved if the specificities (the ratio of the number of the true negatives and the total number of negatives) in the subgroups are close to each other. This function can be considered the ‘inverse’ of the equalized odds. . Formula: TN / (TN + FP) . res_sp &lt;- spec_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;ethnicity&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;African_American&#39;) res_sp$Metric . African_AmericanCaucasianAsianHispanicNative_AmericanOther . Specificity 0.6710526 | 0.8423773 | 0.9166667 | 0.9042553 | 1.000000 | 0.983871 | . Specificity Parity 1.0000000 | 1.2553073 | 1.3660131 | 1.3475177 | 1.490196 | 1.466161 | . Group size962 | 6220 | 160 | 1440 | 4 | 104 | . Apart from the parity-based metrics presented above, two additional comparisons are implemented: ROC AUC comparison and Matthews correlation coefficient comparison. . ROC AUC parity . This function calculates ROC AUC and visualizes ROC curves for all subgroups. Note that probabilities must be defined for this function. Also, as ROC evaluates all possible cutoffs, the cutoff argument is excluded from this function. . res_auc &lt;- roc_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, group = &#39;Female&#39;, probs = &#39;prob_1&#39;, base = &#39;Male&#39;) res_auc$Metric . . MaleFemale . ROC AUC 0.7221429 | 0.7192349 | . ROC AUC Parity 1.0000000 | 0.9959731 | . Group size151 | 337 | . Apart from the standard outputs, the function also returns ROC curves for each of the subgroups: . res_auc$ROCAUC_plot . Matthews correlation coefficient parity . The Matthews correlation coefficient (MCC) takes all four classes of the confusion matrix into consideration. MCC is sometimes referred to as the single most powerful metric in binary classification problems, especially for data with class imbalances. . Formula: (TP×TN-FP×FN)/√((TP+FP)×(TP+FN)×(TN+FP)×(TN+FN)) . res_mcc &lt;- mcc_parity(data = df_valid, outcome = &#39;Two_yr_Recidivism_01&#39;, outcome_base = &#39;0&#39;, group = &#39;Female&#39;, probs = &#39;prob_1&#39;, cutoff = 0.5, base = &#39;Male&#39;) res_mcc$Metric . MaleFemale . MCC0.3316558 | 0.2893650 | . MCC Parity1.0000000 | 0.8724859 | . 7. Closing words . You have read through the fairness R package tutorial! By now, you should have a solid grip on algorithmic group fairness metrics. . We hope that you will be able to use the R package in your data analysis! Please let me know if you run into any issues while working with the package in the comments below or on GitHub. Please also feel free to contact the authors if you have any feedback. . Acknowlegments: . Calders, T., &amp; Verwer, S. (2010). Three naive Bayes approaches for discrimination-free classification. Data Mining and Knowledge Discovery, 21(2), 277-292. | Chouldechova, A. (2017). Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data, 5(2), 153-163. | Feldman, M., Friedler, S. A., Moeller, J., Scheidegger, C., &amp; Venkatasubramanian, S. (2015, August). Certifying and removing disparate impact. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp. 259-268). ACM. | Friedler, S. A., Scheidegger, C., Venkatasubramanian, S., Choudhary, S., Hamilton, E. P., &amp; Roth, D. (2018). A comparative study of fairness-enhancing interventions in machine learning. arXiv preprint arXiv:1802.04422. | Zafar, M. B., Valera, I., Gomez Rodriguez, M., &amp; Gummadi, K. P. (2017, April). Fairness beyond disparate treatment &amp; disparate impact: Learning classification without disparate mistreatment. In Proceedings of the 26th International Conference on World Wide Web (pp. 1171-1180). International World Wide Web Conferences Steering Committee. | .",
            "url": "https://kozodoi.me/blog/20200501/fairness-tutorial",
            "relUrl": "/blog/20200501/fairness-tutorial",
            "date": " • May 1, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "Hi, I am Abhishek! . Click here to download my CV and check out my portfolio to see my work. . Click here to download my CV and check out my portfolio to see my work. . About me . As a Junior in Computer Science Engineering at Bennett University, I’ve amassed two years of academic experience in Artificial Intelligence and Machine Learning. Proficient in TensorFlow, Continously pushing myself through making projects in AL-ML, keen to explore innovations like GANs and diffusion models in image processing. I bring passion, skills, and a drive to learn and contribute. In my free time, I enjoy playing arcade and challenging myself in lifting weights. . . . About this website . This website hosts my personal portfolio including all kaggle competetions, projects’ notebook and research papers, where I share interesting project findings. All opinions published here are my own. The website includes different sections featuring my work: . 📁 my portfolio with ML projects on different topics | 📚 my ML publications with paper abstracts and full-text PDFs | 🥇 my Kaggle solutions with links to code and write-ups | 🖥 my certifications with links to certificates and completed courses | . . . Contact . Would like to have a chat? Click here to send me an e-mail. . I am also happy to connect on different social and professional platforms. Click the badges below to see my profile. . | | | .",
          "url": "https://kozodoi.me/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Portfolio",
          "content": ". Portfolio . My public ML portfolio includes projects on different topics, including generative AI, NLP, computer vision, and tabular data. To see more of my work, visit my GitHub page, download my CV or check out the about page. . My public ML portfolio includes projects on different topics, including generative AI, NLP, computer vision, and tabular data. To see more of my work, visit my GitHub page, download my CV or check out the about page. . . My portfolio features the following projects: . &#128214; Transformer from Scratch(Implementing ML Papers) | &#129516; Finetuning Minstral 7b for Curriculam Based Question Generation | &#128200; Retrieval Augmented Generation Model from LLaMa-2 | &#128200; Disease Recognizer using word vector embeddings | . Scroll down to see more generative AI and ML projects grouped by application domains. Click &quot;read more&quot; to see project summaries, and follow GitHub links for code and documentation. .",
          "url": "https://kozodoi.me/portfolio/",
          "relUrl": "/portfolio/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Papers",
          "content": "Research . This page summarizes my research activities, including publications in academic journals and conference papers. More to be added as the time passes by.... . This page summarizes my research activities, including publications in academic journals and conference papers. More to be added as the time passes by.... . . This page overviews my research activities: . &#128218; Selected academic publications | . . . . Paper . 2024 . Abhishek Singh, Krishnendu Ghosh Curriculum-based Question Generation for Mathematics and Science ArXiv preprint. | . . Abstract: The purpose of this study is to leverage the use of Natural Language Processing by using Large Language Models (LLMs) so that we can generate questions with respect to the curriculum for Mathematics and Science subjects. To achieve this, we implemented two principal methodologies from the field of Generative-AI: Fine-tuning and Retrieval Augmented Generation (RAG) are two of these approaches. The Fine-tuning approach which we are using is Transfer-Learning using unsloth, which offers two types of Hugging Face&#39;s trainers for fine-tuning in its href{https://github.com/unslothai/unsloth#finetune-mistral-gemma-llama-2-5x-faster-with-70-less-memory}{GitHub repository}: Direct Performance Optimizer (DPO) and Supervised QA Fine-tuning using SFT Trainer. We have used the SFT Trainer, which requires reward modelling or reinforcement learning and works based on labelled data to fine-tune the pre-trained LLM for specific tasks cite{li2024getting}. To generate curriculum-based questions, we have used a supervised and labelled dataset of Mathematics questions cite{hendrycksmath2021}. In contrast, for Science, we have used NCERT (National Council of Educational Research and Training) books issued by CBSE (Central Board of Secondary Education), a fine-tuned model for structured and supervised mathematical questions data, while RAG model for books&#39; PDF for generating contextually appropriate science questions due to containing a large number of factual details in a book. This research highlights how these approaches can be used to design highly effective instructional tools compatible with generating curriculum-based questions on required subjects. Thus, using these approaches, there is a possibility of developing educational content which is beneficial for students and teachers. | &#128220; Abstract &#128214; PDF | . .",
          "url": "https://kozodoi.me/papers/",
          "relUrl": "/papers/",
          "date": ""
      }
      
  

  
      ,"page5": {
          "title": "Kaggle",
          "content": "Kaggle . Started taking part at ML competitions on Kaggle from 2024. This page summarizes my participations, writeups and GitHub repos with my solutions. Check out my Kaggle profile to see more. . Started taking part at ML competitions on Kaggle from 2024. This page summarizes my participations, writeups and GitHub repos with my solutions. Check out my Kaggle profile to see more. .",
          "url": "https://kozodoi.me/kaggle/",
          "relUrl": "/kaggle/",
          "date": ""
      }
      
  

  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Certifications . This page provides a list of certifications and courses I completed. Click here to download my CV and check out my portfolio to see my work. . This page provides a list of certifications and courses I completed. Click here to download my CV and check out my portfolio to see my work. . . This page overviews my certifications and courses: . &#9729; Cloud services | &#129302; Machine learning | &#128187; Coding | &#128190; Databases | . . . . Cloud Services . AWS Certified Machine Learning Specialty Issued by Amazon Web Services | . . Summary: Demonstrated in-depth understanding of AWS Machine Learning services. Learned to build, train, tune, and deploy ML models using the AWS Cloud. Showed the ability to derive insight from AWS ML services using either pretrained models or custom models built from open-source frameworks. | &#128220; Summary &#127891; Certificate | . . AWS Certified Developer - Associate Issued by Amazon Web Services | . . Summary: Showed a comprehensive understanding of application life-cycle management. Demonstrated proficiency in deploying with a CI/CD pipeline and using containers. Showed ability to develop, deploy and debug cloud-based applications that follow AWS best practices. | &#128220; Summary &#127891; Certificate | . . AWS Certified Solutions Architect - Associate Issued by Amazon Web Services | . . Summary: Gained a comprehensive understanding of AWS services and technologies. Demonstrated the ability to design well-architected cloud solutions that are scalable, secure, resilient, efficient and fault-tolerant. | &#128220; Summary &#127891; Certificate | . . AWS Certified Cloud Practitioner Issued by Amazon Web Services | . . Summary: Gained a fundamental understanding of IT services and their uses in the AWS Cloud. Demonstrated cloud fluency and foundational AWS knowledge. Learned to identify essential AWS services necessary to set up AWS-focused projects. | &#128220; Summary &#127891; Certificate | . . . . . Machine Learning . Machine Learning Engineer Nanodegree Issued by Udacity | . . Summary: Learned software engineering and object-oriented programming practices and developed an open-source Python package for data processing. Deployed Machine Learning and Deep Learning models using Amazon SageMaker. Used API Gateway and Lambda to integrate deployed models into interactive web apps. | &#128220; Summary &#127891; Certificate &#128187; Capstone project | . . Deep Learning Nanodegree Issued by Udacity | . . Summary: Learned theoretical foundations of Deep Learning. Implemented different neural network architectures from scratch. Developed PyTorch modeling pipelines using CNNs, RNNs and LSTMs for a variety of prediction tasks. Deployed the trained models on Amazon SageMaker. | &#128220; Summary &#127891; Certificate | . . . . . Coding . Algorithmic Toolbox Issued by University of California, San Diego | . . Summary: Learned key algorithmic techniques and concepts arising frequently in practical applications, including sorting and searching, divide and conquer, greedy algorithms, dynamic programming and recursion. Implemented algorithms to solve a variety of computational problems in Python and analyzed their running and memory complexity. | &#128220; Summary &#127891; Certificate | . . Data Structures Issued by University of California, San Diego | . . Summary: Learned common data structures used in various computational problems, including arrays, linked lists, stacks, queues, hash tables and trees. Analyzed typical use cases for these data structures and complexity of common operations. Practiced implementing data structures in Python programming assignments. | &#128220; Summary &#127891; Certificate | . . Data Structures and Algorithms with Python Issued by Codecademy | . . Summary: Reviewed basic data structures and algorithms and extensively practiced their implementation in Python. Practiced analyzing running and memory complexity of different data structures and algorithms. | &#128220; Summary &#127891; Certificate | . . . . . Databases . SQL for Data Science Issued by University of California, Davis | . . Summary: Learned fundamentals of SQL for Data Science purposes, including extracting, manipulating and combining data. Covered filtering, sorting and aggregating functionality. Practiced subqueries and table joins. Solved a variety of SQL programming tasks. | &#128220; Summary &#127891; Certificate | . .",
          "url": "https://kozodoi.me/certifications/",
          "relUrl": "/certifications/",
          "date": ""
      }
      
  

  
  

  
  

  
  

  

  
  

  
      ,"page15": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://kozodoi.me/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}